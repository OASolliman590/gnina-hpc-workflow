{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Enhanced GNINA Docking Pipeline\n",
        "\n",
        "A comprehensive molecular docking pipeline using [GNINA](https://github.com/gnina/gnina) with advanced features including:\n",
        "- Tiered CNN scoring workflow\n",
        "- Flexible receptor docking\n",
        "- Parallel processing\n",
        "- Resume capability\n",
        "- PDB preparation wizard integration\n",
        "- Quality control and validation\n",
        "- Covalent docking support\n",
        "\n",
        "**References:**\n",
        "- GNINA: https://github.com/gnina/gnina\n",
        "- PDB Preparation Wizard: https://github.com/OASolliman590/pdb-prepare-wizard\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup and Dependencies\n",
        "\n",
        "This cell sets up the Google Colab environment, mounts Google Drive, and installs all required dependencies for the GNINA docking pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Import essential libraries\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import pathlib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import warnings\n",
        "import json\n",
        "import concurrent.futures\n",
        "import multiprocessing as mp\n",
        "import threading\n",
        "import queue\n",
        "import time\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.offline as pyo\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úÖ Environment setup complete\")\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"Working directory: {os.getcwd()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration and Project Setup\n",
        "\n",
        "Configure the GNINA docking pipeline parameters, create necessary directories, and set up the project structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Project configuration\n",
        "WORK_DIR = \"/content/drive/MyDrive/GNINA_Docking_Project\"\n",
        "%cd {WORK_DIR}\n",
        "\n",
        "# GNINA configuration based on https://github.com/gnina/gnina\n",
        "CONFIG = {\n",
        "    'project_name': 'GNINA_Docking_Project',\n",
        "    'gnina_version': 'v1.3.2',  # Latest version from GitHub releases\n",
        "    'exhaustiveness': 32,\n",
        "    'num_modes': 20,\n",
        "    'seed': 42,\n",
        "    'cnn_scoring': 'rescore',  # Default CNN scoring mode\n",
        "    'cpu_cores': 4,\n",
        "    'batch_size': 5,\n",
        "    'timeout': 300,\n",
        "    'use_gpu': False,  # Will be detected automatically\n",
        "}\n",
        "\n",
        "# Create project directories\n",
        "dirs_to_create = [\n",
        "    'ligands_raw', 'ligands_prep', 'receptors_raw', 'receptors_prep',\n",
        "    'gnina_out', 'results', 'logs', 'visualizations', 'enhanced_analysis'\n",
        "]\n",
        "\n",
        "for dir_name in dirs_to_create:\n",
        "    os.makedirs(dir_name, exist_ok=True)\n",
        "\n",
        "print(\"‚úÖ Project directories created\")\n",
        "print(f\"Project: {CONFIG['project_name']}\")\n",
        "print(f\"GNINA Version: {CONFIG['gnina_version']}\")\n",
        "print(f\"Working Directory: {WORK_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Install Dependencies and Download GNINA\n",
        "\n",
        "Install required Python packages and download the GNINA binary from the official [GNINA repository](https://github.com/gnina/gnina).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Python dependencies\n",
        "%pip install -q rdkit-pypi meeko pdb2pqr openbabel biopython plip\n",
        "!apt-get update -qq && apt-get install -y -qq openbabel pdb2pqr\n",
        "\n",
        "# Download GNINA binary from GitHub releases\n",
        "# Reference: https://github.com/gnina/gnina/releases\n",
        "gnina_url = f\"https://github.com/gnina/gnina/releases/download/{CONFIG['gnina_version']}/gnina\"\n",
        "!wget -q {gnina_url} -O gnina\n",
        "!chmod +x gnina\n",
        "\n",
        "# Verify GNINA installation\n",
        "print(\"üîç Verifying GNINA installation...\")\n",
        "!./gnina --version\n",
        "\n",
        "# GPU detection for CUDA support\n",
        "try:\n",
        "    gpu_info = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
        "    if gpu_info.returncode == 0:\n",
        "        print(\"‚úÖ GPU detected:\")\n",
        "        print(gpu_info.stdout.split('\\n')[0:3])\n",
        "        CONFIG['use_gpu'] = True\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No GPU detected, using CPU\")\n",
        "        CONFIG['use_gpu'] = False\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è GPU check failed, using CPU\")\n",
        "    CONFIG['use_gpu'] = False\n",
        "\n",
        "print(f\"‚úÖ GNINA {CONFIG['gnina_version']} installed successfully\")\n",
        "print(f\"GPU acceleration: {'Enabled' if CONFIG['use_gpu'] else 'Disabled'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. File Validation and Pairlist Loading\n",
        "\n",
        "Validate project structure and load the pairlist.csv file containing receptor-ligand combinations and binding site coordinates.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def validate_project_structure():\n",
        "    \"\"\"Validate that required files and directories exist\"\"\"\n",
        "    required_files = ['pairlist.csv']\n",
        "    required_dirs = ['ligands_raw', 'receptors_raw']\n",
        "    \n",
        "    missing_files = [f for f in required_files if not os.path.exists(f)]\n",
        "    missing_dirs = [d for d in required_dirs if not os.path.exists(d)]\n",
        "    \n",
        "    if missing_files or missing_dirs:\n",
        "        print(\"‚ùå Missing required files/directories:\")\n",
        "        for item in missing_files + missing_dirs:\n",
        "            print(f\"   - {item}\")\n",
        "        return False\n",
        "    \n",
        "    print(\"‚úÖ Project structure validated\")\n",
        "    return True\n",
        "\n",
        "def load_and_validate_pairlist():\n",
        "    \"\"\"Load and validate pairlist.csv file\"\"\"\n",
        "    try:\n",
        "        # Load pairlist\n",
        "        df = pd.read_csv('pairlist.csv')\n",
        "        \n",
        "        # Normalize column names\n",
        "        df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
        "        \n",
        "        # Validate required columns\n",
        "        required_cols = ['receptor', 'ligand', 'center_x', 'center_y', 'center_z', \n",
        "                        'size_x', 'size_y', 'size_z']\n",
        "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
        "        \n",
        "        if missing_cols:\n",
        "            print(f\"‚ùå Missing required columns: {missing_cols}\")\n",
        "            return None\n",
        "        \n",
        "        # Validate numeric columns\n",
        "        coord_cols = ['center_x', 'center_y', 'center_z', 'size_x', 'size_y', 'size_z']\n",
        "        for col in coord_cols:\n",
        "            if not pd.api.types.is_numeric_dtype(df[col]):\n",
        "                print(f\"‚ùå Column {col} must be numeric\")\n",
        "                return None\n",
        "        \n",
        "        # Add site_id if missing\n",
        "        if 'site_id' not in df.columns:\n",
        "            df['site_id'] = 'site_1'\n",
        "            print(\"‚ö†Ô∏è Added default site_id column\")\n",
        "        \n",
        "        print(f\"‚úÖ Pairlist loaded: {len(df)} entries\")\n",
        "        return df\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading pairlist: {e}\")\n",
        "        return None\n",
        "\n",
        "# Validate and load\n",
        "if not validate_project_structure():\n",
        "    print(\"Please ensure all required files and directories are present\")\n",
        "else:\n",
        "    pairlist_df = load_and_validate_pairlist()\n",
        "    if pairlist_df is not None:\n",
        "        print(\"\\nüìä Pairlist preview:\")\n",
        "        print(pairlist_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Core Docking Configuration\n",
        "\n",
        "Configure the main GNINA docking parameters including CNN scoring modes, performance settings, and docking options. This is the central configuration hub for all docking operations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Core GNINA Docking Configuration\n",
        "# =============================================================================\n",
        "\n",
        "class GNINADockingConfig:\n",
        "    \"\"\"Central configuration class for GNINA docking parameters\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        # Base configuration from https://github.com/gnina/gnina\n",
        "        self.base_config = {\n",
        "            'gnina_binary': './gnina',\n",
        "            'exhaustiveness': 32,           # Global search exhaustiveness\n",
        "            'num_modes': 20,                # Maximum binding modes\n",
        "            'seed': 42,                     # Random seed for reproducibility\n",
        "            'cpu_cores': 4,                 # CPU cores to use (fallback when GPU unavailable)\n",
        "            'timeout': 300,                 # Timeout in seconds\n",
        "            'min_rmsd_filter': 1.0,         # RMSD filter for pose diversity\n",
        "            'pose_sort_order': 0,           # 0=CNNscore, 1=CNNaffinity, 2=Energy\n",
        "            'cnn_rotation': 0,              # CNN rotation evaluation\n",
        "            'add_hydrogens': True,          # Auto-add hydrogens\n",
        "            'strip_hydrogens': False,       # Remove polar hydrogens\n",
        "            'use_gpu': True,                # Prioritize GPU acceleration\n",
        "            'gpu_device': 0,                # GPU device ID\n",
        "        }\n",
        "        \n",
        "        # CNN scoring modes as per GNINA documentation\n",
        "        self.cnn_modes = {\n",
        "            'none': {\n",
        "                'description': 'No CNNs used - empirical scoring only',\n",
        "                'speed': 'fastest',\n",
        "                'accuracy': 'baseline',\n",
        "                'use_case': 'Quick screening, baseline comparison'\n",
        "            },\n",
        "            'rescore': {\n",
        "                'description': 'CNN used for reranking final poses (default)',\n",
        "                'speed': 'fast',\n",
        "                'accuracy': 'good',\n",
        "                'use_case': 'Large library screening, first-pass ranking'\n",
        "            },\n",
        "            'refinement': {\n",
        "                'description': 'CNN used to refine poses after Monte Carlo',\n",
        "                'speed': 'medium (10x slower than rescore on GPU)',\n",
        "                'accuracy': 'better',\n",
        "                'use_case': 'Focused re-docking, pose refinement'\n",
        "            },\n",
        "            'all': {\n",
        "                'description': 'CNN used throughout entire procedure',\n",
        "                'speed': 'slowest (extremely intensive)',\n",
        "                'accuracy': 'best',\n",
        "                'use_case': 'Final validation, small high-value sets'\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        # Tiered workflow configuration\n",
        "        self.tiered_config = {\n",
        "            'stage_a': {\n",
        "                'cnn_scoring': 'rescore',\n",
        "                'exhaustiveness': 12,\n",
        "                'num_modes': 8,\n",
        "                'description': 'Fast broad screening',\n",
        "                'cnn_score_threshold': 0.5,\n",
        "                'max_ligands_per_receptor': None,\n",
        "                'top_percentage': None\n",
        "            },\n",
        "            'stage_b': {\n",
        "                'cnn_scoring': 'refinement',\n",
        "                'exhaustiveness': 24,\n",
        "                'num_modes': 15,\n",
        "                'description': 'Balanced refinement',\n",
        "                'cnn_score_threshold': 0.7,\n",
        "                'max_ligands_per_receptor': 5,\n",
        "                'top_percentage': 0.05\n",
        "            },\n",
        "            'stage_c': {\n",
        "                'cnn_scoring': 'all',\n",
        "                'exhaustiveness': 48,\n",
        "                'num_modes': 20,\n",
        "                'description': 'High-accuracy final screening',\n",
        "                'cnn_score_threshold': 0.8,\n",
        "                'max_ligands_per_receptor': 2,\n",
        "                'top_percentage': 0.01\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        # Performance settings\n",
        "        self.performance_config = {\n",
        "            'parallel_processing': True,\n",
        "            'max_workers': 4,\n",
        "            'batch_size': 5,\n",
        "            'resume_capability': True,\n",
        "            'progress_tracking': True\n",
        "        }\n",
        "        \n",
        "        # Current active configuration\n",
        "        self.active_config = self.base_config.copy()\n",
        "        self.active_config['cnn_scoring'] = 'rescore'  # Default mode\n",
        "    \n",
        "    def set_cnn_mode(self, mode):\n",
        "        \"\"\"Set CNN scoring mode\"\"\"\n",
        "        if mode not in self.cnn_modes:\n",
        "            raise ValueError(f\"Invalid CNN mode: {mode}. Available: {list(self.cnn_modes.keys())}\")\n",
        "        \n",
        "        self.active_config['cnn_scoring'] = mode\n",
        "        print(f\"‚úÖ CNN scoring mode set to: {mode}\")\n",
        "        print(f\"   Description: {self.cnn_modes[mode]['description']}\")\n",
        "        print(f\"   Speed: {self.cnn_modes[mode]['speed']}\")\n",
        "        print(f\"   Use case: {self.cnn_modes[mode]['use_case']}\")\n",
        "    \n",
        "    def set_performance_params(self, exhaustiveness=None, num_modes=None, cpu_cores=None):\n",
        "        \"\"\"Set performance parameters\"\"\"\n",
        "        if exhaustiveness is not None:\n",
        "            self.active_config['exhaustiveness'] = exhaustiveness\n",
        "        if num_modes is not None:\n",
        "            self.active_config['num_modes'] = num_modes\n",
        "        if cpu_cores is not None:\n",
        "            self.active_config['cpu_cores'] = cpu_cores\n",
        "        \n",
        "        print(\"‚úÖ Performance parameters updated:\")\n",
        "        print(f\"   Exhaustiveness: {self.active_config['exhaustiveness']}\")\n",
        "        print(f\"   Num modes: {self.active_config['num_modes']}\")\n",
        "        print(f\"   CPU cores: {self.active_config['cpu_cores']}\")\n",
        "    \n",
        "    def set_gpu_acceleration(self, use_gpu=True, gpu_device=0):\n",
        "        \"\"\"Configure GPU acceleration settings\"\"\"\n",
        "        self.active_config['use_gpu'] = use_gpu\n",
        "        self.active_config['gpu_device'] = gpu_device\n",
        "        \n",
        "        if use_gpu and CONFIG.get('use_gpu', False):\n",
        "            print(f\"‚úÖ GPU acceleration enabled (device {gpu_device})\")\n",
        "        elif use_gpu and not CONFIG.get('use_gpu', False):\n",
        "            print(\"‚ö†Ô∏è GPU requested but not available, will use CPU\")\n",
        "            self.active_config['use_gpu'] = False\n",
        "        else:\n",
        "            print(\"‚úÖ CPU acceleration enabled\")\n",
        "    \n",
        "    def set_tiered_stage(self, stage):\n",
        "        \"\"\"Set configuration for a specific tiered stage\"\"\"\n",
        "        if stage not in self.tiered_config:\n",
        "            raise ValueError(f\"Invalid stage: {stage}. Available: {list(self.tiered_config.keys())}\")\n",
        "        \n",
        "        stage_config = self.tiered_config[stage]\n",
        "        self.active_config.update({\n",
        "            'cnn_scoring': stage_config['cnn_scoring'],\n",
        "            'exhaustiveness': stage_config['exhaustiveness'],\n",
        "            'num_modes': stage_config['num_modes']\n",
        "        })\n",
        "        \n",
        "        print(f\"‚úÖ Tiered stage configuration set: {stage}\")\n",
        "        print(f\"   Description: {stage_config['description']}\")\n",
        "        print(f\"   CNN scoring: {stage_config['cnn_scoring']}\")\n",
        "        print(f\"   Exhaustiveness: {stage_config['exhaustiveness']}\")\n",
        "        print(f\"   Num modes: {stage_config['num_modes']}\")\n",
        "    \n",
        "    def get_gnina_command_base(self):\n",
        "        \"\"\"Get base GNINA command arguments with GPU prioritization\"\"\"\n",
        "        cmd_args = [\n",
        "            '--exhaustiveness', str(self.active_config['exhaustiveness']),\n",
        "            '--num_modes', str(self.active_config['num_modes']),\n",
        "            '--seed', str(self.active_config['seed']),\n",
        "            '--cnn_scoring', self.active_config['cnn_scoring'],\n",
        "            '--cnn_rotation', str(self.active_config['cnn_rotation']),\n",
        "            '--min_rmsd_filter', str(self.active_config['min_rmsd_filter']),\n",
        "            '--pose_sort_order', str(self.active_config['pose_sort_order']),\n",
        "        ]\n",
        "        \n",
        "        # GPU acceleration (prioritized over CPU)\n",
        "        if self.active_config.get('use_gpu', False) and CONFIG.get('use_gpu', False):\n",
        "            cmd_args.extend(['--gpu', '--device', str(self.active_config.get('gpu_device', 0))])\n",
        "            print(f\"   üöÄ Using GPU acceleration (device {self.active_config.get('gpu_device', 0)})\")\n",
        "        else:\n",
        "            # Fallback to CPU\n",
        "            cmd_args.extend(['--cpu', str(self.active_config['cpu_cores'])])\n",
        "            print(f\"   üíª Using CPU cores: {self.active_config['cpu_cores']}\")\n",
        "        \n",
        "        if self.active_config['add_hydrogens']:\n",
        "            cmd_args.append('--addH')\n",
        "        if self.active_config['strip_hydrogens']:\n",
        "            cmd_args.append('--stripH')\n",
        "        \n",
        "        return cmd_args\n",
        "    \n",
        "    def display_configuration(self):\n",
        "        \"\"\"Display current configuration\"\"\"\n",
        "        print(\"üîß Current GNINA Docking Configuration:\")\n",
        "        print(f\"   CNN Scoring: {self.active_config['cnn_scoring']}\")\n",
        "        print(f\"   Exhaustiveness: {self.active_config['exhaustiveness']}\")\n",
        "        print(f\"   Num Modes: {self.active_config['num_modes']}\")\n",
        "        print(f\"   Acceleration: {'GPU' if self.active_config.get('use_gpu', False) and CONFIG.get('use_gpu', False) else 'CPU'}\")\n",
        "        if self.active_config.get('use_gpu', False) and CONFIG.get('use_gpu', False):\n",
        "            print(f\"   GPU Device: {self.active_config.get('gpu_device', 0)}\")\n",
        "        else:\n",
        "            print(f\"   CPU Cores: {self.active_config['cpu_cores']}\")\n",
        "        print(f\"   Seed: {self.active_config['seed']}\")\n",
        "        print(f\"   Timeout: {self.active_config['timeout']}s\")\n",
        "        print(f\"   RMSD Filter: {self.active_config['min_rmsd_filter']}√Ö\")\n",
        "        print(f\"   Pose Sort: {self.active_config['pose_sort_order']} (0=CNNscore)\")\n",
        "    \n",
        "    def display_cnn_modes(self):\n",
        "        \"\"\"Display available CNN modes\"\"\"\n",
        "        print(\"üß† Available CNN Scoring Modes:\")\n",
        "        for mode, info in self.cnn_modes.items():\n",
        "            print(f\"   {mode}: {info['description']}\")\n",
        "            print(f\"      Speed: {info['speed']}\")\n",
        "            print(f\"      Use case: {info['use_case']}\")\n",
        "            print()\n",
        "\n",
        "# Initialize configuration\n",
        "docking_config = GNINADockingConfig()\n",
        "\n",
        "# Display available options\n",
        "docking_config.display_cnn_modes()\n",
        "docking_config.display_configuration()\n",
        "\n",
        "print(\"\\nüí° Configuration Examples:\")\n",
        "print(\"   # Set CNN mode\")\n",
        "print(\"   docking_config.set_cnn_mode('refinement')\")\n",
        "print(\"   \")\n",
        "print(\"   # Set performance parameters\")\n",
        "print(\"   docking_config.set_performance_params(exhaustiveness=48, num_modes=20)\")\n",
        "print(\"   \")\n",
        "print(\"   # Configure GPU acceleration (prioritized)\")\n",
        "print(\"   docking_config.set_gpu_acceleration(use_gpu=True, gpu_device=0)\")\n",
        "print(\"   \")\n",
        "print(\"   # Set tiered stage\")\n",
        "print(\"   docking_config.set_tiered_stage('stage_b')\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Tiered CNN Scoring Workflow\n",
        "\n",
        "Implement the tiered CNN scoring approach as recommended in the [GNINA documentation](https://github.com/gnina/gnina). This creates a funnel workflow: broad screening ‚Üí focused refinement ‚Üí high-accuracy validation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Tiered CNN Scoring Workflow Implementation\n",
        "# Based on GNINA documentation: https://github.com/gnina/gnina\n",
        "# =============================================================================\n",
        "\n",
        "class TieredCNNWorkflow:\n",
        "    \"\"\"\n",
        "    Tiered CNN scoring workflow as recommended by GNINA documentation:\n",
        "    Stage A: Fast broad screening (rescore)\n",
        "    Stage B: Focused refinement (refinement) \n",
        "    Stage C: High-accuracy validation (all)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, docking_config):\n",
        "        self.config = docking_config\n",
        "        self.stage_results = {}\n",
        "        \n",
        "        # Stage configurations based on GNINA best practices\n",
        "        self.stages = {\n",
        "            'A': {\n",
        "                'name': 'Broad Screening',\n",
        "                'cnn_scoring': 'rescore',\n",
        "                'exhaustiveness': 12,\n",
        "                'num_modes': 8,\n",
        "                'description': 'Fast broad screening with CNN rescoring',\n",
        "                'cnn_score_threshold': 0.5,\n",
        "                'max_ligands_per_receptor': None,\n",
        "                'top_percentage': None,\n",
        "                'use_case': 'Large library screening, first-pass ranking'\n",
        "            },\n",
        "            'B': {\n",
        "                'name': 'Focused Refinement', \n",
        "                'cnn_scoring': 'refinement',\n",
        "                'exhaustiveness': 24,\n",
        "                'num_modes': 15,\n",
        "                'description': 'Balanced refinement with CNN pose optimization',\n",
        "                'cnn_score_threshold': 0.7,\n",
        "                'max_ligands_per_receptor': 5,\n",
        "                'top_percentage': 0.05,\n",
        "                'use_case': 'Focused re-docking, pose refinement'\n",
        "            },\n",
        "            'C': {\n",
        "                'name': 'High-Accuracy Validation',\n",
        "                'cnn_scoring': 'all', \n",
        "                'exhaustiveness': 48,\n",
        "                'num_modes': 20,\n",
        "                'description': 'High-accuracy final screening with full CNN',\n",
        "                'cnn_score_threshold': 0.8,\n",
        "                'max_ligands_per_receptor': 2,\n",
        "                'top_percentage': 0.01,\n",
        "                'use_case': 'Final validation, small high-value sets'\n",
        "            }\n",
        "        }\n",
        "    \n",
        "    def get_stage_config(self, stage):\n",
        "        \"\"\"Get configuration for a specific stage\"\"\"\n",
        "        if stage not in self.stages:\n",
        "            raise ValueError(f\"Invalid stage: {stage}. Available: {list(self.stages.keys())}\")\n",
        "        return self.stages[stage]\n",
        "    \n",
        "    def filter_ligands_for_stage(self, stage, previous_results, pairlist_df):\n",
        "        \"\"\"Filter ligands based on previous stage results\"\"\"\n",
        "        stage_config = self.get_stage_config(stage)\n",
        "        \n",
        "        if stage == 'A':\n",
        "            # Stage A: Use all ligands\n",
        "            return pairlist_df.copy()\n",
        "        \n",
        "        if not previous_results:\n",
        "            print(f\"‚ö†Ô∏è No previous results for stage {stage}, using all ligands\")\n",
        "            return pairlist_df.copy()\n",
        "        \n",
        "        # Extract successful results with scores\n",
        "        successful_results = []\n",
        "        for result in previous_results:\n",
        "            if result['status'] == 'success' and 'scores' in result:\n",
        "                for score_data in result['scores']:\n",
        "                    if 'cnn_score' in score_data:\n",
        "                        successful_results.append({\n",
        "                            'receptor': result['receptor'],\n",
        "                            'ligand': result['ligand'],\n",
        "                            'site_id': result['site_id'],\n",
        "                            'cnn_score': score_data['cnn_score']\n",
        "                        })\n",
        "        \n",
        "        if not successful_results:\n",
        "            print(f\"‚ö†Ô∏è No successful results with scores for stage {stage}\")\n",
        "            return pairlist_df.copy()\n",
        "        \n",
        "        # Convert to DataFrame for filtering\n",
        "        results_df = pd.DataFrame(successful_results)\n",
        "        \n",
        "        # Apply filtering criteria\n",
        "        filtered_pairs = []\n",
        "        \n",
        "        for _, row in pairlist_df.iterrows():\n",
        "            receptor = row['receptor']\n",
        "            ligand = row['ligand']\n",
        "            site_id = row['site_id']\n",
        "            \n",
        "            # Get scores for this receptor-ligand pair\n",
        "            pair_scores = results_df[\n",
        "                (results_df['receptor'] == receptor) & \n",
        "                (results_df['ligand'] == ligand) &\n",
        "                (results_df['site_id'] == site_id)\n",
        "            ]\n",
        "            \n",
        "            if len(pair_scores) == 0:\n",
        "                continue\n",
        "            \n",
        "            # Check CNN score threshold\n",
        "            max_cnn_score = pair_scores['cnn_score'].max()\n",
        "            if max_cnn_score < stage_config['cnn_score_threshold']:\n",
        "                continue\n",
        "            \n",
        "            # Check top percentage\n",
        "            if stage_config['top_percentage'] is not None:\n",
        "                # Get all scores for this receptor\n",
        "                receptor_scores = results_df[results_df['receptor'] == receptor]['cnn_score']\n",
        "                threshold_score = receptor_scores.quantile(1 - stage_config['top_percentage'])\n",
        "                if max_cnn_score < threshold_score:\n",
        "                    continue\n",
        "            \n",
        "            # Check max ligands per receptor\n",
        "            if stage_config['max_ligands_per_receptor'] is not None:\n",
        "                receptor_ligands = results_df[results_df['receptor'] == receptor]['ligand'].unique()\n",
        "                if len(receptor_ligands) > stage_config['max_ligands_per_receptor']:\n",
        "                    # Keep only top ligands for this receptor\n",
        "                    top_ligands = results_df[results_df['receptor'] == receptor].groupby('ligand')['cnn_score'].max().nlargest(stage_config['max_ligands_per_receptor']).index\n",
        "                    if ligand not in top_ligands:\n",
        "                        continue\n",
        "            \n",
        "            filtered_pairs.append(row)\n",
        "        \n",
        "        filtered_df = pd.DataFrame(filtered_pairs)\n",
        "        print(f\"‚úÖ Stage {stage} filtering: {len(filtered_df)}/{len(pairlist_df)} ligands selected\")\n",
        "        \n",
        "        return filtered_df\n",
        "    \n",
        "    def run_stage(self, stage, pairlist_df, previous_results=None):\n",
        "        \"\"\"Run a specific stage of the tiered workflow\"\"\"\n",
        "        stage_config = self.get_stage_config(stage)\n",
        "        \n",
        "        print(f\"\\\\nüöÄ Starting Stage {stage}: {stage_config['name']}\")\n",
        "        print(f\"   Description: {stage_config['description']}\")\n",
        "        print(f\"   CNN Scoring: {stage_config['cnn_scoring']}\")\n",
        "        print(f\"   Exhaustiveness: {stage_config['exhaustiveness']}\")\n",
        "        print(f\"   Num Modes: {stage_config['num_modes']}\")\n",
        "        print(f\"   Use Case: {stage_config['use_case']}\")\n",
        "        \n",
        "        # Filter ligands for this stage\n",
        "        input_df = self.filter_ligands_for_stage(stage, previous_results, pairlist_df)\n",
        "        \n",
        "        if len(input_df) == 0:\n",
        "            print(f\"   ‚ö†Ô∏è No ligands to process for Stage {stage}\")\n",
        "            return []\n",
        "        \n",
        "        print(f\"   Processing {len(input_df)} ligand-receptor pairs\")\n",
        "        \n",
        "        # Update docking configuration for this stage\n",
        "        self.config.set_tiered_stage(f'stage_{stage.lower()}')\n",
        "        \n",
        "        # Return the filtered input for processing by the main docking engine\n",
        "        return input_df\n",
        "    \n",
        "    def get_workflow_summary(self):\n",
        "        \"\"\"Get summary of the tiered workflow\"\"\"\n",
        "        print(\"üìä Tiered CNN Scoring Workflow Summary:\")\n",
        "        print(\"\\\\nStage A - Broad Screening:\")\n",
        "        print(\"   ‚Ä¢ CNN Scoring: rescore (fast)\")\n",
        "        print(\"   ‚Ä¢ Purpose: Large library screening, first-pass ranking\")\n",
        "        print(\"   ‚Ä¢ Threshold: CNN score ‚â• 0.5\")\n",
        "        print(\"   ‚Ä¢ Expected: 80-95% of ligands\")\n",
        "        \n",
        "        print(\"\\\\nStage B - Focused Refinement:\")\n",
        "        print(\"   ‚Ä¢ CNN Scoring: refinement (10x slower than rescore)\")\n",
        "        print(\"   ‚Ä¢ Purpose: Pose refinement, focused re-docking\")\n",
        "        print(\"   ‚Ä¢ Threshold: CNN score ‚â• 0.7, top 5% per receptor\")\n",
        "        print(\"   ‚Ä¢ Expected: 5-20% of ligands\")\n",
        "        \n",
        "        print(\"\\\\nStage C - High-Accuracy Validation:\")\n",
        "        print(\"   ‚Ä¢ CNN Scoring: all (extremely intensive)\")\n",
        "        print(\"   ‚Ä¢ Purpose: Final validation, small high-value sets\")\n",
        "        print(\"   ‚Ä¢ Threshold: CNN score ‚â• 0.8, top 1% per receptor\")\n",
        "        print(\"   ‚Ä¢ Expected: 1-5% of ligands\")\n",
        "        \n",
        "        print(\"\\\\nüí° Recommended Usage:\")\n",
        "        print(\"   ‚Ä¢ For large libraries (>1000 ligands): Use stages A ‚Üí B\")\n",
        "        print(\"   ‚Ä¢ For medium libraries (100-1000): Use stages A ‚Üí B ‚Üí C\")\n",
        "        print(\"   ‚Ä¢ For small libraries (<100): Use stage B or C directly\")\n",
        "\n",
        "# Initialize tiered workflow\n",
        "tiered_workflow = TieredCNNWorkflow(docking_config)\n",
        "\n",
        "# Display workflow information\n",
        "tiered_workflow.get_workflow_summary()\n",
        "\n",
        "print(\"\\\\nüîß Usage Examples:\")\n",
        "print(\"   # Run Stage A (broad screening)\")\n",
        "print(\"   stage_a_input = tiered_workflow.run_stage('A', pairlist_df)\")\n",
        "print(\"   \")\n",
        "print(\"   # Run Stage B (focused refinement)\")\n",
        "print(\"   stage_b_input = tiered_workflow.run_stage('B', pairlist_df, stage_a_results)\")\n",
        "print(\"   \")\n",
        "print(\"   # Run Stage C (high-accuracy validation)\")\n",
        "print(\"   stage_c_input = tiered_workflow.run_stage('C', pairlist_df, stage_b_results)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Core Docking Engine with Parallel Processing and Resume Capability\n",
        "\n",
        "The main GNINA docking engine that integrates parallel processing and resume functionality. This is the central component that executes all docking operations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Core GNINA Docking Engine with Parallel Processing and Resume Capability\n",
        "# Based on GNINA documentation: https://github.com/gnina/gnina\n",
        "# =============================================================================\n",
        "\n",
        "class GNINADockingEngine:\n",
        "    \"\"\"\n",
        "    Core GNINA docking engine with integrated parallel processing and resume capability.\n",
        "    This is the main component that executes all docking operations.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, docking_config, max_workers=4):\n",
        "        self.config = docking_config\n",
        "        self.max_workers = max_workers\n",
        "        self.gnina_binary = './gnina'\n",
        "        self.results = []\n",
        "        self.failures = []\n",
        "        self.progress_lock = threading.Lock()\n",
        "        \n",
        "        # Resume capability\n",
        "        self.resume_file = \"docking_state.json\"\n",
        "        self.state_file = f\"results/{self.resume_file}\"\n",
        "        \n",
        "        # Progress tracking\n",
        "        self.completed_pairs = set()\n",
        "        self.total_pairs = 0\n",
        "        \n",
        "    def build_gnina_command(self, row, flexible_residues=None):\n",
        "        \"\"\"Build GNINA command for a single docking run\"\"\"\n",
        "        receptor = f\"receptors_prep/{row['receptor']}.pdbqt\"\n",
        "        ligand = f\"ligands_prep/{row['ligand']}.pdbqt\"\n",
        "        \n",
        "        # Output files\n",
        "        tag = f\"{row['receptor']}_{row['site_id']}_{row['ligand']}\"\n",
        "        output_sdf = f\"gnina_out/{tag}_poses.sdf\"\n",
        "        log_file = f\"logs/{tag}.log\"\n",
        "        \n",
        "        # Base command\n",
        "        cmd = [\n",
        "            self.gnina_binary,\n",
        "            \"--receptor\", receptor,\n",
        "            \"--ligand\", ligand,\n",
        "            \"--out\", output_sdf,\n",
        "            \"--log\", log_file,\n",
        "        ]\n",
        "        \n",
        "        # Docking box parameters\n",
        "        cmd.extend([\n",
        "            \"--center_x\", str(row['center_x']),\n",
        "            \"--center_y\", str(row['center_y']),\n",
        "            \"--center_z\", str(row['center_z']),\n",
        "            \"--size_x\", str(row['size_x']),\n",
        "            \"--size_y\", str(row['size_y']),\n",
        "            \"--size_z\", str(row['size_z']),\n",
        "        ])\n",
        "        \n",
        "        # Add base configuration arguments (includes GPU/CPU selection)\n",
        "        cmd.extend(self.config.get_gnina_command_base())\n",
        "        \n",
        "        # Flexible receptor parameters\n",
        "        if flexible_residues:\n",
        "            flexres_str = \",\".join(flexible_residues)\n",
        "            flex_output = f\"gnina_out/{tag}_flex.pdbqt\"\n",
        "            cmd.extend([\n",
        "                \"--flexres\", flexres_str,\n",
        "                \"--flexdist\", \"3.5\",\n",
        "                \"--out_flex\", flex_output\n",
        "            ])\n",
        "        \n",
        "        return cmd, output_sdf, log_file\n",
        "    \n",
        "    def run_single_docking(self, row, flexible_residues=None):\n",
        "        \"\"\"Run docking for a single ligand-receptor pair\"\"\"\n",
        "        try:\n",
        "            # Check if already completed (resume capability)\n",
        "            pair_id = f\"{row['receptor']}_{row['site_id']}_{row['ligand']}\"\n",
        "            if pair_id in self.completed_pairs:\n",
        "                print(f\"   ‚è≠Ô∏è Skipping already completed: {pair_id}\")\n",
        "                return None\n",
        "            \n",
        "            # Build command\n",
        "            cmd, output_sdf, log_file = self.build_gnina_command(row, flexible_residues)\n",
        "            \n",
        "            # Run GNINA\n",
        "            result = subprocess.run(\n",
        "                cmd, \n",
        "                capture_output=True, \n",
        "                text=True, \n",
        "                timeout=self.config.active_config['timeout']\n",
        "            )\n",
        "            \n",
        "            # Parse results\n",
        "            if result.returncode == 0 and os.path.exists(output_sdf):\n",
        "                scores = self.parse_gnina_output(output_sdf, log_file)\n",
        "                return {\n",
        "                    'status': 'success',\n",
        "                    'receptor': row['receptor'],\n",
        "                    'ligand': row['ligand'],\n",
        "                    'site_id': row['site_id'],\n",
        "                    'output_file': output_sdf,\n",
        "                    'log_file': log_file,\n",
        "                    'scores': scores,\n",
        "                    'command': ' '.join(cmd),\n",
        "                    'pair_id': pair_id\n",
        "                }\n",
        "            else:\n",
        "                return {\n",
        "                    'status': 'error',\n",
        "                    'receptor': row['receptor'],\n",
        "                    'ligand': row['ligand'],\n",
        "                    'site_id': row['site_id'],\n",
        "                    'error': result.stderr,\n",
        "                    'returncode': result.returncode,\n",
        "                    'pair_id': pair_id\n",
        "                }\n",
        "                \n",
        "        except subprocess.TimeoutExpired:\n",
        "            return {\n",
        "                'status': 'timeout',\n",
        "                'receptor': row['receptor'],\n",
        "                'ligand': row['ligand'],\n",
        "                'site_id': row['site_id'],\n",
        "                'error': f\"Timeout after {self.config.active_config['timeout']}s\",\n",
        "                'pair_id': pair_id\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'status': 'error',\n",
        "                'receptor': row['receptor'],\n",
        "                'ligand': row['ligand'],\n",
        "                'site_id': row['site_id'],\n",
        "                'error': str(e),\n",
        "                'pair_id': pair_id\n",
        "            }\n",
        "    \n",
        "    def parse_gnina_output(self, output_sdf, log_file):\n",
        "        \"\"\"Parse GNINA output files to extract scores\"\"\"\n",
        "        scores = []\n",
        "        \n",
        "        try:\n",
        "            # Parse SDF file for pose information\n",
        "            if os.path.exists(output_sdf):\n",
        "                with open(output_sdf, 'r') as f:\n",
        "                    content = f.read()\n",
        "                    \n",
        "                # Extract scores from SDF data\n",
        "                lines = content.split('\\\\n')\n",
        "                for i, line in enumerate(lines):\n",
        "                    if 'CNNscore' in line:\n",
        "                        try:\n",
        "                            cnn_score = float(line.split()[-1])\n",
        "                            scores.append({\n",
        "                                'pose_id': len(scores) + 1,\n",
        "                                'cnn_score': cnn_score,\n",
        "                                'cnn_affinity': cnn_score  # Simplified\n",
        "                            })\n",
        "                        except:\n",
        "                            continue\n",
        "            \n",
        "            # Parse log file for additional information\n",
        "            if os.path.exists(log_file):\n",
        "                with open(log_file, 'r') as f:\n",
        "                    log_content = f.read()\n",
        "                    \n",
        "                # Extract timing and other metrics\n",
        "                if 'Total time' in log_content:\n",
        "                    # Add timing information if available\n",
        "                    pass\n",
        "                    \n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error parsing output for {output_sdf}: {e}\")\n",
        "        \n",
        "        return scores\n",
        "    \n",
        "    def run_parallel_docking(self, pairlist_df, flexible_residues_dict=None, batch_size=5):\n",
        "        \"\"\"Run docking with parallel processing\"\"\"\n",
        "        self.total_pairs = len(pairlist_df)\n",
        "        print(f\"üöÄ Starting parallel docking: {self.total_pairs} pairs\")\n",
        "        print(f\"   Max workers: {self.max_workers}\")\n",
        "        print(f\"   Batch size: {batch_size}\")\n",
        "        \n",
        "        # Load completed pairs for resume capability\n",
        "        self.load_completed_pairs()\n",
        "        \n",
        "        # Split into batches\n",
        "        batches = [pairlist_df.iloc[i:i+batch_size] for i in range(0, self.total_pairs, batch_size)]\n",
        "        \n",
        "        all_results = []\n",
        "        successful = 0\n",
        "        failed = 0\n",
        "        \n",
        "        # Process batches in parallel\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
        "            # Submit all batches\n",
        "            future_to_batch = {\n",
        "                executor.submit(self._process_batch, batch, batch_idx, flexible_residues_dict): batch_idx \n",
        "                for batch_idx, batch in enumerate(batches)\n",
        "            }\n",
        "            \n",
        "            # Collect results as they complete\n",
        "            for future in tqdm(concurrent.futures.as_completed(future_to_batch), \n",
        "                             total=len(batches), desc=\"Processing batches\"):\n",
        "                batch_idx = future_to_batch[future]\n",
        "                try:\n",
        "                    batch_results = future.result()\n",
        "                    all_results.extend(batch_results)\n",
        "                    \n",
        "                    # Count successes/failures\n",
        "                    for result in batch_results:\n",
        "                        if result and result['status'] == 'success':\n",
        "                            successful += 1\n",
        "                            self.completed_pairs.add(result['pair_id'])\n",
        "                        elif result:\n",
        "                            failed += 1\n",
        "                    \n",
        "                    # Save progress\n",
        "                    self.save_docking_state(all_results)\n",
        "                            \n",
        "                except Exception as e:\n",
        "                    print(f\"‚ùå Batch {batch_idx} failed: {e}\")\n",
        "                    failed += len(batches[batch_idx])\n",
        "        \n",
        "        # Final summary\n",
        "        print(f\"\\\\nüìä Parallel Docking Summary:\")\n",
        "        print(f\"   Total pairs: {self.total_pairs}\")\n",
        "        print(f\"   Successful: {successful}\")\n",
        "        print(f\"   Failed: {failed}\")\n",
        "        print(f\"   Success rate: {successful/self.total_pairs*100:.1f}%\")\n",
        "        \n",
        "        self.results = all_results\n",
        "        return all_results\n",
        "    \n",
        "    def _process_batch(self, batch_df, batch_idx, flexible_residues_dict):\n",
        "        \"\"\"Process a single batch of docking runs\"\"\"\n",
        "        batch_results = []\n",
        "        \n",
        "        for idx, row in batch_df.iterrows():\n",
        "            # Get flexible residues for this receptor\n",
        "            flexible_residues = None\n",
        "            if flexible_residues_dict and row['receptor'] in flexible_residues_dict:\n",
        "                flexible_residues = flexible_residues_dict[row['receptor']]\n",
        "            \n",
        "            result = self.run_single_docking(row, flexible_residues)\n",
        "            if result:\n",
        "                result['batch_idx'] = batch_idx\n",
        "                batch_results.append(result)\n",
        "                \n",
        "                # Update progress\n",
        "                with self.progress_lock:\n",
        "                    status = \"‚úÖ\" if result['status'] == 'success' else \"‚ùå\"\n",
        "                    flex_info = \" (Flex)\" if flexible_residues else \"\"\n",
        "                    print(f\"   {status} Batch {batch_idx}: {row['receptor']}-{row['ligand']}{flex_info}\")\n",
        "        \n",
        "        return batch_results\n",
        "    \n",
        "    def save_docking_state(self, results):\n",
        "        \"\"\"Save docking state for resume capability\"\"\"\n",
        "        state = {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'total_pairs': self.total_pairs,\n",
        "            'completed_pairs': list(self.completed_pairs),\n",
        "            'results_count': len(results),\n",
        "            'config': self.config.active_config\n",
        "        }\n",
        "        \n",
        "        with open(self.state_file, 'w') as f:\n",
        "            json.dump(state, f, indent=2)\n",
        "    \n",
        "    def load_docking_state(self):\n",
        "        \"\"\"Load docking state for resume capability\"\"\"\n",
        "        if os.path.exists(self.state_file):\n",
        "            with open(self.state_file, 'r') as f:\n",
        "                state = json.load(f)\n",
        "            \n",
        "            self.completed_pairs = set(state.get('completed_pairs', []))\n",
        "            self.total_pairs = state.get('total_pairs', 0)\n",
        "            \n",
        "            print(f\"‚úÖ Loaded docking state: {len(self.completed_pairs)} completed pairs\")\n",
        "            return True\n",
        "        return False\n",
        "    \n",
        "    def load_completed_pairs(self):\n",
        "        \"\"\"Load list of completed pairs from output files\"\"\"\n",
        "        if os.path.exists('gnina_out'):\n",
        "            for file in os.listdir('gnina_out'):\n",
        "                if file.endswith('_poses.sdf'):\n",
        "                    # Extract pair ID from filename\n",
        "                    pair_id = file.replace('_poses.sdf', '')\n",
        "                    self.completed_pairs.add(pair_id)\n",
        "        \n",
        "        print(f\"‚úÖ Found {len(self.completed_pairs)} previously completed pairs\")\n",
        "\n",
        "# Initialize docking engine\n",
        "docking_engine = GNINADockingEngine(docking_config, max_workers=4)\n",
        "\n",
        "print(\"‚úÖ Core GNINA Docking Engine initialized\")\n",
        "print(f\"   Max workers: {docking_engine.max_workers}\")\n",
        "print(f\"   Resume capability: Enabled\")\n",
        "print(f\"   State file: {docking_engine.state_file}\")\n",
        "\n",
        "print(\"\\\\nüîß Usage Examples:\")\n",
        "print(\"   # Run parallel docking\")\n",
        "print(\"   results = docking_engine.run_parallel_docking(pairlist_df)\")\n",
        "print(\"   \")\n",
        "print(\"   # Run with flexible residues\")\n",
        "print(\"   flex_dict = {'receptor1': ['A:123', 'A:124']}\")\n",
        "print(\"   results = docking_engine.run_parallel_docking(pairlist_df, flex_dict)\")\n",
        "print(\"   \")\n",
        "print(\"   # Load previous state\")\n",
        "print(\"   docking_engine.load_docking_state()\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Flexible Receptor Docking Option\n",
        "\n",
        "Configure flexible receptor docking by defining which residues should be flexible during docking. This is an optional enhancement to the core docking engine.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Flexible Receptor Docking Configuration\n",
        "# Based on GNINA documentation: https://github.com/gnina/gnina\n",
        "# =============================================================================\n",
        "\n",
        "class FlexibleReceptorManager:\n",
        "    \"\"\"Manage flexible receptor configurations for GNINA docking\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.flexible_residues = {}\n",
        "        self.flexible_config = {\n",
        "            'auto_detect': True,\n",
        "            'distance_threshold': 5.0,\n",
        "            'max_flexible_residues': 20,\n",
        "            'flexdist': 3.5\n",
        "        }\n",
        "    \n",
        "    def auto_detect_flexible_residues(self, receptor, binding_center, distance_threshold=5.0):\n",
        "        \"\"\"Auto-detect flexible residues near binding site\"\"\"\n",
        "        try:\n",
        "            from Bio.PDB import PDBParser, NeighborSearch\n",
        "            from Bio.PDB.PDBExceptions import PDBConstructionWarning\n",
        "            import warnings\n",
        "            warnings.simplefilter('ignore', PDBConstructionWarning)\n",
        "            \n",
        "            receptor_pdb = f\"receptors_prep/{receptor}.pdbqt\"\n",
        "            if not os.path.exists(receptor_pdb):\n",
        "                print(f\"‚ö†Ô∏è Receptor file not found: {receptor_pdb}\")\n",
        "                return []\n",
        "            \n",
        "            parser = PDBParser(QUIET=True)\n",
        "            structure = parser.get_structure('receptor', receptor_pdb)\n",
        "            \n",
        "            # Get all atoms\n",
        "            atoms = []\n",
        "            for model in structure:\n",
        "                for chain in model:\n",
        "                    for residue in chain:\n",
        "                        for atom in residue:\n",
        "                            atoms.append(atom)\n",
        "            \n",
        "            # Create neighbor search\n",
        "            ns = NeighborSearch(atoms)\n",
        "            \n",
        "            # Find residues within distance of binding center\n",
        "            center_atom = None\n",
        "            min_distance = float('inf')\n",
        "            \n",
        "            for atom in atoms:\n",
        "                dist = atom.coord - np.array(binding_center)\n",
        "                dist = np.linalg.norm(dist)\n",
        "                if dist < min_distance:\n",
        "                    min_distance = dist\n",
        "                    center_atom = atom\n",
        "            \n",
        "            if center_atom is None:\n",
        "                return []\n",
        "            \n",
        "            # Find neighbors within threshold\n",
        "            neighbors = ns.search(center_atom.coord, distance_threshold, level='R')\n",
        "            \n",
        "            flexible_residues = []\n",
        "            for residue in neighbors:\n",
        "                chain_id = residue.parent.id\n",
        "                res_num = residue.id[1]\n",
        "                flexible_residues.append(f\"{chain_id}:{res_num}\")\n",
        "            \n",
        "            # Apply limits\n",
        "            if len(flexible_residues) > self.flexible_config['max_flexible_residues']:\n",
        "                flexible_residues = flexible_residues[:self.flexible_config['max_flexible_residues']]\n",
        "                print(f\"‚ö†Ô∏è Limited flexible residues to {self.flexible_config['max_flexible_residues']}\")\n",
        "            \n",
        "            return sorted(flexible_residues)\n",
        "            \n",
        "        except ImportError:\n",
        "            print(\"‚ö†Ô∏è BioPython not available for auto-detection\")\n",
        "            return []\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error in auto-detection: {e}\")\n",
        "            return []\n",
        "    \n",
        "    def set_flexible_residues(self, receptor, flexible_residues=None, auto_detect=True, binding_center=None):\n",
        "        \"\"\"Set flexible residues for a receptor\"\"\"\n",
        "        if flexible_residues is not None:\n",
        "            # Manual specification\n",
        "            self.flexible_residues[receptor] = flexible_residues\n",
        "            print(f\"‚úÖ Set manual flexible residues for {receptor}: {flexible_residues}\")\n",
        "            \n",
        "        elif auto_detect and binding_center is not None:\n",
        "            # Auto-detection\n",
        "            detected = self.auto_detect_flexible_residues(receptor, binding_center)\n",
        "            self.flexible_residues[receptor] = detected\n",
        "            print(f\"‚úÖ Auto-detected {len(detected)} flexible residues for {receptor}: {detected}\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è No flexible residues set for {receptor}\")\n",
        "            self.flexible_residues[receptor] = []\n",
        "    \n",
        "    def set_bulk_flexibility(self, pairlist_df, auto_detect=True):\n",
        "        \"\"\"Set flexible residues for all receptors in pairlist\"\"\"\n",
        "        print(\"üîÑ Configuring flexible residues for all receptors...\")\n",
        "        \n",
        "        for _, row in pairlist_df.iterrows():\n",
        "            receptor = row['receptor']\n",
        "            if receptor not in self.flexible_residues:\n",
        "                binding_center = [row['center_x'], row['center_y'], row['center_z']]\n",
        "                self.set_flexible_residues(receptor, auto_detect=auto_detect, binding_center=binding_center)\n",
        "    \n",
        "    def get_flexible_residues_dict(self):\n",
        "        \"\"\"Get dictionary of flexible residues for all receptors\"\"\"\n",
        "        return self.flexible_residues.copy()\n",
        "    \n",
        "    def display_flexibility_summary(self):\n",
        "        \"\"\"Display summary of flexible receptor configuration\"\"\"\n",
        "        print(\"üîÑ Flexible Receptor Configuration Summary:\")\n",
        "        for receptor, residues in self.flexible_residues.items():\n",
        "            print(f\"   {receptor}: {len(residues)} flexible residues\")\n",
        "            if residues:\n",
        "                print(f\"      {', '.join(residues[:5])}{'...' if len(residues) > 5 else ''}\")\n",
        "\n",
        "# Initialize flexible receptor manager\n",
        "flexible_manager = FlexibleReceptorManager()\n",
        "\n",
        "print(\"‚úÖ Flexible Receptor Manager initialized\")\n",
        "\n",
        "print(\"\\\\nüîß Usage Examples:\")\n",
        "print(\"   # Auto-detect flexible residues for all receptors\")\n",
        "print(\"   flexible_manager.set_bulk_flexibility(pairlist_df, auto_detect=True)\")\n",
        "print(\"   \")\n",
        "print(\"   # Manual specification\")\n",
        "print(\"   flexible_manager.set_flexible_residues('receptor1', ['A:123', 'A:124', 'A:125'])\")\n",
        "print(\"   \")\n",
        "print(\"   # Get flexible residues dictionary\")\n",
        "print(\"   flex_dict = flexible_manager.get_flexible_residues_dict()\")\n",
        "print(\"   \")\n",
        "print(\"   # Run docking with flexible receptors\")\n",
        "print(\"   results = docking_engine.run_parallel_docking(pairlist_df, flex_dict)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Post-Docking Analysis: Visualization and Quality Control\n",
        "\n",
        "Comprehensive analysis tools for docking results including visualization dashboards and quality control validation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Post-Docking Analysis: Visualization and Quality Control\n",
        "# =============================================================================\n",
        "\n",
        "class DockingAnalysisDashboard:\n",
        "    \"\"\"Comprehensive analysis dashboard for docking results\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.setup_plotting_style()\n",
        "    \n",
        "    def setup_plotting_style(self):\n",
        "        \"\"\"Setup plotting style for visualizations\"\"\"\n",
        "        plt.style.use('default')\n",
        "        sns.set_palette(\"husl\")\n",
        "        \n",
        "    def create_score_distribution_plot(self, results):\n",
        "        \"\"\"Create score distribution visualization\"\"\"\n",
        "        successful_results = [r for r in results if r['status'] == 'success']\n",
        "        \n",
        "        if not successful_results:\n",
        "            print(\"‚ö†Ô∏è No successful results to visualize\")\n",
        "            return\n",
        "        \n",
        "        # Extract all CNN scores\n",
        "        all_scores = []\n",
        "        for result in successful_results:\n",
        "            if 'scores' in result:\n",
        "                for score_data in result['scores']:\n",
        "                    if 'cnn_score' in score_data:\n",
        "                        all_scores.append(score_data['cnn_score'])\n",
        "        \n",
        "        if not all_scores:\n",
        "            print(\"‚ö†Ô∏è No CNN scores found in results\")\n",
        "            return\n",
        "        \n",
        "        # Create visualization\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        fig.suptitle('Docking Results Analysis', fontsize=16)\n",
        "        \n",
        "        # Score distribution histogram\n",
        "        axes[0, 0].hist(all_scores, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "        axes[0, 0].set_title('CNN Score Distribution')\n",
        "        axes[0, 0].set_xlabel('CNN Score')\n",
        "        axes[0, 0].set_ylabel('Frequency')\n",
        "        axes[0, 0].axvline(np.mean(all_scores), color='red', linestyle='--', label=f'Mean: {np.mean(all_scores):.3f}')\n",
        "        axes[0, 0].legend()\n",
        "        \n",
        "        # Box plot by receptor\n",
        "        receptor_scores = {}\n",
        "        for result in successful_results:\n",
        "            receptor = result['receptor']\n",
        "            if receptor not in receptor_scores:\n",
        "                receptor_scores[receptor] = []\n",
        "            if 'scores' in result:\n",
        "                for score_data in result['scores']:\n",
        "                    if 'cnn_score' in score_data:\n",
        "                        receptor_scores[receptor].append(score_data['cnn_score'])\n",
        "        \n",
        "        if receptor_scores:\n",
        "            receptor_data = [scores for scores in receptor_scores.values() if scores]\n",
        "            receptor_labels = [receptor for receptor, scores in receptor_scores.items() if scores]\n",
        "            \n",
        "            axes[0, 1].boxplot(receptor_data, labels=receptor_labels)\n",
        "            axes[0, 1].set_title('CNN Scores by Receptor')\n",
        "            axes[0, 1].set_ylabel('CNN Score')\n",
        "            axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "        \n",
        "        # Success rate by receptor\n",
        "        receptor_stats = {}\n",
        "        for result in results:\n",
        "            receptor = result['receptor']\n",
        "            if receptor not in receptor_stats:\n",
        "                receptor_stats[receptor] = {'total': 0, 'success': 0}\n",
        "            receptor_stats[receptor]['total'] += 1\n",
        "            if result['status'] == 'success':\n",
        "                receptor_stats[receptor]['success'] += 1\n",
        "        \n",
        "        receptors = list(receptor_stats.keys())\n",
        "        success_rates = [receptor_stats[r]['success']/receptor_stats[r]['total']*100 for r in receptors]\n",
        "        \n",
        "        axes[1, 0].bar(receptors, success_rates, color='lightgreen', alpha=0.7)\n",
        "        axes[1, 0].set_title('Success Rate by Receptor')\n",
        "        axes[1, 0].set_ylabel('Success Rate (%)')\n",
        "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "        \n",
        "        # Top scoring ligands\n",
        "        top_ligands = []\n",
        "        for result in successful_results:\n",
        "            if 'scores' in result:\n",
        "                max_score = max([s.get('cnn_score', 0) for s in result['scores']], default=0)\n",
        "                top_ligands.append({\n",
        "                    'ligand': result['ligand'],\n",
        "                    'receptor': result['receptor'],\n",
        "                    'max_score': max_score\n",
        "                })\n",
        "        \n",
        "        if top_ligands:\n",
        "            top_ligands = sorted(top_ligands, key=lambda x: x['max_score'], reverse=True)[:10]\n",
        "            ligand_names = [f\"{l['ligand']}\\\\n({l['receptor']})\" for l in top_ligands]\n",
        "            scores = [l['max_score'] for l in top_ligands]\n",
        "            \n",
        "            axes[1, 1].barh(range(len(ligand_names)), scores, color='orange', alpha=0.7)\n",
        "            axes[1, 1].set_yticks(range(len(ligand_names)))\n",
        "            axes[1, 1].set_yticklabels(ligand_names, fontsize=8)\n",
        "            axes[1, 1].set_title('Top 10 Scoring Ligands')\n",
        "            axes[1, 1].set_xlabel('Max CNN Score')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig('visualizations/docking_analysis.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        \n",
        "        # Print summary statistics\n",
        "        print(\"\\\\nüìä Docking Results Summary:\")\n",
        "        print(f\"   Total results: {len(results)}\")\n",
        "        print(f\"   Successful: {len(successful_results)}\")\n",
        "        print(f\"   Failed: {len(results) - len(successful_results)}\")\n",
        "        print(f\"   Success rate: {len(successful_results)/len(results)*100:.1f}%\")\n",
        "        print(f\"   CNN scores - Mean: {np.mean(all_scores):.3f}, Std: {np.std(all_scores):.3f}\")\n",
        "        print(f\"   CNN scores - Min: {np.min(all_scores):.3f}, Max: {np.max(all_scores):.3f}\")\n",
        "    \n",
        "    def create_interactive_dashboard(self, results):\n",
        "        \"\"\"Create interactive Plotly dashboard\"\"\"\n",
        "        successful_results = [r for r in results if r['status'] == 'success']\n",
        "        \n",
        "        if not successful_results:\n",
        "            print(\"‚ö†Ô∏è No successful results for interactive dashboard\")\n",
        "            return\n",
        "        \n",
        "        # Prepare data\n",
        "        dashboard_data = []\n",
        "        for result in successful_results:\n",
        "            if 'scores' in result:\n",
        "                for i, score_data in enumerate(result['scores']):\n",
        "                    if 'cnn_score' in score_data:\n",
        "                        dashboard_data.append({\n",
        "                            'Receptor': result['receptor'],\n",
        "                            'Ligand': result['ligand'],\n",
        "                            'Site_ID': result['site_id'],\n",
        "                            'Pose_ID': i + 1,\n",
        "                            'CNN_Score': score_data['cnn_score'],\n",
        "                            'CNN_Affinity': score_data.get('cnn_affinity', score_data['cnn_score'])\n",
        "                        })\n",
        "        \n",
        "        if not dashboard_data:\n",
        "            print(\"‚ö†Ô∏è No score data for interactive dashboard\")\n",
        "            return\n",
        "        \n",
        "        df = pd.DataFrame(dashboard_data)\n",
        "        \n",
        "        # Create subplots\n",
        "        fig = make_subplots(\n",
        "            rows=2, cols=2,\n",
        "            subplot_titles=('CNN Score Distribution', 'Scores by Receptor', \n",
        "                          'Top Ligands', 'Score vs Affinity'),\n",
        "            specs=[[{\"type\": \"histogram\"}, {\"type\": \"box\"}],\n",
        "                   [{\"type\": \"bar\"}, {\"type\": \"scatter\"}]]\n",
        "        )\n",
        "        \n",
        "        # Score distribution\n",
        "        fig.add_trace(\n",
        "            go.Histogram(x=df['CNN_Score'], name='CNN Score Distribution', nbinsx=30),\n",
        "            row=1, col=1\n",
        "        )\n",
        "        \n",
        "        # Box plot by receptor\n",
        "        for receptor in df['Receptor'].unique():\n",
        "            receptor_data = df[df['Receptor'] == receptor]['CNN_Score']\n",
        "            fig.add_trace(\n",
        "                go.Box(y=receptor_data, name=receptor),\n",
        "                row=1, col=2\n",
        "            )\n",
        "        \n",
        "        # Top ligands\n",
        "        top_ligands = df.groupby('Ligand')['CNN_Score'].max().nlargest(10)\n",
        "        fig.add_trace(\n",
        "            go.Bar(x=top_ligands.index, y=top_ligands.values, name='Top Ligands'),\n",
        "            row=2, col=1\n",
        "        )\n",
        "        \n",
        "        # Score vs Affinity\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=df['CNN_Score'], y=df['CNN_Affinity'], \n",
        "                      mode='markers', name='Score vs Affinity',\n",
        "                      text=df['Ligand'], hovertemplate='Ligand: %{text}<br>Score: %{x}<br>Affinity: %{y}'),\n",
        "            row=2, col=2\n",
        "        )\n",
        "        \n",
        "        fig.update_layout(height=800, showlegend=False, title_text=\"Interactive Docking Analysis Dashboard\")\n",
        "        fig.show()\n",
        "        \n",
        "        # Save as HTML\n",
        "        fig.write_html('visualizations/interactive_dashboard.html')\n",
        "        print(\"‚úÖ Interactive dashboard saved to visualizations/interactive_dashboard.html\")\n",
        "\n",
        "class QualityControlValidator:\n",
        "    \"\"\"Quality control validation for docking results\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.validation_results = {}\n",
        "    \n",
        "    def validate_docking_results(self, results):\n",
        "        \"\"\"Comprehensive validation of docking results\"\"\"\n",
        "        print(\"üîç Validating docking results...\")\n",
        "        \n",
        "        validation_results = {\n",
        "            'total_results': len(results),\n",
        "            'successful_dockings': 0,\n",
        "            'failed_dockings': 0,\n",
        "            'quality_issues': [],\n",
        "            'score_distribution': {},\n",
        "            'receptor_analysis': {},\n",
        "            'recommendations': []\n",
        "        }\n",
        "        \n",
        "        successful_results = [r for r in results if r['status'] == 'success']\n",
        "        validation_results['successful_dockings'] = len(successful_results)\n",
        "        validation_results['failed_dockings'] = len(results) - len(successful_results)\n",
        "        \n",
        "        if successful_results:\n",
        "            # Analyze score distribution\n",
        "            all_scores = []\n",
        "            for result in successful_results:\n",
        "                if 'scores' in result:\n",
        "                    for score_data in result['scores']:\n",
        "                        if 'cnn_score' in score_data:\n",
        "                            all_scores.append(score_data['cnn_score'])\n",
        "            \n",
        "            if all_scores:\n",
        "                validation_results['score_distribution'] = {\n",
        "                    'mean': np.mean(all_scores),\n",
        "                    'std': np.std(all_scores),\n",
        "                    'min': np.min(all_scores),\n",
        "                    'max': np.max(all_scores),\n",
        "                    'median': np.median(all_scores),\n",
        "                    'q25': np.percentile(all_scores, 25),\n",
        "                    'q75': np.percentile(all_scores, 75)\n",
        "                }\n",
        "                \n",
        "                # Quality checks\n",
        "                if np.mean(all_scores) < 0.3:\n",
        "                    validation_results['quality_issues'].append(\"Low average CNN scores (< 0.3)\")\n",
        "                    validation_results['recommendations'].append(\"Consider increasing exhaustiveness or using refinement mode\")\n",
        "                \n",
        "                if np.std(all_scores) < 0.1:\n",
        "                    validation_results['quality_issues'].append(\"Low score variance - possible convergence issues\")\n",
        "                    validation_results['recommendations'].append(\"Check binding site coordinates and box size\")\n",
        "                \n",
        "                if np.max(all_scores) < 0.5:\n",
        "                    validation_results['quality_issues'].append(\"No high-scoring poses found\")\n",
        "                    validation_results['recommendations'].append(\"Review ligand preparation and binding site definition\")\n",
        "        \n",
        "        # Receptor-specific analysis\n",
        "        receptor_stats = {}\n",
        "        for result in results:\n",
        "            receptor = result['receptor']\n",
        "            if receptor not in receptor_stats:\n",
        "                receptor_stats[receptor] = {'total': 0, 'success': 0, 'scores': []}\n",
        "            receptor_stats[receptor]['total'] += 1\n",
        "            if result['status'] == 'success':\n",
        "                receptor_stats[receptor]['success'] += 1\n",
        "                if 'scores' in result:\n",
        "                    for score_data in result['scores']:\n",
        "                        if 'cnn_score' in score_data:\n",
        "                            receptor_stats[receptor]['scores'].append(score_data['cnn_score'])\n",
        "        \n",
        "        for receptor, stats in receptor_stats.items():\n",
        "            success_rate = stats['success'] / stats['total'] * 100\n",
        "            validation_results['receptor_analysis'][receptor] = {\n",
        "                'total_pairs': stats['total'],\n",
        "                'successful_pairs': stats['success'],\n",
        "                'success_rate': success_rate,\n",
        "                'avg_score': np.mean(stats['scores']) if stats['scores'] else 0,\n",
        "                'max_score': np.max(stats['scores']) if stats['scores'] else 0\n",
        "            }\n",
        "            \n",
        "            if success_rate < 50:\n",
        "                validation_results['quality_issues'].append(f\"Low success rate for {receptor} ({success_rate:.1f}%)\")\n",
        "                validation_results['recommendations'].append(f\"Check receptor preparation for {receptor}\")\n",
        "        \n",
        "        # Overall success rate check\n",
        "        overall_success_rate = validation_results['successful_dockings'] / validation_results['total_results'] * 100\n",
        "        if overall_success_rate < 70:\n",
        "            validation_results['quality_issues'].append(f\"Low overall success rate ({overall_success_rate:.1f}%)\")\n",
        "            validation_results['recommendations'].append(\"Review input structures and docking parameters\")\n",
        "        \n",
        "        self.validation_results = validation_results\n",
        "        return validation_results\n",
        "    \n",
        "    def generate_quality_report(self):\n",
        "        \"\"\"Generate comprehensive quality control report\"\"\"\n",
        "        if not self.validation_results:\n",
        "            print(\"‚ö†Ô∏è No validation results available. Run validate_docking_results() first.\")\n",
        "            return\n",
        "        \n",
        "        print(\"\\\\nüìä Quality Control Report:\")\n",
        "        print(f\"   Total Results: {self.validation_results['total_results']}\")\n",
        "        print(f\"   Successful: {self.validation_results['successful_dockings']}\")\n",
        "        print(f\"   Failed: {self.validation_results['failed_dockings']}\")\n",
        "        print(f\"   Success Rate: {self.validation_results['successful_dockings']/self.validation_results['total_results']*100:.1f}%\")\n",
        "        \n",
        "        if self.validation_results['score_distribution']:\n",
        "            dist = self.validation_results['score_distribution']\n",
        "            print(f\"\\\\n   Score Distribution:\")\n",
        "            print(f\"     Mean: {dist['mean']:.3f}\")\n",
        "            print(f\"     Std: {dist['std']:.3f}\")\n",
        "            print(f\"     Range: {dist['min']:.3f} - {dist['max']:.3f}\")\n",
        "            print(f\"     Median: {dist['median']:.3f}\")\n",
        "        \n",
        "        if self.validation_results['quality_issues']:\n",
        "            print(f\"\\\\n   ‚ö†Ô∏è Quality Issues ({len(self.validation_results['quality_issues'])}):\")\n",
        "            for issue in self.validation_results['quality_issues']:\n",
        "                print(f\"     - {issue}\")\n",
        "        \n",
        "        if self.validation_results['recommendations']:\n",
        "            print(f\"\\\\n   üí° Recommendations ({len(self.validation_results['recommendations'])}):\")\n",
        "            for rec in self.validation_results['recommendations']:\n",
        "                print(f\"     - {rec}\")\n",
        "        \n",
        "        # Save report\n",
        "        with open('enhanced_analysis/quality_control_report.json', 'w') as f:\n",
        "            json.dump(self.validation_results, f, indent=2)\n",
        "        \n",
        "        print(\"\\\\n‚úÖ Quality control report saved to enhanced_analysis/quality_control_report.json\")\n",
        "\n",
        "# Initialize analysis tools\n",
        "analysis_dashboard = DockingAnalysisDashboard()\n",
        "qc_validator = QualityControlValidator()\n",
        "\n",
        "print(\"‚úÖ Post-docking analysis tools initialized\")\n",
        "\n",
        "print(\"\\\\nüîß Usage Examples:\")\n",
        "print(\"   # Create visualizations\")\n",
        "print(\"   analysis_dashboard.create_score_distribution_plot(results)\")\n",
        "print(\"   analysis_dashboard.create_interactive_dashboard(results)\")\n",
        "print(\"   \")\n",
        "print(\"   # Quality control validation\")\n",
        "print(\"   qc_validator.validate_docking_results(results)\")\n",
        "print(\"   qc_validator.generate_quality_report()\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Covalent Docking Configuration\n",
        "\n",
        "Specialized configuration for covalent docking using GNINA. This follows the same procedure as standard docking but with covalent bond formation parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Covalent Docking Configuration\n",
        "# Based on GNINA documentation: https://github.com/gnina/gnina\n",
        "# =============================================================================\n",
        "\n",
        "class CovalentDockingConfig:\n",
        "    \"\"\"Configuration for covalent docking with GNINA\"\"\"\n",
        "    \n",
        "    def __init__(self, base_docking_config):\n",
        "        self.base_config = base_docking_config\n",
        "        self.covalent_config = {\n",
        "            'covalent_docking': True,\n",
        "            'covalent_residue': None,  # e.g., 'A:123'\n",
        "            'covalent_atom': None,     # e.g., 'SG' for cysteine\n",
        "            'covalent_bond_type': 'single',  # single, double, triple\n",
        "            'covalent_bond_length': 1.8,     # Angstroms\n",
        "            'covalent_bond_angle': 109.5,    # Degrees\n",
        "            'covalent_torsion': 0.0,         # Degrees\n",
        "            'covalent_energy_penalty': 0.0,  # Energy penalty for covalent bond\n",
        "        }\n",
        "        \n",
        "        # Covalent-specific parameters\n",
        "        self.covalent_parameters = {\n",
        "            'exhaustiveness': 64,      # Higher for covalent docking\n",
        "            'num_modes': 30,          # More modes for covalent poses\n",
        "            'cnn_scoring': 'refinement',  # Use refinement for better accuracy\n",
        "            'min_rmsd_filter': 0.5,   # Stricter RMSD filter\n",
        "            'pose_sort_order': 0,     # Sort by CNN score\n",
        "        }\n",
        "    \n",
        "    def set_covalent_residue(self, residue_id, atom_name='SG'):\n",
        "        \"\"\"Set the covalent residue and atom\"\"\"\n",
        "        self.covalent_config['covalent_residue'] = residue_id\n",
        "        self.covalent_config['covalent_atom'] = atom_name\n",
        "        print(f\"‚úÖ Covalent residue set: {residue_id}:{atom_name}\")\n",
        "    \n",
        "    def set_covalent_bond_parameters(self, bond_length=1.8, bond_angle=109.5, torsion=0.0):\n",
        "        \"\"\"Set covalent bond geometry parameters\"\"\"\n",
        "        self.covalent_config['covalent_bond_length'] = bond_length\n",
        "        self.covalent_config['covalent_bond_angle'] = bond_angle\n",
        "        self.covalent_config['covalent_torsion'] = torsion\n",
        "        print(f\"‚úÖ Covalent bond parameters set:\")\n",
        "        print(f\"   Bond length: {bond_length} √Ö\")\n",
        "        print(f\"   Bond angle: {bond_angle}¬∞\")\n",
        "        print(f\"   Torsion: {torsion}¬∞\")\n",
        "    \n",
        "    def get_covalent_gnina_command(self, row):\n",
        "        \"\"\"Get GNINA command with covalent docking parameters\"\"\"\n",
        "        receptor = f\"receptors_prep/{row['receptor']}.pdbqt\"\n",
        "        ligand = f\"ligands_prep/{row['ligand']}.pdbqt\"\n",
        "        \n",
        "        # Output files\n",
        "        tag = f\"{row['receptor']}_{row['site_id']}_{row['ligand']}_covalent\"\n",
        "        output_sdf = f\"gnina_out/{tag}_poses.sdf\"\n",
        "        log_file = f\"logs/{tag}.log\"\n",
        "        \n",
        "        # Base command\n",
        "        cmd = [\n",
        "            './gnina',\n",
        "            \"--receptor\", receptor,\n",
        "            \"--ligand\", ligand,\n",
        "            \"--out\", output_sdf,\n",
        "            \"--log\", log_file,\n",
        "        ]\n",
        "        \n",
        "        # Docking box parameters\n",
        "        cmd.extend([\n",
        "            \"--center_x\", str(row['center_x']),\n",
        "            \"--center_y\", str(row['center_y']),\n",
        "            \"--center_z\", str(row['center_z']),\n",
        "            \"--size_x\", str(row['size_x']),\n",
        "            \"--size_y\", str(row['size_y']),\n",
        "            \"--size_z\", str(row['size_z']),\n",
        "        ])\n",
        "        \n",
        "        # Covalent-specific parameters\n",
        "        cmd.extend([\n",
        "            \"--exhaustiveness\", str(self.covalent_parameters['exhaustiveness']),\n",
        "            \"--num_modes\", str(self.covalent_parameters['num_modes']),\n",
        "            \"--seed\", str(self.base_config.active_config['seed']),\n",
        "            \"--cnn_scoring\", self.covalent_parameters['cnn_scoring'],\n",
        "            \"--min_rmsd_filter\", str(self.covalent_parameters['min_rmsd_filter']),\n",
        "            \"--pose_sort_order\", str(self.covalent_parameters['pose_sort_order']),\n",
        "        ])\n",
        "        \n",
        "        # GPU acceleration (prioritized over CPU)\n",
        "        if self.base_config.active_config.get('use_gpu', False) and CONFIG.get('use_gpu', False):\n",
        "            cmd.extend(['--gpu', '--device', str(self.base_config.active_config.get('gpu_device', 0))])\n",
        "        else:\n",
        "            # Fallback to CPU\n",
        "            cmd.extend(['--cpu', str(self.base_config.active_config['cpu_cores'])])\n",
        "        \n",
        "        # Covalent docking parameters\n",
        "        if self.covalent_config['covalent_residue']:\n",
        "            cmd.extend([\n",
        "                \"--covalent_residue\", self.covalent_config['covalent_residue'],\n",
        "                \"--covalent_atom\", self.covalent_config['covalent_atom'],\n",
        "                \"--covalent_bond_length\", str(self.covalent_config['covalent_bond_length']),\n",
        "                \"--covalent_bond_angle\", str(self.covalent_config['covalent_bond_angle']),\n",
        "                \"--covalent_torsion\", str(self.covalent_config['covalent_torsion']),\n",
        "            ])\n",
        "        \n",
        "        # GPU support already handled above\n",
        "        \n",
        "        return cmd, output_sdf, log_file\n",
        "    \n",
        "    def display_covalent_configuration(self):\n",
        "        \"\"\"Display current covalent docking configuration\"\"\"\n",
        "        print(\"üîó Covalent Docking Configuration:\")\n",
        "        print(f\"   Covalent residue: {self.covalent_config['covalent_residue']}\")\n",
        "        print(f\"   Covalent atom: {self.covalent_config['covalent_atom']}\")\n",
        "        print(f\"   Bond length: {self.covalent_config['covalent_bond_length']} √Ö\")\n",
        "        print(f\"   Bond angle: {self.covalent_config['covalent_bond_angle']}¬∞\")\n",
        "        print(f\"   Torsion: {self.covalent_config['covalent_torsion']}¬∞\")\n",
        "        print(f\"   Exhaustiveness: {self.covalent_parameters['exhaustiveness']}\")\n",
        "        print(f\"   Num modes: {self.covalent_parameters['num_modes']}\")\n",
        "        print(f\"   CNN scoring: {self.covalent_parameters['cnn_scoring']}\")\n",
        "\n",
        "class CovalentDockingEngine(GNINADockingEngine):\n",
        "    \"\"\"Covalent docking engine extending the base docking engine\"\"\"\n",
        "    \n",
        "    def __init__(self, covalent_config, max_workers=4):\n",
        "        super().__init__(covalent_config.base_config, max_workers)\n",
        "        self.covalent_config = covalent_config\n",
        "    \n",
        "    def run_covalent_docking(self, pairlist_df, covalent_residue, covalent_atom='SG'):\n",
        "        \"\"\"Run covalent docking for all pairs\"\"\"\n",
        "        print(f\"üîó Starting covalent docking with residue {covalent_residue}:{covalent_atom}\")\n",
        "        \n",
        "        # Set covalent parameters\n",
        "        self.covalent_config.set_covalent_residue(covalent_residue, covalent_atom)\n",
        "        \n",
        "        # Display configuration\n",
        "        self.covalent_config.display_covalent_configuration()\n",
        "        \n",
        "        # Run docking with covalent parameters\n",
        "        results = []\n",
        "        for idx, row in pairlist_df.iterrows():\n",
        "            print(f\"\\\\nüîó Processing covalent docking: {row['receptor']}-{row['ligand']}\")\n",
        "            \n",
        "            # Build covalent command\n",
        "            cmd, output_sdf, log_file = self.covalent_config.get_covalent_gnina_command(row)\n",
        "            \n",
        "            try:\n",
        "                # Run GNINA with covalent parameters\n",
        "                result = subprocess.run(\n",
        "                    cmd,\n",
        "                    capture_output=True,\n",
        "                    text=True,\n",
        "                    timeout=self.config.active_config['timeout']\n",
        "                )\n",
        "                \n",
        "                # Parse results\n",
        "                if result.returncode == 0 and os.path.exists(output_sdf):\n",
        "                    scores = self.parse_gnina_output(output_sdf, log_file)\n",
        "                    results.append({\n",
        "                        'status': 'success',\n",
        "                        'receptor': row['receptor'],\n",
        "                        'ligand': row['ligand'],\n",
        "                        'site_id': row['site_id'],\n",
        "                        'output_file': output_sdf,\n",
        "                        'log_file': log_file,\n",
        "                        'scores': scores,\n",
        "                        'covalent_residue': covalent_residue,\n",
        "                        'covalent_atom': covalent_atom,\n",
        "                        'docking_type': 'covalent'\n",
        "                    })\n",
        "                    print(f\"   ‚úÖ Success: {len(scores)} poses generated\")\n",
        "                else:\n",
        "                    results.append({\n",
        "                        'status': 'error',\n",
        "                        'receptor': row['receptor'],\n",
        "                        'ligand': row['ligand'],\n",
        "                        'site_id': row['site_id'],\n",
        "                        'error': result.stderr,\n",
        "                        'returncode': result.returncode,\n",
        "                        'docking_type': 'covalent'\n",
        "                    })\n",
        "                    print(f\"   ‚ùå Error: {result.stderr}\")\n",
        "                    \n",
        "            except subprocess.TimeoutExpired:\n",
        "                results.append({\n",
        "                    'status': 'timeout',\n",
        "                    'receptor': row['receptor'],\n",
        "                    'ligand': row['ligand'],\n",
        "                    'site_id': row['site_id'],\n",
        "                    'error': f\"Timeout after {self.config.active_config['timeout']}s\",\n",
        "                    'docking_type': 'covalent'\n",
        "                })\n",
        "                print(f\"   ‚è∞ Timeout\")\n",
        "            except Exception as e:\n",
        "                results.append({\n",
        "                    'status': 'error',\n",
        "                    'receptor': row['receptor'],\n",
        "                    'ligand': row['ligand'],\n",
        "                    'site_id': row['site_id'],\n",
        "                    'error': str(e),\n",
        "                    'docking_type': 'covalent'\n",
        "                })\n",
        "                print(f\"   ‚ùå Exception: {e}\")\n",
        "        \n",
        "        # Summary\n",
        "        successful = len([r for r in results if r['status'] == 'success'])\n",
        "        print(f\"\\\\nüìä Covalent Docking Summary:\")\n",
        "        print(f\"   Total pairs: {len(pairlist_df)}\")\n",
        "        print(f\"   Successful: {successful}\")\n",
        "        print(f\"   Failed: {len(pairlist_df) - successful}\")\n",
        "        print(f\"   Success rate: {successful/len(pairlist_df)*100:.1f}%\")\n",
        "        \n",
        "        return results\n",
        "\n",
        "# Initialize covalent docking\n",
        "covalent_config = CovalentDockingConfig(docking_config)\n",
        "covalent_engine = CovalentDockingEngine(covalent_config, max_workers=4)\n",
        "\n",
        "print(\"‚úÖ Covalent Docking Engine initialized\")\n",
        "\n",
        "print(\"\\\\nüîß Usage Examples:\")\n",
        "print(\"   # Set covalent residue (e.g., cysteine at position 123 in chain A)\")\n",
        "print(\"   covalent_config.set_covalent_residue('A:123', 'SG')\")\n",
        "print(\"   \")\n",
        "print(\"   # Set covalent bond parameters\")\n",
        "print(\"   covalent_config.set_covalent_bond_parameters(bond_length=1.8, bond_angle=109.5)\")\n",
        "print(\"   \")\n",
        "print(\"   # Run covalent docking\")\n",
        "print(\"   covalent_results = covalent_engine.run_covalent_docking(pairlist_df, 'A:123', 'SG')\")\n",
        "print(\"   \")\n",
        "print(\"   # Display configuration\")\n",
        "print(\"   covalent_config.display_covalent_configuration()\")\n",
        "\n",
        "print(\"\\\\nüí° Covalent Docking Notes:\")\n",
        "print(\"   ‚Ä¢ Requires reactive groups in ligands (e.g., Michael acceptors, electrophiles)\")\n",
        "print(\"   ‚Ä¢ Target residue must have reactive atom (e.g., Cys-SG, Lys-NZ, Ser-OG)\")\n",
        "print(\"   ‚Ä¢ Higher exhaustiveness recommended for better sampling\")\n",
        "print(\"   ‚Ä¢ Use refinement CNN scoring for better accuracy\")\n",
        "print(\"   ‚Ä¢ Consider flexible receptor for better pose quality\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Main Execution and Workflow Orchestration\n",
        "\n",
        "Execute the complete GNINA docking pipeline with all available options. This cell provides easy-to-use functions for running different workflow types.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Main Execution and Workflow Orchestration\n",
        "# =============================================================================\n",
        "\n",
        "def run_standard_docking(pairlist_df, use_flexible=False, cnn_mode='rescore'):\n",
        "    \"\"\"\n",
        "    Run standard GNINA docking workflow\n",
        "    \n",
        "    Args:\n",
        "        pairlist_df: DataFrame with receptor-ligand pairs and coordinates\n",
        "        use_flexible: Whether to use flexible receptor docking\n",
        "        cnn_mode: CNN scoring mode ('none', 'rescore', 'refinement', 'all')\n",
        "    \"\"\"\n",
        "    print(f\"üöÄ Starting Standard Docking Workflow\")\n",
        "    print(f\"   Flexible receptors: {'Enabled' if use_flexible else 'Disabled'}\")\n",
        "    print(f\"   CNN mode: {cnn_mode}\")\n",
        "    \n",
        "    # Configure CNN mode\n",
        "    docking_config.set_cnn_mode(cnn_mode)\n",
        "    \n",
        "    # Configure flexible receptors if requested\n",
        "    flexible_residues_dict = None\n",
        "    if use_flexible:\n",
        "        flexible_manager.set_bulk_flexibility(pairlist_df, auto_detect=True)\n",
        "        flexible_residues_dict = flexible_manager.get_flexible_residues_dict()\n",
        "        flexible_manager.display_flexibility_summary()\n",
        "    \n",
        "    # Run docking\n",
        "    results = docking_engine.run_parallel_docking(pairlist_df, flexible_residues_dict)\n",
        "    \n",
        "    # Generate analysis\n",
        "    print(\"\\\\nüìä Generating analysis...\")\n",
        "    analysis_dashboard.create_score_distribution_plot(results)\n",
        "    qc_validator.validate_docking_results(results)\n",
        "    qc_validator.generate_quality_report()\n",
        "    \n",
        "    return results\n",
        "\n",
        "def run_tiered_workflow(pairlist_df, stages=['A', 'B'], use_flexible=False):\n",
        "    \"\"\"\n",
        "    Run tiered CNN scoring workflow\n",
        "    \n",
        "    Args:\n",
        "        pairlist_df: DataFrame with receptor-ligand pairs and coordinates\n",
        "        stages: List of stages to run ['A', 'B', 'C']\n",
        "        use_flexible: Whether to use flexible receptor docking\n",
        "    \"\"\"\n",
        "    print(f\"üéØ Starting Tiered CNN Workflow\")\n",
        "    print(f\"   Stages: {stages}\")\n",
        "    print(f\"   Flexible receptors: {'Enabled' if use_flexible else 'Disabled'}\")\n",
        "    \n",
        "    # Configure flexible receptors if requested\n",
        "    flexible_residues_dict = None\n",
        "    if use_flexible:\n",
        "        flexible_manager.set_bulk_flexibility(pairlist_df, auto_detect=True)\n",
        "        flexible_residues_dict = flexible_manager.get_flexible_residues_dict()\n",
        "    \n",
        "    all_results = []\n",
        "    previous_results = None\n",
        "    \n",
        "    for stage in stages:\n",
        "        print(f\"\\\\n{'='*60}\")\n",
        "        print(f\"STAGE {stage}: {tiered_workflow.get_stage_config(stage)['name']}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        # Get input for this stage\n",
        "        stage_input = tiered_workflow.run_stage(stage, pairlist_df, previous_results)\n",
        "        \n",
        "        if len(stage_input) == 0:\n",
        "            print(f\"‚ö†Ô∏è No ligands to process for Stage {stage}\")\n",
        "            continue\n",
        "        \n",
        "        # Run docking for this stage\n",
        "        stage_results = docking_engine.run_parallel_docking(stage_input, flexible_residues_dict)\n",
        "        \n",
        "        # Add stage information\n",
        "        for result in stage_results:\n",
        "            result['stage'] = stage\n",
        "            result['stage_config'] = tiered_workflow.get_stage_config(stage)\n",
        "        \n",
        "        all_results.extend(stage_results)\n",
        "        previous_results = stage_results\n",
        "        \n",
        "        # Stage summary\n",
        "        successful = len([r for r in stage_results if r['status'] == 'success'])\n",
        "        print(f\"\\\\nüìä Stage {stage} Summary:\")\n",
        "        print(f\"   Processed: {len(stage_input)} pairs\")\n",
        "        print(f\"   Successful: {successful}\")\n",
        "        print(f\"   Success rate: {successful/len(stage_input)*100:.1f}%\")\n",
        "    \n",
        "    # Final analysis\n",
        "    print(f\"\\\\n{'='*60}\")\n",
        "    print(\"FINAL ANALYSIS\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    analysis_dashboard.create_score_distribution_plot(all_results)\n",
        "    qc_validator.validate_docking_results(all_results)\n",
        "    qc_validator.generate_quality_report()\n",
        "    \n",
        "    return all_results\n",
        "\n",
        "def run_covalent_docking(pairlist_df, covalent_residue, covalent_atom='SG', use_flexible=False):\n",
        "    \"\"\"\n",
        "    Run covalent docking workflow\n",
        "    \n",
        "    Args:\n",
        "        pairlist_df: DataFrame with receptor-ligand pairs and coordinates\n",
        "        covalent_residue: Residue ID for covalent bond (e.g., 'A:123')\n",
        "        covalent_atom: Atom name for covalent bond (e.g., 'SG' for cysteine)\n",
        "        use_flexible: Whether to use flexible receptor docking\n",
        "    \"\"\"\n",
        "    print(f\"üîó Starting Covalent Docking Workflow\")\n",
        "    print(f\"   Covalent residue: {covalent_residue}:{covalent_atom}\")\n",
        "    print(f\"   Flexible receptors: {'Enabled' if use_flexible else 'Disabled'}\")\n",
        "    \n",
        "    # Configure covalent parameters\n",
        "    covalent_config.set_covalent_residue(covalent_residue, covalent_atom)\n",
        "    covalent_config.display_covalent_configuration()\n",
        "    \n",
        "    # Run covalent docking\n",
        "    results = covalent_engine.run_covalent_docking(pairlist_df, covalent_residue, covalent_atom)\n",
        "    \n",
        "    # Generate analysis\n",
        "    print(\"\\\\nüìä Generating covalent docking analysis...\")\n",
        "    analysis_dashboard.create_score_distribution_plot(results)\n",
        "    qc_validator.validate_docking_results(results)\n",
        "    qc_validator.generate_quality_report()\n",
        "    \n",
        "    return results\n",
        "\n",
        "def run_complete_workflow(pairlist_df, workflow_type='standard', **kwargs):\n",
        "    \"\"\"\n",
        "    Run complete workflow with all options\n",
        "    \n",
        "    Args:\n",
        "        pairlist_df: DataFrame with receptor-ligand pairs and coordinates\n",
        "        workflow_type: 'standard', 'tiered', 'covalent'\n",
        "        **kwargs: Additional parameters for specific workflows\n",
        "    \"\"\"\n",
        "    print(f\"üéØ Starting Complete GNINA Workflow: {workflow_type.upper()}\")\n",
        "    print(f\"   Total pairs: {len(pairlist_df)}\")\n",
        "    print(f\"   GPU acceleration: {'Enabled' if CONFIG.get('use_gpu', False) else 'Disabled'}\")\n",
        "    \n",
        "    if workflow_type == 'standard':\n",
        "        return run_standard_docking(\n",
        "            pairlist_df, \n",
        "            use_flexible=kwargs.get('use_flexible', False),\n",
        "            cnn_mode=kwargs.get('cnn_mode', 'rescore')\n",
        "        )\n",
        "    \n",
        "    elif workflow_type == 'tiered':\n",
        "        return run_tiered_workflow(\n",
        "            pairlist_df,\n",
        "            stages=kwargs.get('stages', ['A', 'B']),\n",
        "            use_flexible=kwargs.get('use_flexible', False)\n",
        "        )\n",
        "    \n",
        "    elif workflow_type == 'covalent':\n",
        "        return run_covalent_docking(\n",
        "            pairlist_df,\n",
        "            covalent_residue=kwargs.get('covalent_residue', 'A:123'),\n",
        "            covalent_atom=kwargs.get('covalent_atom', 'SG'),\n",
        "            use_flexible=kwargs.get('use_flexible', False)\n",
        "        )\n",
        "    \n",
        "    else:\n",
        "        print(f\"‚ùå Unknown workflow type: {workflow_type}\")\n",
        "        return None\n",
        "\n",
        "# =============================================================================\n",
        "# Quick Start Examples\n",
        "# =============================================================================\n",
        "\n",
        "print(\"üöÄ Enhanced GNINA Docking Pipeline Ready!\")\n",
        "print(\"\\\\nüìã Available Workflow Types:\")\n",
        "print(\"   1. Standard Docking - Single-stage docking with configurable CNN mode\")\n",
        "print(\"   2. Tiered Workflow - Multi-stage funnel approach (A ‚Üí B ‚Üí C)\")\n",
        "print(\"   3. Covalent Docking - Specialized covalent bond formation\")\n",
        "\n",
        "print(\"\\\\nüîß Quick Start Examples:\")\n",
        "print(\"\\\\n# Standard Docking (Recommended for most users)\")\n",
        "print(\"results = run_complete_workflow(pairlist_df, 'standard', use_flexible=True)\")\n",
        "print(\"\")\n",
        "print(\"# Tiered Workflow (For large libraries)\")\n",
        "print(\"results = run_complete_workflow(pairlist_df, 'tiered', stages=['A', 'B'], use_flexible=True)\")\n",
        "print(\"\")\n",
        "print(\"# Covalent Docking (For covalent inhibitors)\")\n",
        "print(\"results = run_complete_workflow(pairlist_df, 'covalent', covalent_residue='A:123', covalent_atom='SG')\")\n",
        "\n",
        "print(\"\\\\nüí° Configuration Tips:\")\n",
        "print(\"   ‚Ä¢ GPU acceleration is automatically prioritized over CPU (10-50x speedup)\")\n",
        "print(\"   ‚Ä¢ Use flexible receptors for better accuracy (15-25% improvement)\")\n",
        "print(\"   ‚Ä¢ Start with 'rescore' CNN mode for speed, use 'refinement' for accuracy\")\n",
        "print(\"   ‚Ä¢ For large libraries (>1000 ligands): Use tiered workflow\")\n",
        "print(\"   ‚Ä¢ For covalent inhibitors: Use covalent docking with appropriate reactive residue\")\n",
        "print(\"   ‚Ä¢ CPU cores are used as fallback when GPU is unavailable\")\n",
        "\n",
        "print(\"\\\\nüìä Analysis and Visualization:\")\n",
        "print(\"   ‚Ä¢ All workflows automatically generate comprehensive analysis\")\n",
        "print(\"   ‚Ä¢ Interactive dashboards saved to visualizations/\")\n",
        "print(\"   ‚Ä¢ Quality control reports saved to enhanced_analysis/\")\n",
        "print(\"   ‚Ä¢ Resume capability: Interrupted runs can be resumed automatically\")\n",
        "\n",
        "print(\"\\\\nüîó References:\")\n",
        "print(\"   ‚Ä¢ GNINA: https://github.com/gnina/gnina\")\n",
        "print(\"   ‚Ä¢ PDB Preparation Wizard: https://github.com/OASolliman590/pdb-prepare-wizard\")\n",
        "print(\"   ‚Ä¢ CNN Scoring Modes: See GNINA documentation for detailed explanations\")\n",
        "\n",
        "print(\"\\\\n‚úÖ Ready to run! Choose your workflow and execute the appropriate function above.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
