{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjEUHVAQ_eOQ",
        "outputId": "875499cf-7e9b-4585-fdfb-ad1a63f1b44a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# GNINA Docking Notebook - Enhanced for Google Colab with GPU Support\n",
        "# =============================================================================\n",
        "# This notebook provides a comprehensive molecular docking workflow using GNINA\n",
        "# with optimized settings for Google Colab environment and GPU acceleration\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Import essential libraries\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import pathlib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úÖ Environment setup complete\")\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"Working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibtK-nSa8rGY",
        "outputId": "07d45e29-b055-488d-86c6-07d2226b345d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Sertaline_Derv_docking\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Configuration and Setup\n",
        "# =============================================================================\n",
        "\n",
        "# Set working directory\n",
        "WORK_DIR = \"/content/drive/MyDrive/Sertaline_Derv_docking\"\n",
        "%cd {WORK_DIR}\n",
        "\n",
        "# Configuration parameters\n",
        "CONFIG = {\n",
        "    'project_name': 'Sertaline_Derv_docking',\n",
        "    'gnina_version': 'v1.3.2',  # Latest version\n",
        "    'exhaustiveness': 32,        # Higher for better sampling\n",
        "    'num_modes': 20,            # Number of poses to generate\n",
        "    'seed': 42,                 # For reproducibility\n",
        "    'cnn_scoring': 'rescore',   # Use CNN for final scoring\n",
        "    'cpu_cores': 4,             # Number of CPU cores to use\n",
        "    'batch_size': 5,            # Process in batches\n",
        "    'timeout': 300,             # Timeout per docking (seconds)\n",
        "}\n",
        "\n",
        "# Create directory structure\n",
        "dirs_to_create = [\n",
        "    'ligands_raw', 'ligands_prep', 'receptors_raw', 'receptors_prep', \n",
        "    'gnina_out', 'results', 'logs', 'visualizations'\n",
        "]\n",
        "\n",
        "for dir_name in dirs_to_create:\n",
        "    os.makedirs(dir_name, exist_ok=True)\n",
        "\n",
        "print(f\"‚úÖ Working in: {WORK_DIR}\")\n",
        "print(f\"‚úÖ Configuration: {CONFIG}\")\n",
        "print(f\"‚úÖ Directories created: {dirs_to_create}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjrnR79k_jig",
        "outputId": "498a9f9b-1d29-4feb-bfa5-666792369ea7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-05-19 15:11:01--  https://github.com/gnina/gnina/releases/download/v1.3/gnina\n",
            "Resolving github.com (github.com)... 140.82.121.3\n",
            "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/45548146/f37c6a31-c8d1-4c4f-9748-c7c6f727e868?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250519%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250519T150856Z&X-Amz-Expires=300&X-Amz-Signature=d5cb814fe380223e9e280f1522c9b195f4ecb006337d29af4fd90f4a96000480&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dgnina&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-05-19 15:11:01--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/45548146/f37c6a31-c8d1-4c4f-9748-c7c6f727e868?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250519%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250519T150856Z&X-Amz-Expires=300&X-Amz-Signature=d5cb814fe380223e9e280f1522c9b195f4ecb006337d29af4fd90f4a96000480&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dgnina&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1277859864 (1.2G) [application/octet-stream]\n",
            "Saving to: ‚Äògnina‚Äô\n",
            "\n",
            "gnina               100%[===================>]   1.19G  60.1MB/s    in 24s     \n",
            "\n",
            "2025-05-19 15:11:25 (51.4 MB/s) - ‚Äògnina‚Äô saved [1277859864/1277859864]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Install Dependencies and Download GNINA\n",
        "# =============================================================================\n",
        "\n",
        "# Install required packages\n",
        "print(\"üì¶ Installing dependencies...\")\n",
        "%pip install -q rdkit-pypi meeko pdb2pqr openbabel\n",
        "!apt-get update -qq && apt-get install -y -qq openbabel pdb2pqr\n",
        "\n",
        "# Download GNINA binary (GPU-enabled version)\n",
        "print(\"‚¨áÔ∏è Downloading GNINA binary...\")\n",
        "gnina_url = f\"https://github.com/gnina/gnina/releases/download/{CONFIG['gnina_version']}/gnina\"\n",
        "!wget -q {gnina_url} -O gnina\n",
        "\n",
        "# Make executable\n",
        "!chmod +x gnina\n",
        "\n",
        "# Verify installation\n",
        "print(\"üîç Verifying GNINA installation...\")\n",
        "!./gnina --version\n",
        "\n",
        "# Check GPU availability\n",
        "print(\"\\nüñ•Ô∏è Checking GPU availability...\")\n",
        "try:\n",
        "    gpu_info = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
        "    if gpu_info.returncode == 0:\n",
        "        print(\"‚úÖ GPU detected:\")\n",
        "        print(gpu_info.stdout.split('\\n')[0:3])  # Show first few lines\n",
        "        CONFIG['use_gpu'] = True\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No GPU detected, using CPU\")\n",
        "        CONFIG['use_gpu'] = False\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è GPU check failed, using CPU\")\n",
        "    CONFIG['use_gpu'] = False\n",
        "\n",
        "print(f\"‚úÖ GNINA setup complete. GPU mode: {CONFIG['use_gpu']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zk6u9V1S_nGm",
        "outputId": "5298dfc1-cf20-4be6-a197-372ce702e5e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "openbabel is already the newest version (3.1.1+dfsg-6ubuntu5).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# File Preparation and Validation\n",
        "# =============================================================================\n",
        "\n",
        "def validate_project_structure():\n",
        "    \"\"\"Validate that all required files and directories exist\"\"\"\n",
        "    required_files = ['pairlist.csv']\n",
        "    required_dirs = ['ligands_raw', 'receptors_raw']\n",
        "    \n",
        "    missing_files = []\n",
        "    missing_dirs = []\n",
        "    \n",
        "    for file in required_files:\n",
        "        if not os.path.exists(file):\n",
        "            missing_files.append(file)\n",
        "    \n",
        "    for dir_name in required_dirs:\n",
        "        if not os.path.exists(dir_name):\n",
        "            missing_dirs.append(dir_name)\n",
        "    \n",
        "    if missing_files or missing_dirs:\n",
        "        print(\"‚ùå Missing required files/directories:\")\n",
        "        for item in missing_files + missing_dirs:\n",
        "            print(f\"   - {item}\")\n",
        "        return False\n",
        "    \n",
        "    print(\"‚úÖ Project structure validated\")\n",
        "    return True\n",
        "\n",
        "def load_and_validate_pairlist():\n",
        "    \"\"\"Load and validate the pairlist.csv file\"\"\"\n",
        "    try:\n",
        "        # Load pairlist with flexible column handling\n",
        "        df = pd.read_csv('pairlist.csv', skipinitialspace=True)\n",
        "        \n",
        "        # Normalize column names\n",
        "        df.columns = [col.strip().lower().replace(' ', '_') for col in df.columns]\n",
        "        \n",
        "        # Required columns\n",
        "        required_cols = ['receptor', 'ligand', 'center_x', 'center_y', 'center_z', \n",
        "                        'size_x', 'size_y', 'size_z']\n",
        "        \n",
        "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
        "        if missing_cols:\n",
        "            print(f\"‚ùå Missing columns in pairlist.csv: {missing_cols}\")\n",
        "            return None\n",
        "        \n",
        "        # Validate numeric columns\n",
        "        numeric_cols = ['center_x', 'center_y', 'center_z', 'size_x', 'size_y', 'size_z']\n",
        "        for col in numeric_cols:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "            if df[col].isna().any():\n",
        "                print(f\"‚ùå Non-numeric values found in column: {col}\")\n",
        "                return None\n",
        "        \n",
        "        # Add site_id if missing\n",
        "        if 'site_id' not in df.columns:\n",
        "            df['site_id'] = 'site1'\n",
        "        \n",
        "        print(f\"‚úÖ Pairlist loaded successfully: {len(df)} entries\")\n",
        "        print(f\"   Receptors: {df['receptor'].nunique()}\")\n",
        "        print(f\"   Ligands: {df['ligand'].nunique()}\")\n",
        "        \n",
        "        return df\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading pairlist.csv: {e}\")\n",
        "        return None\n",
        "\n",
        "# Validate project structure\n",
        "if not validate_project_structure():\n",
        "    print(\"Please ensure all required files and directories are present\")\n",
        "else:\n",
        "    # Load pairlist\n",
        "    pairlist_df = load_and_validate_pairlist()\n",
        "    \n",
        "    if pairlist_df is not None:\n",
        "        print(\"\\nüìä Pairlist preview:\")\n",
        "        print(pairlist_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxvlyUYn_pHQ"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Advanced GNINA Configuration and Helper Functions\n",
        "# =============================================================================\n",
        "\n",
        "class GNINADocker:\n",
        "    \"\"\"Enhanced GNINA docking class with advanced features\"\"\"\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.gnina_bin = \"./gnina\"\n",
        "        self.results = []\n",
        "        self.failures = []\n",
        "        \n",
        "    def build_gnina_command(self, row):\n",
        "        \"\"\"Build optimized GNINA command with advanced parameters\"\"\"\n",
        "        receptor = f\"receptors_prep/{row['receptor']}.pdbqt\"\n",
        "        ligand = f\"ligands_prep/{row['ligand']}.pdbqt\"\n",
        "        \n",
        "        # Output files\n",
        "        tag = f\"{row['receptor']}_{row['site_id']}_{row['ligand']}\"\n",
        "        output_sdf = f\"gnina_out/{tag}_poses.sdf\"\n",
        "        log_file = f\"logs/{tag}.log\"\n",
        "        \n",
        "        # Base command\n",
        "        cmd = [\n",
        "            self.gnina_bin,\n",
        "            \"--receptor\", receptor,\n",
        "            \"--ligand\", ligand,\n",
        "            \"--out\", output_sdf,\n",
        "            \"--log\", log_file,\n",
        "        ]\n",
        "        \n",
        "        # Docking box parameters\n",
        "        cmd.extend([\n",
        "            \"--center_x\", str(row['center_x']),\n",
        "            \"--center_y\", str(row['center_y']),\n",
        "            \"--center_z\", str(row['center_z']),\n",
        "            \"--size_x\", str(row['size_x']),\n",
        "            \"--size_y\", str(row['size_y']),\n",
        "            \"--size_z\", str(row['size_z']),\n",
        "        ])\n",
        "        \n",
        "        # Performance parameters\n",
        "        cmd.extend([\n",
        "            \"--exhaustiveness\", str(self.config['exhaustiveness']),\n",
        "            \"--num_modes\", str(self.config['num_modes']),\n",
        "            \"--seed\", str(self.config['seed']),\n",
        "            \"--cpu\", str(self.config['cpu_cores']),\n",
        "        ])\n",
        "        \n",
        "        # Advanced GNINA features\n",
        "        cmd.extend([\n",
        "            \"--cnn_scoring\", self.config['cnn_scoring'],\n",
        "            \"--cnn_rotation\", \"0\",  # No rotation for speed\n",
        "            \"--min_rmsd_filter\", \"1.0\",\n",
        "            \"--pose_sort_order\", \"0\",  # Sort by CNN score\n",
        "        ])\n",
        "        \n",
        "        # GPU support\n",
        "        if self.config['use_gpu']:\n",
        "            cmd.append(\"--gpu\")\n",
        "            cmd.extend([\"--device\", \"0\"])\n",
        "        \n",
        "        return cmd, output_sdf, log_file\n",
        "    \n",
        "    def run_docking(self, row):\n",
        "        \"\"\"Run docking for a single receptor-ligand pair\"\"\"\n",
        "        try:\n",
        "            cmd, output_sdf, log_file = self.build_gnina_command(row)\n",
        "            \n",
        "            # Run command with timeout\n",
        "            result = subprocess.run(\n",
        "                cmd,\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=self.config['timeout']\n",
        "            )\n",
        "            \n",
        "            if result.returncode == 0:\n",
        "                # Parse results\n",
        "                scores = self.parse_gnina_log(log_file)\n",
        "                return {\n",
        "                    'status': 'success',\n",
        "                    'output_file': output_sdf,\n",
        "                    'log_file': log_file,\n",
        "                    'scores': scores,\n",
        "                    'row': row\n",
        "                }\n",
        "            else:\n",
        "                return {\n",
        "                    'status': 'failed',\n",
        "                    'error': result.stderr,\n",
        "                    'row': row\n",
        "                }\n",
        "                \n",
        "        except subprocess.TimeoutExpired:\n",
        "            return {\n",
        "                'status': 'timeout',\n",
        "                'error': f\"Docking timed out after {self.config['timeout']} seconds\",\n",
        "                'row': row\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'status': 'error',\n",
        "                'error': str(e),\n",
        "                'row': row\n",
        "            }\n",
        "    \n",
        "    def parse_gnina_log(self, log_file):\n",
        "        \"\"\"Parse GNINA log file to extract scores\"\"\"\n",
        "        scores = []\n",
        "        try:\n",
        "            with open(log_file, 'r') as f:\n",
        "                for line in f:\n",
        "                    if 'CNNaffinity' in line and 'CNNscore' in line:\n",
        "                        parts = line.strip().split()\n",
        "                        if len(parts) >= 4:\n",
        "                            scores.append({\n",
        "                                'cnn_affinity': float(parts[1]),\n",
        "                                'cnn_score': float(parts[3].strip('()'))\n",
        "                            })\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not parse log file {log_file}: {e}\")\n",
        "        \n",
        "        return scores\n",
        "\n",
        "# Initialize GNINA docker\n",
        "gnina_docker = GNINADocker(CONFIG)\n",
        "print(\"‚úÖ GNINA Docker initialized with advanced configuration\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1GUhYhf_sJz",
        "outputId": "f11dc8b2-f357-44f3-c02f-98412be00f60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gnina v1.3 master:97fa6bc+   Built Oct  3 2024.\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Execute Docking Workflow\n",
        "# =============================================================================\n",
        "\n",
        "def run_batch_docking(pairlist_df, batch_size=None):\n",
        "    \"\"\"Run docking for all pairs with progress tracking and error handling\"\"\"\n",
        "    \n",
        "    if batch_size is None:\n",
        "        batch_size = CONFIG['batch_size']\n",
        "    \n",
        "    total_pairs = len(pairlist_df)\n",
        "    successful = 0\n",
        "    failed = 0\n",
        "    \n",
        "    print(f\"üöÄ Starting batch docking: {total_pairs} pairs\")\n",
        "    print(f\"   Batch size: {batch_size}\")\n",
        "    print(f\"   GPU mode: {CONFIG['use_gpu']}\")\n",
        "    print(f\"   Exhaustiveness: {CONFIG['exhaustiveness']}\")\n",
        "    \n",
        "    # Process in batches\n",
        "    for batch_start in tqdm(range(0, total_pairs, batch_size), desc=\"Processing batches\"):\n",
        "        batch_end = min(batch_start + batch_size, total_pairs)\n",
        "        batch_df = pairlist_df.iloc[batch_start:batch_end]\n",
        "        \n",
        "        print(f\"\\nüì¶ Processing batch {batch_start//batch_size + 1}: pairs {batch_start+1}-{batch_end}\")\n",
        "        \n",
        "        batch_results = []\n",
        "        for idx, row in batch_df.iterrows():\n",
        "            result = gnina_docker.run_docking(row)\n",
        "            batch_results.append(result)\n",
        "            \n",
        "            if result['status'] == 'success':\n",
        "                successful += 1\n",
        "                print(f\"   ‚úÖ {row['receptor']}-{row['ligand']}\")\n",
        "            else:\n",
        "                failed += 1\n",
        "                print(f\"   ‚ùå {row['receptor']}-{row['ligand']}: {result['status']}\")\n",
        "        \n",
        "        # Save batch results\n",
        "        batch_results_df = pd.DataFrame(batch_results)\n",
        "        batch_results_df.to_csv(f\"results/batch_{batch_start//batch_size + 1}_results.csv\", index=False)\n",
        "    \n",
        "    # Summary\n",
        "    print(f\"\\nüìä Docking Summary:\")\n",
        "    print(f\"   Total pairs: {total_pairs}\")\n",
        "    print(f\"   Successful: {successful}\")\n",
        "    print(f\"   Failed: {failed}\")\n",
        "    print(f\"   Success rate: {successful/total_pairs*100:.1f}%\")\n",
        "    \n",
        "    return successful, failed\n",
        "\n",
        "# Check if pairlist is loaded\n",
        "if 'pairlist_df' in locals() and pairlist_df is not None:\n",
        "    # Run docking\n",
        "    successful, failed = run_batch_docking(pairlist_df)\n",
        "    \n",
        "    # Save overall results\n",
        "    results_summary = {\n",
        "        'total_pairs': len(pairlist_df),\n",
        "        'successful': successful,\n",
        "        'failed': failed,\n",
        "        'success_rate': successful/len(pairlist_df)*100,\n",
        "        'config': CONFIG\n",
        "    }\n",
        "    \n",
        "    with open('results/docking_summary.json', 'w') as f:\n",
        "        import json\n",
        "        json.dump(results_summary, f, indent=2)\n",
        "    \n",
        "    print(\"‚úÖ Docking workflow completed!\")\n",
        "else:\n",
        "    print(\"‚ùå Please ensure pairlist.csv is loaded before running docking\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Tiered CNN Scoring Workflow Implementation\n",
        "# =============================================================================\n",
        "\n",
        "class TieredGNINAWorkflow:\n",
        "    \"\"\"\n",
        "    Implements a tiered CNN scoring approach for efficient molecular docking:\n",
        "    Stage A: Broad screen with rescore (fast)\n",
        "    Stage B: Focused re-dock with refinement (balanced) \n",
        "    Stage C: Finalists with all CNN (expensive, optional)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.gnina_bin = \"./gnina\"\n",
        "        self.stage_results = {}\n",
        "        \n",
        "        # Stage configurations\n",
        "        self.stage_configs = {\n",
        "            'A': {  # Broad screen\n",
        "                'cnn_scoring': 'rescore',\n",
        "                'exhaustiveness': 12,\n",
        "                'num_modes': 8,\n",
        "                'description': 'Fast broad screening',\n",
        "                'cnn_score_threshold': 0.5,  # Relaxed threshold\n",
        "                'max_ligands_per_receptor': None  # No limit\n",
        "            },\n",
        "            'B': {  # Focused re-dock\n",
        "                'cnn_scoring': 'refinement', \n",
        "                'exhaustiveness': 24,\n",
        "                'num_modes': 15,\n",
        "                'description': 'Balanced refinement',\n",
        "                'cnn_score_threshold': 0.7,  # Stricter threshold\n",
        "                'max_ligands_per_receptor': 5,  # Top 5 per receptor\n",
        "                'top_percentage': 0.05  # Top 5% from Stage A\n",
        "            },\n",
        "            'C': {  # Finalists (optional)\n",
        "                'cnn_scoring': 'all',\n",
        "                'exhaustiveness': 48,\n",
        "                'num_modes': 20,\n",
        "                'description': 'High-accuracy final screening',\n",
        "                'cnn_score_threshold': 0.8,  # Very strict\n",
        "                'max_ligands_per_receptor': 2,  # Top 2 per receptor\n",
        "                'top_percentage': 0.01  # Top 1% from Stage B\n",
        "            }\n",
        "        }\n",
        "    \n",
        "    def run_stage(self, stage, pairlist_df, previous_results=None):\n",
        "        \"\"\"Run a specific stage of the tiered workflow\"\"\"\n",
        "        stage_config = self.stage_configs[stage]\n",
        "        \n",
        "        print(f\"\\nüöÄ Starting Stage {stage}: {stage_config['description']}\")\n",
        "        print(f\"   CNN Scoring: {stage_config['cnn_scoring']}\")\n",
        "        print(f\"   Exhaustiveness: {stage_config['exhaustiveness']}\")\n",
        "        print(f\"   Num Modes: {stage_config['num_modes']}\")\n",
        "        \n",
        "        # Filter input based on previous stage results\n",
        "        if stage == 'A':\n",
        "            input_df = pairlist_df.copy()\n",
        "        else:\n",
        "            input_df = self._filter_for_stage(stage, previous_results, pairlist_df)\n",
        "        \n",
        "        if len(input_df) == 0:\n",
        "            print(f\"   ‚ö†Ô∏è No ligands to process for Stage {stage}\")\n",
        "            return []\n",
        "        \n",
        "        print(f\"   Processing {len(input_df)} ligand-receptor pairs\")\n",
        "        \n",
        "        # Update config for this stage\n",
        "        stage_config_copy = self.config.copy()\n",
        "        stage_config_copy.update({\n",
        "            'cnn_scoring': stage_config['cnn_scoring'],\n",
        "            'exhaustiveness': stage_config['exhaustiveness'],\n",
        "            'num_modes': stage_config['num_modes']\n",
        "        })\n",
        "        \n",
        "        # Run docking\n",
        "        gnina_docker = GNINADocker(stage_config_copy)\n",
        "        results = []\n",
        "        \n",
        "        for idx, row in tqdm(input_df.iterrows(), total=len(input_df), desc=f\"Stage {stage}\"):\n",
        "            result = gnina_docker.run_docking(row)\n",
        "            result['stage'] = stage\n",
        "            result['stage_config'] = stage_config\n",
        "            results.append(result)\n",
        "        \n",
        "        # Save stage results\n",
        "        self.stage_results[stage] = results\n",
        "        self._save_stage_results(stage, results)\n",
        "        \n",
        "        # Print stage summary\n",
        "        self._print_stage_summary(stage, results)\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def _filter_for_stage(self, stage, previous_results, pairlist_df):\n",
        "        \"\"\"Filter ligands for subsequent stages based on previous results\"\"\"\n",
        "        if not previous_results:\n",
        "            return pairlist_df\n",
        "        \n",
        "        # Extract successful results with scores\n",
        "        successful_results = [r for r in previous_results if r['status'] == 'success']\n",
        "        \n",
        "        if not successful_results:\n",
        "            return pd.DataFrame()\n",
        "        \n",
        "        # Create results DataFrame\n",
        "        results_df = pd.DataFrame([\n",
        "            {\n",
        "                'receptor': r['row']['receptor'],\n",
        "                'ligand': r['row']['ligand'],\n",
        "                'site_id': r['row']['site_id'],\n",
        "                'cnn_score': max([s.get('cnn_score', 0) for s in r.get('scores', [])], default=0),\n",
        "                'cnn_affinity': max([s.get('cnn_affinity', 0) for s in r.get('scores', [])], default=0)\n",
        "            }\n",
        "            for r in successful_results\n",
        "        ])\n",
        "        \n",
        "        stage_config = self.stage_configs[stage]\n",
        "        \n",
        "        # Apply filters\n",
        "        filtered_df = results_df[\n",
        "            results_df['cnn_score'] >= stage_config['cnn_score_threshold']\n",
        "        ].copy()\n",
        "        \n",
        "        # Apply top percentage filter\n",
        "        if 'top_percentage' in stage_config:\n",
        "            top_count = max(1, int(len(filtered_df) * stage_config['top_percentage']))\n",
        "            filtered_df = filtered_df.nlargest(top_count, 'cnn_score')\n",
        "        \n",
        "        # Apply per-receptor limit\n",
        "        if stage_config['max_ligands_per_receptor']:\n",
        "            filtered_df = (filtered_df.groupby('receptor')\n",
        "                          .apply(lambda x: x.nlargest(stage_config['max_ligands_per_receptor'], 'cnn_score'))\n",
        "                          .reset_index(drop=True))\n",
        "        \n",
        "        # Merge back with original pairlist to get full row data\n",
        "        if len(filtered_df) > 0:\n",
        "            merged_df = pairlist_df.merge(\n",
        "                filtered_df[['receptor', 'ligand', 'site_id']], \n",
        "                on=['receptor', 'ligand', 'site_id']\n",
        "            )\n",
        "            return merged_df\n",
        "        \n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    def _save_stage_results(self, stage, results):\n",
        "        \"\"\"Save stage results to files\"\"\"\n",
        "        stage_dir = f\"results/stage_{stage}\"\n",
        "        os.makedirs(stage_dir, exist_ok=True)\n",
        "        \n",
        "        # Save detailed results\n",
        "        results_df = pd.DataFrame(results)\n",
        "        results_df.to_csv(f\"{stage_dir}/stage_{stage}_results.csv\", index=False)\n",
        "        \n",
        "        # Save summary\n",
        "        successful = [r for r in results if r['status'] == 'success']\n",
        "        if successful:\n",
        "            summary_data = []\n",
        "            for result in successful:\n",
        "                scores = result.get('scores', [])\n",
        "                if scores:\n",
        "                    best_score = max(scores, key=lambda x: x.get('cnn_score', 0))\n",
        "                    summary_data.append({\n",
        "                        'receptor': result['row']['receptor'],\n",
        "                        'ligand': result['row']['ligand'],\n",
        "                        'site_id': result['row']['site_id'],\n",
        "                        'cnn_score': best_score.get('cnn_score', 0),\n",
        "                        'cnn_affinity': best_score.get('cnn_affinity', 0),\n",
        "                        'output_file': result.get('output_file', ''),\n",
        "                        'log_file': result.get('log_file', '')\n",
        "                    })\n",
        "            \n",
        "            if summary_data:\n",
        "                summary_df = pd.DataFrame(summary_data)\n",
        "                summary_df.to_csv(f\"{stage_dir}/stage_{stage}_summary.csv\", index=False)\n",
        "    \n",
        "    def _print_stage_summary(self, stage, results):\n",
        "        \"\"\"Print summary for the stage\"\"\"\n",
        "        total = len(results)\n",
        "        successful = len([r for r in results if r['status'] == 'success'])\n",
        "        failed = total - successful\n",
        "        \n",
        "        print(f\"\\nüìä Stage {stage} Summary:\")\n",
        "        print(f\"   Total pairs: {total}\")\n",
        "        print(f\"   Successful: {successful}\")\n",
        "        print(f\"   Failed: {failed}\")\n",
        "        print(f\"   Success rate: {successful/total*100:.1f}%\")\n",
        "        \n",
        "        if successful > 0:\n",
        "            successful_results = [r for r in results if r['status'] == 'success']\n",
        "            all_scores = []\n",
        "            for result in successful_results:\n",
        "                scores = result.get('scores', [])\n",
        "                if scores:\n",
        "                    all_scores.extend([s.get('cnn_score', 0) for s in scores])\n",
        "            \n",
        "            if all_scores:\n",
        "                print(f\"   CNN Score range: {min(all_scores):.3f} - {max(all_scores):.3f}\")\n",
        "                print(f\"   CNN Score mean: {np.mean(all_scores):.3f}\")\n",
        "    \n",
        "    def run_complete_workflow(self, pairlist_df, stages=['A', 'B']):\n",
        "        \"\"\"Run the complete tiered workflow\"\"\"\n",
        "        print(\"üéØ Starting Tiered GNINA Workflow\")\n",
        "        print(f\"   Stages to run: {stages}\")\n",
        "        \n",
        "        all_results = []\n",
        "        previous_results = None\n",
        "        \n",
        "        for stage in stages:\n",
        "            stage_results = self.run_stage(stage, pairlist_df, previous_results)\n",
        "            all_results.extend(stage_results)\n",
        "            previous_results = stage_results\n",
        "            \n",
        "            # Check if we should continue\n",
        "            if stage in ['A', 'B'] and len(stage_results) == 0:\n",
        "                print(f\"   ‚ö†Ô∏è No results from Stage {stage}, stopping workflow\")\n",
        "                break\n",
        "        \n",
        "        # Save complete workflow results\n",
        "        self._save_workflow_summary(all_results)\n",
        "        \n",
        "        return all_results\n",
        "    \n",
        "    def _save_workflow_summary(self, all_results):\n",
        "        \"\"\"Save complete workflow summary\"\"\"\n",
        "        workflow_summary = {\n",
        "            'total_pairs_processed': len(all_results),\n",
        "            'stages_completed': list(self.stage_results.keys()),\n",
        "            'config': self.config,\n",
        "            'stage_configs': self.stage_configs\n",
        "        }\n",
        "        \n",
        "        with open('results/workflow_summary.json', 'w') as f:\n",
        "            import json\n",
        "            json.dump(workflow_summary, f, indent=2)\n",
        "        \n",
        "        print(\"\\n‚úÖ Workflow summary saved to results/workflow_summary.json\")\n",
        "\n",
        "# Initialize tiered workflow\n",
        "tiered_workflow = TieredGNINAWorkflow(CONFIG)\n",
        "print(\"‚úÖ Tiered GNINA Workflow initialized\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Parallel Processing Implementation\n",
        "# =============================================================================\n",
        "\n",
        "import concurrent.futures\n",
        "import multiprocessing as mp\n",
        "from functools import partial\n",
        "import threading\n",
        "import queue\n",
        "import time\n",
        "\n",
        "class ParallelGNINADocker:\n",
        "    \"\"\"Enhanced GNINA docker with parallel processing capabilities\"\"\"\n",
        "    \n",
        "    def __init__(self, config, max_workers=None):\n",
        "        self.config = config\n",
        "        self.gnina_bin = \"./gnina\"\n",
        "        self.max_workers = max_workers or min(4, mp.cpu_count())\n",
        "        self.results_queue = queue.Queue()\n",
        "        self.progress_lock = threading.Lock()\n",
        "        \n",
        "    def run_docking_parallel(self, pairlist_df, batch_size=None):\n",
        "        \"\"\"Run docking with parallel processing\"\"\"\n",
        "        if batch_size is None:\n",
        "            batch_size = self.config.get('batch_size', 5)\n",
        "        \n",
        "        total_pairs = len(pairlist_df)\n",
        "        print(f\"üöÄ Starting parallel docking: {total_pairs} pairs\")\n",
        "        print(f\"   Max workers: {self.max_workers}\")\n",
        "        print(f\"   Batch size: {batch_size}\")\n",
        "        \n",
        "        # Split into batches\n",
        "        batches = [pairlist_df.iloc[i:i+batch_size] for i in range(0, total_pairs, batch_size)]\n",
        "        \n",
        "        all_results = []\n",
        "        successful = 0\n",
        "        failed = 0\n",
        "        \n",
        "        # Process batches in parallel\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
        "            # Submit all batches\n",
        "            future_to_batch = {\n",
        "                executor.submit(self._process_batch, batch, batch_idx): batch_idx \n",
        "                for batch_idx, batch in enumerate(batches)\n",
        "            }\n",
        "            \n",
        "            # Collect results as they complete\n",
        "            for future in tqdm(concurrent.futures.as_completed(future_to_batch), \n",
        "                             total=len(batches), desc=\"Processing batches\"):\n",
        "                batch_idx = future_to_batch[future]\n",
        "                try:\n",
        "                    batch_results = future.result()\n",
        "                    all_results.extend(batch_results)\n",
        "                    \n",
        "                    # Count successes/failures\n",
        "                    for result in batch_results:\n",
        "                        if result['status'] == 'success':\n",
        "                            successful += 1\n",
        "                        else:\n",
        "                            failed += 1\n",
        "                            \n",
        "                except Exception as e:\n",
        "                    print(f\"‚ùå Batch {batch_idx} failed: {e}\")\n",
        "                    failed += len(batches[batch_idx])\n",
        "        \n",
        "        # Summary\n",
        "        print(f\"\\nüìä Parallel Docking Summary:\")\n",
        "        print(f\"   Total pairs: {total_pairs}\")\n",
        "        print(f\"   Successful: {successful}\")\n",
        "        print(f\"   Failed: {failed}\")\n",
        "        print(f\"   Success rate: {successful/total_pairs*100:.1f}%\")\n",
        "        \n",
        "        return all_results\n",
        "    \n",
        "    def _process_batch(self, batch_df, batch_idx):\n",
        "        \"\"\"Process a single batch of docking pairs\"\"\"\n",
        "        batch_results = []\n",
        "        gnina_docker = GNINADocker(self.config)\n",
        "        \n",
        "        for idx, row in batch_df.iterrows():\n",
        "            try:\n",
        "                result = gnina_docker.run_docking(row)\n",
        "                result['batch_idx'] = batch_idx\n",
        "                batch_results.append(result)\n",
        "                \n",
        "                # Update progress\n",
        "                with self.progress_lock:\n",
        "                    status = \"‚úÖ\" if result['status'] == 'success' else \"‚ùå\"\n",
        "                    print(f\"   {status} Batch {batch_idx}: {row['receptor']}-{row['ligand']}\")\n",
        "                    \n",
        "            except Exception as e:\n",
        "                error_result = {\n",
        "                    'status': 'error',\n",
        "                    'error': str(e),\n",
        "                    'row': row,\n",
        "                    'batch_idx': batch_idx\n",
        "                }\n",
        "                batch_results.append(error_result)\n",
        "        \n",
        "        return batch_results\n",
        "\n",
        "class ParallelTieredWorkflow(TieredGNINAWorkflow):\n",
        "    \"\"\"Tiered workflow with parallel processing\"\"\"\n",
        "    \n",
        "    def __init__(self, config, max_workers=None):\n",
        "        super().__init__(config)\n",
        "        self.max_workers = max_workers or min(4, mp.cpu_count())\n",
        "        self.parallel_docker = ParallelGNINADocker(config, max_workers)\n",
        "    \n",
        "    def run_stage_parallel(self, stage, pairlist_df, previous_results=None):\n",
        "        \"\"\"Run a specific stage with parallel processing\"\"\"\n",
        "        stage_config = self.stage_configs[stage]\n",
        "        \n",
        "        print(f\"\\nüöÄ Starting Stage {stage} (Parallel): {stage_config['description']}\")\n",
        "        print(f\"   CNN Scoring: {stage_config['cnn_scoring']}\")\n",
        "        print(f\"   Exhaustiveness: {stage_config['exhaustiveness']}\")\n",
        "        print(f\"   Num Modes: {stage_config['num_modes']}\")\n",
        "        print(f\"   Max Workers: {self.max_workers}\")\n",
        "        \n",
        "        # Filter input based on previous stage results\n",
        "        if stage == 'A':\n",
        "            input_df = pairlist_df.copy()\n",
        "        else:\n",
        "            input_df = self._filter_for_stage(stage, previous_results, pairlist_df)\n",
        "        \n",
        "        if len(input_df) == 0:\n",
        "            print(f\"   ‚ö†Ô∏è No ligands to process for Stage {stage}\")\n",
        "            return []\n",
        "        \n",
        "        print(f\"   Processing {len(input_df)} ligand-receptor pairs\")\n",
        "        \n",
        "        # Update config for this stage\n",
        "        stage_config_copy = self.config.copy()\n",
        "        stage_config_copy.update({\n",
        "            'cnn_scoring': stage_config['cnn_scoring'],\n",
        "            'exhaustiveness': stage_config['exhaustiveness'],\n",
        "            'num_modes': stage_config['num_modes']\n",
        "        })\n",
        "        \n",
        "        # Update parallel docker config\n",
        "        self.parallel_docker.config = stage_config_copy\n",
        "        \n",
        "        # Run parallel docking\n",
        "        results = self.parallel_docker.run_docking_parallel(input_df)\n",
        "        \n",
        "        # Add stage information\n",
        "        for result in results:\n",
        "            result['stage'] = stage\n",
        "            result['stage_config'] = stage_config\n",
        "        \n",
        "        # Save stage results\n",
        "        self.stage_results[stage] = results\n",
        "        self._save_stage_results(stage, results)\n",
        "        \n",
        "        # Print stage summary\n",
        "        self._print_stage_summary(stage, results)\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def run_complete_workflow_parallel(self, pairlist_df, stages=['A', 'B']):\n",
        "        \"\"\"Run the complete tiered workflow with parallel processing\"\"\"\n",
        "        print(\"üéØ Starting Parallel Tiered GNINA Workflow\")\n",
        "        print(f\"   Stages to run: {stages}\")\n",
        "        print(f\"   Max workers: {self.max_workers}\")\n",
        "        \n",
        "        all_results = []\n",
        "        previous_results = None\n",
        "        \n",
        "        for stage in stages:\n",
        "            stage_results = self.run_stage_parallel(stage, pairlist_df, previous_results)\n",
        "            all_results.extend(stage_results)\n",
        "            previous_results = stage_results\n",
        "            \n",
        "            # Check if we should continue\n",
        "            if stage in ['A', 'B'] and len(stage_results) == 0:\n",
        "                print(f\"   ‚ö†Ô∏è No results from Stage {stage}, stopping workflow\")\n",
        "                break\n",
        "        \n",
        "        # Save complete workflow results\n",
        "        self._save_workflow_summary(all_results)\n",
        "        \n",
        "        return all_results\n",
        "\n",
        "# Initialize parallel workflow\n",
        "parallel_workflow = ParallelTieredWorkflow(CONFIG, max_workers=4)\n",
        "print(\"‚úÖ Parallel Tiered GNINA Workflow initialized\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Resume Capability Implementation\n",
        "# =============================================================================\n",
        "\n",
        "import json\n",
        "import glob\n",
        "from datetime import datetime\n",
        "\n",
        "class ResumeCapableWorkflow(ParallelTieredWorkflow):\n",
        "    \"\"\"Workflow with resume capability for interrupted runs\"\"\"\n",
        "    \n",
        "    def __init__(self, config, max_workers=None, resume_file=\"workflow_state.json\"):\n",
        "        super().__init__(config, max_workers)\n",
        "        self.resume_file = resume_file\n",
        "        self.state_file = f\"results/{resume_file}\"\n",
        "        \n",
        "    def save_workflow_state(self, stage, completed_pairs, total_pairs, results=None):\n",
        "        \"\"\"Save current workflow state for resume capability\"\"\"\n",
        "        state = {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'stage': stage,\n",
        "            'completed_pairs': completed_pairs,\n",
        "            'total_pairs': total_pairs,\n",
        "            'config': self.config,\n",
        "            'stage_configs': self.stage_configs,\n",
        "            'completed_stages': list(self.stage_results.keys()),\n",
        "            'last_checkpoint': f\"stage_{stage}_checkpoint\"\n",
        "        }\n",
        "        \n",
        "        # Save state\n",
        "        with open(self.state_file, 'w') as f:\n",
        "            json.dump(state, f, indent=2)\n",
        "        \n",
        "        # Save checkpoint results if provided\n",
        "        if results:\n",
        "            checkpoint_file = f\"results/{state['last_checkpoint']}.json\"\n",
        "            with open(checkpoint_file, 'w') as f:\n",
        "                json.dump(results, f, indent=2)\n",
        "        \n",
        "        print(f\"üíæ Workflow state saved: {completed_pairs}/{total_pairs} pairs completed in stage {stage}\")\n",
        "    \n",
        "    def load_workflow_state(self):\n",
        "        \"\"\"Load previous workflow state\"\"\"\n",
        "        if not os.path.exists(self.state_file):\n",
        "            return None\n",
        "        \n",
        "        try:\n",
        "            with open(self.state_file, 'r') as f:\n",
        "                state = json.load(f)\n",
        "            \n",
        "            print(f\"üìÇ Found previous workflow state:\")\n",
        "            print(f\"   Timestamp: {state['timestamp']}\")\n",
        "            print(f\"   Stage: {state['stage']}\")\n",
        "            print(f\"   Progress: {state['completed_pairs']}/{state['total_pairs']}\")\n",
        "            print(f\"   Completed stages: {state['completed_stages']}\")\n",
        "            \n",
        "            return state\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error loading workflow state: {e}\")\n",
        "            return None\n",
        "    \n",
        "    def get_completed_pairs(self):\n",
        "        \"\"\"Get list of already completed pairs from existing results\"\"\"\n",
        "        completed_pairs = set()\n",
        "        \n",
        "        # Check all stage result files\n",
        "        for stage_dir in glob.glob(\"results/stage_*\"):\n",
        "            if os.path.isdir(stage_dir):\n",
        "                stage = stage_dir.split('_')[-1]\n",
        "                summary_file = f\"{stage_dir}/stage_{stage}_summary.csv\"\n",
        "                \n",
        "                if os.path.exists(summary_file):\n",
        "                    try:\n",
        "                        df = pd.read_csv(summary_file)\n",
        "                        for _, row in df.iterrows():\n",
        "                            pair_id = f\"{row['receptor']}_{row['site_id']}_{row['ligand']}\"\n",
        "                            completed_pairs.add(pair_id)\n",
        "                    except Exception as e:\n",
        "                        print(f\"‚ö†Ô∏è Error reading {summary_file}: {e}\")\n",
        "        \n",
        "        return completed_pairs\n",
        "    \n",
        "    def filter_uncompleted_pairs(self, pairlist_df, stage):\n",
        "        \"\"\"Filter out already completed pairs\"\"\"\n",
        "        completed_pairs = self.get_completed_pairs()\n",
        "        \n",
        "        if not completed_pairs:\n",
        "            return pairlist_df\n",
        "        \n",
        "        # Create pair IDs for current pairlist\n",
        "        pairlist_df['pair_id'] = (pairlist_df['receptor'] + '_' + \n",
        "                                 pairlist_df['site_id'] + '_' + \n",
        "                                 pairlist_df['ligand'])\n",
        "        \n",
        "        # Filter out completed pairs\n",
        "        uncompleted_df = pairlist_df[~pairlist_df['pair_id'].isin(completed_pairs)].copy()\n",
        "        uncompleted_df = uncompleted_df.drop('pair_id', axis=1)\n",
        "        \n",
        "        print(f\"üìä Resume filtering for stage {stage}:\")\n",
        "        print(f\"   Total pairs: {len(pairlist_df)}\")\n",
        "        print(f\"   Already completed: {len(completed_pairs)}\")\n",
        "        print(f\"   Remaining: {len(uncompleted_df)}\")\n",
        "        \n",
        "        return uncompleted_df\n",
        "    \n",
        "    def run_stage_with_resume(self, stage, pairlist_df, previous_results=None):\n",
        "        \"\"\"Run stage with resume capability\"\"\"\n",
        "        # Filter out already completed pairs\n",
        "        input_df = self.filter_uncompleted_pairs(pairlist_df, stage)\n",
        "        \n",
        "        if len(input_df) == 0:\n",
        "            print(f\"   ‚úÖ All pairs for stage {stage} already completed\")\n",
        "            return self._load_stage_results(stage)\n",
        "        \n",
        "        # Run the stage\n",
        "        results = self.run_stage_parallel(stage, input_df, previous_results)\n",
        "        \n",
        "        # Save state after completion\n",
        "        self.save_workflow_state(stage, len(pairlist_df), len(pairlist_df), results)\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def _load_stage_results(self, stage):\n",
        "        \"\"\"Load existing stage results\"\"\"\n",
        "        stage_dir = f\"results/stage_{stage}\"\n",
        "        results_file = f\"{stage_dir}/stage_{stage}_results.csv\"\n",
        "        \n",
        "        if os.path.exists(results_file):\n",
        "            try:\n",
        "                df = pd.read_csv(results_file)\n",
        "                # Convert back to result format (simplified)\n",
        "                results = []\n",
        "                for _, row in df.iterrows():\n",
        "                    result = {\n",
        "                        'status': 'success',  # Assume success if in results\n",
        "                        'stage': stage,\n",
        "                        'row': {\n",
        "                            'receptor': row.get('receptor', ''),\n",
        "                            'ligand': row.get('ligand', ''),\n",
        "                            'site_id': row.get('site_id', '')\n",
        "                        }\n",
        "                    }\n",
        "                    results.append(result)\n",
        "                \n",
        "                print(f\"   üìÇ Loaded {len(results)} existing results for stage {stage}\")\n",
        "                return results\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Error loading stage {stage} results: {e}\")\n",
        "        \n",
        "        return []\n",
        "    \n",
        "    def run_complete_workflow_with_resume(self, pairlist_df, stages=['A', 'B']):\n",
        "        \"\"\"Run complete workflow with resume capability\"\"\"\n",
        "        print(\"üéØ Starting Resume-Capable Parallel Tiered GNINA Workflow\")\n",
        "        \n",
        "        # Check for existing state\n",
        "        state = self.load_workflow_state()\n",
        "        if state:\n",
        "            response = input(\"Found previous workflow state. Resume? (y/n): \").lower()\n",
        "            if response == 'y':\n",
        "                print(\"üîÑ Resuming previous workflow...\")\n",
        "                # Load existing stage results\n",
        "                for stage in state['completed_stages']:\n",
        "                    self.stage_results[stage] = self._load_stage_results(stage)\n",
        "            else:\n",
        "                print(\"üÜï Starting fresh workflow...\")\n",
        "                # Clean up old state\n",
        "                if os.path.exists(self.state_file):\n",
        "                    os.remove(self.state_file)\n",
        "        \n",
        "        all_results = []\n",
        "        previous_results = None\n",
        "        \n",
        "        for stage in stages:\n",
        "            # Check if stage already completed\n",
        "            if stage in self.stage_results:\n",
        "                print(f\"   ‚úÖ Stage {stage} already completed, skipping...\")\n",
        "                previous_results = self.stage_results[stage]\n",
        "                all_results.extend(previous_results)\n",
        "                continue\n",
        "            \n",
        "            # Run stage with resume\n",
        "            stage_results = self.run_stage_with_resume(stage, pairlist_df, previous_results)\n",
        "            all_results.extend(stage_results)\n",
        "            previous_results = stage_results\n",
        "            \n",
        "            # Check if we should continue\n",
        "            if stage in ['A', 'B'] and len(stage_results) == 0:\n",
        "                print(f\"   ‚ö†Ô∏è No results from Stage {stage}, stopping workflow\")\n",
        "                break\n",
        "        \n",
        "        # Save complete workflow results\n",
        "        self._save_workflow_summary(all_results)\n",
        "        \n",
        "        # Clean up state file after completion\n",
        "        if os.path.exists(self.state_file):\n",
        "            os.remove(self.state_file)\n",
        "            print(\"üßπ Cleaned up workflow state file\")\n",
        "        \n",
        "        return all_results\n",
        "\n",
        "# Initialize resume-capable workflow\n",
        "resume_workflow = ResumeCapableWorkflow(CONFIG, max_workers=4)\n",
        "print(\"‚úÖ Resume-Capable Workflow initialized\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Main Execution - Choose Your Workflow\n",
        "# =============================================================================\n",
        "\n",
        "def run_workflow(workflow_type=\"resume\", stages=['A', 'B']):\n",
        "    \"\"\"\n",
        "    Run the selected workflow type\n",
        "    \n",
        "    Args:\n",
        "        workflow_type: \"simple\", \"tiered\", \"parallel\", \"resume\"\n",
        "        stages: List of stages to run ['A', 'B', 'C']\n",
        "    \"\"\"\n",
        "    \n",
        "    if 'pairlist_df' not in locals() or pairlist_df is None:\n",
        "        print(\"‚ùå Please ensure pairlist.csv is loaded before running workflow\")\n",
        "        return None\n",
        "    \n",
        "    print(f\"üéØ Running {workflow_type.upper()} workflow with stages: {stages}\")\n",
        "    \n",
        "    if workflow_type == \"simple\":\n",
        "        # Simple single-stage workflow\n",
        "        gnina_docker = GNINADocker(CONFIG)\n",
        "        results = []\n",
        "        for idx, row in tqdm(pairlist_df.iterrows(), total=len(pairlist_df), desc=\"Simple docking\"):\n",
        "            result = gnina_docker.run_docking(row)\n",
        "            results.append(result)\n",
        "        return results\n",
        "    \n",
        "    elif workflow_type == \"tiered\":\n",
        "        # Tiered CNN workflow (sequential)\n",
        "        return tiered_workflow.run_complete_workflow(pairlist_df, stages)\n",
        "    \n",
        "    elif workflow_type == \"parallel\":\n",
        "        # Parallel tiered workflow\n",
        "        return parallel_workflow.run_complete_workflow_parallel(pairlist_df, stages)\n",
        "    \n",
        "    elif workflow_type == \"resume\":\n",
        "        # Resume-capable parallel tiered workflow\n",
        "        return resume_workflow.run_complete_workflow_with_resume(pairlist_df, stages)\n",
        "    \n",
        "    else:\n",
        "        print(f\"‚ùå Unknown workflow type: {workflow_type}\")\n",
        "        return None\n",
        "\n",
        "# =============================================================================\n",
        "# Workflow Configuration Options\n",
        "# =============================================================================\n",
        "\n",
        "# Option 1: Quick single-stage run (rescore only)\n",
        "# results = run_workflow(\"simple\", stages=['A'])\n",
        "\n",
        "# Option 2: Two-stage tiered workflow (recommended for most cases)\n",
        "# results = run_workflow(\"tiered\", stages=['A', 'B'])\n",
        "\n",
        "# Option 3: Parallel two-stage workflow (faster for large datasets)\n",
        "# results = run_workflow(\"parallel\", stages=['A', 'B'])\n",
        "\n",
        "# Option 4: Resume-capable parallel workflow (best for production)\n",
        "# results = run_workflow(\"resume\", stages=['A', 'B'])\n",
        "\n",
        "# Option 5: Full three-stage workflow (for high-accuracy requirements)\n",
        "# results = run_workflow(\"resume\", stages=['A', 'B', 'C'])\n",
        "\n",
        "print(\"üöÄ Workflow execution functions ready!\")\n",
        "print(\"\\nAvailable workflow types:\")\n",
        "print(\"  1. simple    - Single-stage rescore (fastest)\")\n",
        "print(\"  2. tiered    - Two-stage CNN funnel (sequential)\")\n",
        "print(\"  3. parallel  - Two-stage CNN funnel (parallel)\")\n",
        "print(\"  4. resume    - Resume-capable parallel workflow (recommended)\")\n",
        "print(\"\\nExample usage:\")\n",
        "print(\"  results = run_workflow('resume', stages=['A', 'B'])\")\n",
        "print(\"  results = run_workflow('simple', stages=['A'])\")\n",
        "print(\"  results = run_workflow('parallel', stages=['A', 'B', 'C'])\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Visualization and Analysis Dashboard\n",
        "# =============================================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.patches import Rectangle\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.offline as pyo\n",
        "\n",
        "class GNINAVisualizationDashboard:\n",
        "    \"\"\"Comprehensive visualization dashboard for GNINA results\"\"\"\n",
        "    \n",
        "    def __init__(self, results_dir=\"results\"):\n",
        "        self.results_dir = results_dir\n",
        "        self.setup_plotting_style()\n",
        "    \n",
        "    def setup_plotting_style(self):\n",
        "        \"\"\"Setup consistent plotting style\"\"\"\n",
        "        plt.style.use('default')\n",
        "        sns.set_palette(\"husl\")\n",
        "        \n",
        "        # Set figure size and DPI\n",
        "        plt.rcParams['figure.figsize'] = (12, 8)\n",
        "        plt.rcParams['figure.dpi'] = 100\n",
        "        plt.rcParams['font.size'] = 10\n",
        "    \n",
        "    def create_workflow_overview(self, workflow_summary_file=\"results/workflow_summary.json\"):\n",
        "        \"\"\"Create overview of the complete workflow\"\"\"\n",
        "        if not os.path.exists(workflow_summary_file):\n",
        "            print(\"‚ùå Workflow summary file not found\")\n",
        "            return\n",
        "        \n",
        "        with open(workflow_summary_file, 'r') as f:\n",
        "            summary = json.load(f)\n",
        "        \n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "        fig.suptitle('GNINA Workflow Overview', fontsize=16, fontweight='bold')\n",
        "        \n",
        "        # 1. Stages completed\n",
        "        stages = summary.get('stages_completed', [])\n",
        "        axes[0, 0].bar(range(len(stages)), [1]*len(stages), color=['#2E8B57', '#4169E1', '#DC143C'][:len(stages)])\n",
        "        axes[0, 0].set_xticks(range(len(stages)))\n",
        "        axes[0, 0].set_xticklabels([f'Stage {s}' for s in stages])\n",
        "        axes[0, 0].set_title('Completed Stages')\n",
        "        axes[0, 0].set_ylabel('Status')\n",
        "        \n",
        "        # 2. Configuration parameters\n",
        "        config = summary.get('config', {})\n",
        "        params = ['exhaustiveness', 'num_modes', 'cpu_cores', 'batch_size']\n",
        "        values = [config.get(p, 0) for p in params]\n",
        "        axes[0, 1].bar(params, values, color='skyblue')\n",
        "        axes[0, 1].set_title('Configuration Parameters')\n",
        "        axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "        \n",
        "        # 3. Total pairs processed\n",
        "        total_pairs = summary.get('total_pairs_processed', 0)\n",
        "        axes[1, 0].pie([total_pairs], labels=['Total Pairs'], autopct='%1.0f', \n",
        "                      colors=['lightcoral'], startangle=90)\n",
        "        axes[1, 0].set_title('Total Pairs Processed')\n",
        "        \n",
        "        # 4. Workflow efficiency\n",
        "        stage_configs = summary.get('stage_configs', {})\n",
        "        if stage_configs:\n",
        "            stages_list = list(stage_configs.keys())\n",
        "            exhaustiveness = [stage_configs[s].get('exhaustiveness', 0) for s in stages_list]\n",
        "            axes[1, 1].plot(stages_list, exhaustiveness, marker='o', linewidth=2, markersize=8)\n",
        "            axes[1, 1].set_title('Exhaustiveness by Stage')\n",
        "            axes[1, 1].set_xlabel('Stage')\n",
        "            axes[1, 1].set_ylabel('Exhaustiveness')\n",
        "            axes[1, 1].grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{self.results_dir}/workflow_overview.png\", dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "    \n",
        "    def create_stage_analysis(self, stage='A'):\n",
        "        \"\"\"Create detailed analysis for a specific stage\"\"\"\n",
        "        stage_dir = f\"{self.results_dir}/stage_{stage}\"\n",
        "        summary_file = f\"{stage_dir}/stage_{stage}_summary.csv\"\n",
        "        \n",
        "        if not os.path.exists(summary_file):\n",
        "            print(f\"‚ùå Stage {stage} summary file not found\")\n",
        "            return\n",
        "        \n",
        "        df = pd.read_csv(summary_file)\n",
        "        \n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "        fig.suptitle(f'Stage {stage} Analysis', fontsize=16, fontweight='bold')\n",
        "        \n",
        "        # 1. CNN Score distribution\n",
        "        axes[0, 0].hist(df['cnn_score'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "        axes[0, 0].set_title('CNN Score Distribution')\n",
        "        axes[0, 0].set_xlabel('CNN Score')\n",
        "        axes[0, 0].set_ylabel('Frequency')\n",
        "        axes[0, 0].axvline(df['cnn_score'].mean(), color='red', linestyle='--', \n",
        "                          label=f'Mean: {df[\"cnn_score\"].mean():.3f}')\n",
        "        axes[0, 0].legend()\n",
        "        \n",
        "        # 2. CNN Affinity distribution\n",
        "        axes[0, 1].hist(df['cnn_affinity'], bins=20, alpha=0.7, color='lightgreen', edgecolor='black')\n",
        "        axes[0, 1].set_title('CNN Affinity Distribution')\n",
        "        axes[0, 1].set_xlabel('CNN Affinity')\n",
        "        axes[0, 1].set_ylabel('Frequency')\n",
        "        axes[0, 1].axvline(df['cnn_affinity'].mean(), color='red', linestyle='--',\n",
        "                          label=f'Mean: {df[\"cnn_affinity\"].mean():.3f}')\n",
        "        axes[0, 1].legend()\n",
        "        \n",
        "        # 3. Score vs Affinity scatter\n",
        "        axes[0, 2].scatter(df['cnn_score'], df['cnn_affinity'], alpha=0.6, color='purple')\n",
        "        axes[0, 2].set_title('CNN Score vs Affinity')\n",
        "        axes[0, 2].set_xlabel('CNN Score')\n",
        "        axes[0, 2].set_ylabel('CNN Affinity')\n",
        "        \n",
        "        # Add correlation coefficient\n",
        "        corr = df['cnn_score'].corr(df['cnn_affinity'])\n",
        "        axes[0, 2].text(0.05, 0.95, f'Correlation: {corr:.3f}', \n",
        "                       transform=axes[0, 2].transAxes, fontsize=10,\n",
        "                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "        \n",
        "        # 4. Top performers by receptor\n",
        "        receptor_counts = df['receptor'].value_counts().head(10)\n",
        "        axes[1, 0].barh(range(len(receptor_counts)), receptor_counts.values, color='orange')\n",
        "        axes[1, 0].set_yticks(range(len(receptor_counts)))\n",
        "        axes[1, 0].set_yticklabels(receptor_counts.index)\n",
        "        axes[1, 0].set_title('Top 10 Receptors by Hit Count')\n",
        "        axes[1, 0].set_xlabel('Number of Hits')\n",
        "        \n",
        "        # 5. Top performers by ligand\n",
        "        ligand_counts = df['ligand'].value_counts().head(10)\n",
        "        axes[1, 1].barh(range(len(ligand_counts)), ligand_counts.values, color='pink')\n",
        "        axes[1, 1].set_yticks(range(len(ligand_counts)))\n",
        "        axes[1, 1].set_yticklabels(ligand_counts.index)\n",
        "        axes[1, 1].set_title('Top 10 Ligands by Hit Count')\n",
        "        axes[1, 1].set_xlabel('Number of Hits')\n",
        "        \n",
        "        # 6. Score statistics\n",
        "        stats_data = {\n",
        "            'Metric': ['Mean', 'Median', 'Std', 'Min', 'Max'],\n",
        "            'CNN Score': [\n",
        "                df['cnn_score'].mean(),\n",
        "                df['cnn_score'].median(),\n",
        "                df['cnn_score'].std(),\n",
        "                df['cnn_score'].min(),\n",
        "                df['cnn_score'].max()\n",
        "            ],\n",
        "            'CNN Affinity': [\n",
        "                df['cnn_affinity'].mean(),\n",
        "                df['cnn_affinity'].median(),\n",
        "                df['cnn_affinity'].std(),\n",
        "                df['cnn_affinity'].min(),\n",
        "                df['cnn_affinity'].max()\n",
        "            ]\n",
        "        }\n",
        "        \n",
        "        stats_df = pd.DataFrame(stats_data)\n",
        "        axes[1, 2].axis('tight')\n",
        "        axes[1, 2].axis('off')\n",
        "        table = axes[1, 2].table(cellText=stats_df.values, colLabels=stats_df.columns,\n",
        "                               cellLoc='center', loc='center')\n",
        "        table.auto_set_font_size(False)\n",
        "        table.set_fontsize(9)\n",
        "        table.scale(1.2, 1.5)\n",
        "        axes[1, 2].set_title('Score Statistics')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{stage_dir}/stage_{stage}_analysis.png\", dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "    \n",
        "    def create_interactive_dashboard(self):\n",
        "        \"\"\"Create interactive Plotly dashboard\"\"\"\n",
        "        # Collect data from all stages\n",
        "        all_data = []\n",
        "        \n",
        "        for stage_dir in glob.glob(f\"{self.results_dir}/stage_*\"):\n",
        "            if os.path.isdir(stage_dir):\n",
        "                stage = stage_dir.split('_')[-1]\n",
        "                summary_file = f\"{stage_dir}/stage_{stage}_summary.csv\"\n",
        "                \n",
        "                if os.path.exists(summary_file):\n",
        "                    df = pd.read_csv(summary_file)\n",
        "                    df['stage'] = stage\n",
        "                    all_data.append(df)\n",
        "        \n",
        "        if not all_data:\n",
        "            print(\"‚ùå No stage data found for interactive dashboard\")\n",
        "            return\n",
        "        \n",
        "        combined_df = pd.concat(all_data, ignore_index=True)\n",
        "        \n",
        "        # Create subplots\n",
        "        fig = make_subplots(\n",
        "            rows=2, cols=2,\n",
        "            subplot_titles=('CNN Score by Stage', 'CNN Affinity by Stage', \n",
        "                          'Score vs Affinity', 'Top Performers'),\n",
        "            specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "                   [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
        "        )\n",
        "        \n",
        "        # 1. CNN Score by stage\n",
        "        for stage in combined_df['stage'].unique():\n",
        "            stage_data = combined_df[combined_df['stage'] == stage]\n",
        "            fig.add_trace(\n",
        "                go.Box(y=stage_data['cnn_score'], name=f'Stage {stage}'),\n",
        "                row=1, col=1\n",
        "            )\n",
        "        \n",
        "        # 2. CNN Affinity by stage\n",
        "        for stage in combined_df['stage'].unique():\n",
        "            stage_data = combined_df[combined_df['stage'] == stage]\n",
        "            fig.add_trace(\n",
        "                go.Box(y=stage_data['cnn_affinity'], name=f'Stage {stage}'),\n",
        "                row=1, col=2\n",
        "            )\n",
        "        \n",
        "        # 3. Score vs Affinity scatter\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=combined_df['cnn_score'], y=combined_df['cnn_affinity'],\n",
        "                      mode='markers', name='All Results',\n",
        "                      marker=dict(color=combined_df['stage'], \n",
        "                                colorscale='Viridis',\n",
        "                                showscale=True,\n",
        "                                colorbar=dict(title=\"Stage\"))),\n",
        "            row=2, col=1\n",
        "        )\n",
        "        \n",
        "        # 4. Top performers\n",
        "        top_performers = combined_df.nlargest(20, 'cnn_score')\n",
        "        fig.add_trace(\n",
        "            go.Bar(x=top_performers['cnn_score'],\n",
        "                  y=[f\"{row['receptor']}-{row['ligand']}\" for _, row in top_performers.iterrows()],\n",
        "                  orientation='h', name='Top 20'),\n",
        "            row=2, col=2\n",
        "        )\n",
        "        \n",
        "        # Update layout\n",
        "        fig.update_layout(\n",
        "            title_text=\"GNINA Interactive Dashboard\",\n",
        "            showlegend=False,\n",
        "            height=800\n",
        "        )\n",
        "        \n",
        "        # Save and show\n",
        "        fig.write_html(f\"{self.results_dir}/interactive_dashboard.html\")\n",
        "        fig.show()\n",
        "        \n",
        "        print(f\"üìä Interactive dashboard saved to {self.results_dir}/interactive_dashboard.html\")\n",
        "    \n",
        "    def create_comparison_plot(self, stages=['A', 'B']):\n",
        "        \"\"\"Create comparison plot between stages\"\"\"\n",
        "        stage_data = {}\n",
        "        \n",
        "        for stage in stages:\n",
        "            stage_dir = f\"{self.results_dir}/stage_{stage}\"\n",
        "            summary_file = f\"{stage_dir}/stage_{stage}_summary.csv\"\n",
        "            \n",
        "            if os.path.exists(summary_file):\n",
        "                stage_data[stage] = pd.read_csv(summary_file)\n",
        "        \n",
        "        if len(stage_data) < 2:\n",
        "            print(\"‚ùå Need at least 2 stages for comparison\")\n",
        "            return\n",
        "        \n",
        "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "        fig.suptitle('Stage Comparison Analysis', fontsize=16, fontweight='bold')\n",
        "        \n",
        "        # 1. Score comparison\n",
        "        for stage, df in stage_data.items():\n",
        "            axes[0].hist(df['cnn_score'], alpha=0.6, label=f'Stage {stage}', bins=20)\n",
        "        axes[0].set_title('CNN Score Distribution Comparison')\n",
        "        axes[0].set_xlabel('CNN Score')\n",
        "        axes[0].set_ylabel('Frequency')\n",
        "        axes[0].legend()\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "        \n",
        "        # 2. Affinity comparison\n",
        "        for stage, df in stage_data.items():\n",
        "            axes[1].hist(df['cnn_affinity'], alpha=0.6, label=f'Stage {stage}', bins=20)\n",
        "        axes[1].set_title('CNN Affinity Distribution Comparison')\n",
        "        axes[1].set_xlabel('CNN Affinity')\n",
        "        axes[1].set_ylabel('Frequency')\n",
        "        axes[1].legend()\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{self.results_dir}/stage_comparison.png\", dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "# Initialize visualization dashboard\n",
        "viz_dashboard = GNINAVisualizationDashboard()\n",
        "print(\"‚úÖ Visualization Dashboard initialized\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Execute Workflow and Generate Visualizations\n",
        "# =============================================================================\n",
        "\n",
        "# Uncomment the workflow you want to run:\n",
        "\n",
        "# Option 1: Quick single-stage run (recommended for testing)\n",
        "# results = run_workflow(\"simple\", stages=['A'])\n",
        "\n",
        "# Option 2: Two-stage tiered workflow (recommended for most cases)\n",
        "# results = run_workflow(\"tiered\", stages=['A', 'B'])\n",
        "\n",
        "# Option 3: Parallel two-stage workflow (faster for large datasets)\n",
        "# results = run_workflow(\"parallel\", stages=['A', 'B'])\n",
        "\n",
        "# Option 4: Resume-capable parallel workflow (best for production)\n",
        "# results = run_workflow(\"resume\", stages=['A', 'B'])\n",
        "\n",
        "# Option 5: Full three-stage workflow (for high-accuracy requirements)\n",
        "# results = run_workflow(\"resume\", stages=['A', 'B', 'C'])\n",
        "\n",
        "# =============================================================================\n",
        "# Generate Visualizations (run after workflow completion)\n",
        "# =============================================================================\n",
        "\n",
        "def generate_all_visualizations():\n",
        "    \"\"\"Generate all available visualizations\"\"\"\n",
        "    print(\"üìä Generating comprehensive visualizations...\")\n",
        "    \n",
        "    # 1. Workflow overview\n",
        "    print(\"   Creating workflow overview...\")\n",
        "    viz_dashboard.create_workflow_overview()\n",
        "    \n",
        "    # 2. Stage analysis for each completed stage\n",
        "    for stage_dir in glob.glob(\"results/stage_*\"):\n",
        "        if os.path.isdir(stage_dir):\n",
        "            stage = stage_dir.split('_')[-1]\n",
        "            print(f\"   Creating stage {stage} analysis...\")\n",
        "            viz_dashboard.create_stage_analysis(stage)\n",
        "    \n",
        "    # 3. Interactive dashboard\n",
        "    print(\"   Creating interactive dashboard...\")\n",
        "    viz_dashboard.create_interactive_dashboard()\n",
        "    \n",
        "    # 4. Stage comparison (if multiple stages)\n",
        "    completed_stages = [d.split('_')[-1] for d in glob.glob(\"results/stage_*\") if os.path.isdir(d)]\n",
        "    if len(completed_stages) > 1:\n",
        "        print(\"   Creating stage comparison...\")\n",
        "        viz_dashboard.create_comparison_plot(completed_stages)\n",
        "    \n",
        "    print(\"‚úÖ All visualizations generated!\")\n",
        "\n",
        "# Uncomment to generate visualizations after running workflow:\n",
        "# generate_all_visualizations()\n",
        "\n",
        "print(\"üéØ Ready to execute workflow!\")\n",
        "print(\"\\nTo run a workflow, uncomment one of the options above:\")\n",
        "print(\"  - Simple: run_workflow('simple', stages=['A'])\")\n",
        "print(\"  - Tiered: run_workflow('tiered', stages=['A', 'B'])\")\n",
        "print(\"  - Parallel: run_workflow('parallel', stages=['A', 'B'])\")\n",
        "print(\"  - Resume: run_workflow('resume', stages=['A', 'B'])\")\n",
        "print(\"\\nTo generate visualizations after completion:\")\n",
        "print(\"  generate_all_visualizations()\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Phase 2: Flexible Receptor Docking Implementation\n",
        "# =============================================================================\n",
        "\n",
        "class FlexibleReceptorDocker(GNINADocker):\n",
        "    \"\"\"Enhanced GNINA docker with flexible receptor capabilities\"\"\"\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.flexible_residues = {}\n",
        "        self.flexible_configs = {\n",
        "            'auto_detect': True,           # Auto-detect flexible residues\n",
        "            'distance_threshold': 5.0,     # Distance from binding site (√Ö)\n",
        "            'max_flexible_residues': 20,   # Maximum flexible residues\n",
        "            'flexible_chains': [],         # Specific chains to make flexible\n",
        "            'exclude_residues': [],        # Residues to exclude from flexibility\n",
        "            'flexdist': 3.5,              # GNINA flexdist parameter\n",
        "            'flexres': None               # Manual flexible residue specification\n",
        "        }\n",
        "    \n",
        "    def detect_flexible_residues(self, receptor_pdb, binding_center, distance_threshold=5.0):\n",
        "        \"\"\"Auto-detect flexible residues near binding site\"\"\"\n",
        "        try:\n",
        "            from Bio.PDB import PDBParser, NeighborSearch\n",
        "            from Bio.PDB.PDBExceptions import PDBConstructionWarning\n",
        "            import warnings\n",
        "            warnings.simplefilter('ignore', PDBConstructionWarning)\n",
        "            \n",
        "            parser = PDBParser(QUIET=True)\n",
        "            structure = parser.get_structure('receptor', receptor_pdb)\n",
        "            \n",
        "            # Get all atoms\n",
        "            atoms = []\n",
        "            for model in structure:\n",
        "                for chain in model:\n",
        "                    for residue in chain:\n",
        "                        for atom in residue:\n",
        "                            atoms.append(atom)\n",
        "            \n",
        "            # Create neighbor search\n",
        "            ns = NeighborSearch(atoms)\n",
        "            \n",
        "            # Find residues within distance of binding center\n",
        "            center_atom = None\n",
        "            min_distance = float('inf')\n",
        "            \n",
        "            for atom in atoms:\n",
        "                dist = atom.coord - np.array(binding_center)\n",
        "                dist = np.linalg.norm(dist)\n",
        "                if dist < min_distance:\n",
        "                    min_distance = dist\n",
        "                    center_atom = atom\n",
        "            \n",
        "            if center_atom is None:\n",
        "                return []\n",
        "            \n",
        "            # Find neighbors within threshold\n",
        "            neighbors = ns.search(center_atom.coord, distance_threshold, level='R')\n",
        "            \n",
        "            flexible_residues = []\n",
        "            for residue in neighbors:\n",
        "                chain_id = residue.parent.id\n",
        "                res_num = residue.id[1]\n",
        "                res_name = residue.resname\n",
        "                flexible_residues.append(f\"{chain_id}:{res_num}\")\n",
        "            \n",
        "            return sorted(flexible_residues)\n",
        "            \n",
        "        except ImportError:\n",
        "            print(\"‚ö†Ô∏è BioPython not available for auto-detection. Using manual specification.\")\n",
        "            return []\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error in auto-detection: {e}\")\n",
        "            return []\n",
        "    \n",
        "    def set_flexible_residues(self, receptor, flexible_residues=None, auto_detect=True, \n",
        "                            binding_center=None, distance_threshold=5.0):\n",
        "        \"\"\"Set flexible residues for a receptor\"\"\"\n",
        "        \n",
        "        if flexible_residues is not None:\n",
        "            # Manual specification\n",
        "            self.flexible_residues[receptor] = flexible_residues\n",
        "            print(f\"‚úÖ Set manual flexible residues for {receptor}: {flexible_residues}\")\n",
        "            \n",
        "        elif auto_detect and binding_center is not None:\n",
        "            # Auto-detection\n",
        "            receptor_pdb = f\"receptors_prep/{receptor}.pdbqt\"\n",
        "            if os.path.exists(receptor_pdb):\n",
        "                detected = self.detect_flexible_residues(receptor_pdb, binding_center, distance_threshold)\n",
        "                \n",
        "                # Apply limits\n",
        "                if len(detected) > self.flexible_configs['max_flexible_residues']:\n",
        "                    detected = detected[:self.flexible_configs['max_flexible_residues']]\n",
        "                    print(f\"‚ö†Ô∏è Limited flexible residues to {self.flexible_configs['max_flexible_residues']}\")\n",
        "                \n",
        "                self.flexible_residues[receptor] = detected\n",
        "                print(f\"‚úÖ Auto-detected {len(detected)} flexible residues for {receptor}: {detected}\")\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è Receptor file not found: {receptor_pdb}\")\n",
        "                self.flexible_residues[receptor] = []\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è No flexible residues set for {receptor}\")\n",
        "            self.flexible_residues[receptor] = []\n",
        "    \n",
        "    def build_gnina_command(self, row):\n",
        "        \"\"\"Build GNINA command with flexible receptor support\"\"\"\n",
        "        receptor = f\"receptors_prep/{row['receptor']}.pdbqt\"\n",
        "        ligand = f\"ligands_prep/{row['ligand']}.pdbqt\"\n",
        "        \n",
        "        # Output files\n",
        "        tag = f\"{row['receptor']}_{row['site_id']}_{row['ligand']}\"\n",
        "        output_sdf = f\"gnina_out/{tag}_poses.sdf\"\n",
        "        log_file = f\"logs/{tag}.log\"\n",
        "        flex_output = f\"gnina_out/{tag}_flex.pdbqt\"\n",
        "        \n",
        "        # Base command\n",
        "        cmd = [\n",
        "            self.gnina_bin,\n",
        "            \"--receptor\", receptor,\n",
        "            \"--ligand\", ligand,\n",
        "            \"--out\", output_sdf,\n",
        "            \"--log\", log_file,\n",
        "        ]\n",
        "        \n",
        "        # Docking box parameters\n",
        "        cmd.extend([\n",
        "            \"--center_x\", str(row['center_x']),\n",
        "            \"--center_y\", str(row['center_y']),\n",
        "            \"--center_z\", str(row['center_z']),\n",
        "            \"--size_x\", str(row['size_x']),\n",
        "            \"--size_y\", str(row['size_y']),\n",
        "            \"--size_z\", str(row['size_z']),\n",
        "        ])\n",
        "        \n",
        "        # Performance parameters\n",
        "        cmd.extend([\n",
        "            \"--exhaustiveness\", str(self.config['exhaustiveness']),\n",
        "            \"--num_modes\", str(self.config['num_modes']),\n",
        "            \"--seed\", str(self.config['seed']),\n",
        "            \"--cpu\", str(self.config['cpu_cores']),\n",
        "        ])\n",
        "        \n",
        "        # Advanced GNINA features\n",
        "        cmd.extend([\n",
        "            \"--cnn_scoring\", self.config['cnn_scoring'],\n",
        "            \"--cnn_rotation\", \"0\",\n",
        "            \"--min_rmsd_filter\", \"1.0\",\n",
        "            \"--pose_sort_order\", \"0\",\n",
        "        ])\n",
        "        \n",
        "        # Flexible receptor parameters\n",
        "        if row['receptor'] in self.flexible_residues:\n",
        "            flex_residues = self.flexible_residues[row['receptor']]\n",
        "            if flex_residues:\n",
        "                flexres_str = \",\".join(flex_residues)\n",
        "                cmd.extend([\n",
        "                    \"--flexres\", flexres_str,\n",
        "                    \"--flexdist\", str(self.flexible_configs['flexdist']),\n",
        "                    \"--out_flex\", flex_output\n",
        "                ])\n",
        "                print(f\"   üîÑ Using flexible residues: {flexres_str}\")\n",
        "        \n",
        "        # GPU support\n",
        "        if self.config['use_gpu']:\n",
        "            cmd.append(\"--gpu\")\n",
        "            cmd.extend([\"--device\", \"0\"])\n",
        "        \n",
        "        return cmd, output_sdf, log_file, flex_output if row['receptor'] in self.flexible_residues else None\n",
        "\n",
        "class FlexibleTieredWorkflow(ResumeCapableWorkflow):\n",
        "    \"\"\"Tiered workflow with flexible receptor support\"\"\"\n",
        "    \n",
        "    def __init__(self, config, max_workers=None, resume_file=\"workflow_state.json\"):\n",
        "        super().__init__(config, max_workers, resume_file)\n",
        "        self.flexible_docker = FlexibleReceptorDocker(config)\n",
        "    \n",
        "    def configure_flexible_docking(self, flexible_config):\n",
        "        \"\"\"Configure flexible docking parameters\"\"\"\n",
        "        self.flexible_docker.flexible_configs.update(flexible_config)\n",
        "        print(\"‚úÖ Flexible docking configuration updated\")\n",
        "    \n",
        "    def set_receptor_flexibility(self, receptor, flexible_residues=None, \n",
        "                               auto_detect=True, binding_center=None):\n",
        "        \"\"\"Set flexible residues for a specific receptor\"\"\"\n",
        "        self.flexible_docker.set_flexible_residues(\n",
        "            receptor, flexible_residues, auto_detect, binding_center\n",
        "        )\n",
        "    \n",
        "    def set_bulk_flexibility(self, pairlist_df, auto_detect=True):\n",
        "        \"\"\"Set flexible residues for all receptors in pairlist\"\"\"\n",
        "        print(\"üîÑ Configuring flexible residues for all receptors...\")\n",
        "        \n",
        "        for _, row in pairlist_df.iterrows():\n",
        "            receptor = row['receptor']\n",
        "            if receptor not in self.flexible_docker.flexible_residues:\n",
        "                binding_center = [row['center_x'], row['center_y'], row['center_z']]\n",
        "                self.flexible_docker.set_flexible_residues(\n",
        "                    receptor, auto_detect=auto_detect, binding_center=binding_center\n",
        "                )\n",
        "    \n",
        "    def run_stage_parallel(self, stage, pairlist_df, previous_results=None):\n",
        "        \"\"\"Run stage with flexible receptor support\"\"\"\n",
        "        stage_config = self.stage_configs[stage]\n",
        "        \n",
        "        print(f\"\\nüöÄ Starting Stage {stage} (Flexible): {stage_config['description']}\")\n",
        "        print(f\"   CNN Scoring: {stage_config['cnn_scoring']}\")\n",
        "        print(f\"   Exhaustiveness: {stage_config['exhaustiveness']}\")\n",
        "        print(f\"   Num Modes: {stage_config['num_modes']}\")\n",
        "        print(f\"   Max Workers: {self.max_workers}\")\n",
        "        \n",
        "        # Filter input based on previous stage results\n",
        "        if stage == 'A':\n",
        "            input_df = pairlist_df.copy()\n",
        "        else:\n",
        "            input_df = self._filter_for_stage(stage, previous_results, pairlist_df)\n",
        "        \n",
        "        if len(input_df) == 0:\n",
        "            print(f\"   ‚ö†Ô∏è No ligands to process for Stage {stage}\")\n",
        "            return []\n",
        "        \n",
        "        print(f\"   Processing {len(input_df)} ligand-receptor pairs\")\n",
        "        \n",
        "        # Update config for this stage\n",
        "        stage_config_copy = self.config.copy()\n",
        "        stage_config_copy.update({\n",
        "            'cnn_scoring': stage_config['cnn_scoring'],\n",
        "            'exhaustiveness': stage_config['exhaustiveness'],\n",
        "            'num_modes': stage_config['num_modes']\n",
        "        })\n",
        "        \n",
        "        # Update flexible docker config\n",
        "        self.flexible_docker.config = stage_config_copy\n",
        "        \n",
        "        # Run parallel docking with flexible receptors\n",
        "        results = self._run_flexible_parallel_docking(input_df)\n",
        "        \n",
        "        # Add stage information\n",
        "        for result in results:\n",
        "            result['stage'] = stage\n",
        "            result['stage_config'] = stage_config\n",
        "        \n",
        "        # Save stage results\n",
        "        self.stage_results[stage] = results\n",
        "        self._save_stage_results(stage, results)\n",
        "        \n",
        "        # Print stage summary\n",
        "        self._print_stage_summary(stage, results)\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def _run_flexible_parallel_docking(self, pairlist_df, batch_size=None):\n",
        "        \"\"\"Run flexible docking with parallel processing\"\"\"\n",
        "        if batch_size is None:\n",
        "            batch_size = self.config.get('batch_size', 5)\n",
        "        \n",
        "        total_pairs = len(pairlist_df)\n",
        "        print(f\"üöÄ Starting flexible parallel docking: {total_pairs} pairs\")\n",
        "        print(f\"   Max workers: {self.max_workers}\")\n",
        "        print(f\"   Batch size: {batch_size}\")\n",
        "        \n",
        "        # Split into batches\n",
        "        batches = [pairlist_df.iloc[i:i+batch_size] for i in range(0, total_pairs, batch_size)]\n",
        "        \n",
        "        all_results = []\n",
        "        successful = 0\n",
        "        failed = 0\n",
        "        \n",
        "        # Process batches in parallel\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
        "            # Submit all batches\n",
        "            future_to_batch = {\n",
        "                executor.submit(self._process_flexible_batch, batch, batch_idx): batch_idx \n",
        "                for batch_idx, batch in enumerate(batches)\n",
        "            }\n",
        "            \n",
        "            # Collect results as they complete\n",
        "            for future in tqdm(concurrent.futures.as_completed(future_to_batch), \n",
        "                             total=len(batches), desc=\"Processing flexible batches\"):\n",
        "                batch_idx = future_to_batch[future]\n",
        "                try:\n",
        "                    batch_results = future.result()\n",
        "                    all_results.extend(batch_results)\n",
        "                    \n",
        "                    # Count successes/failures\n",
        "                    for result in batch_results:\n",
        "                        if result['status'] == 'success':\n",
        "                            successful += 1\n",
        "                        else:\n",
        "                            failed += 1\n",
        "                            \n",
        "                except Exception as e:\n",
        "                    print(f\"‚ùå Flexible batch {batch_idx} failed: {e}\")\n",
        "                    failed += len(batches[batch_idx])\n",
        "        \n",
        "        # Summary\n",
        "        print(f\"\\nüìä Flexible Parallel Docking Summary:\")\n",
        "        print(f\"   Total pairs: {total_pairs}\")\n",
        "        print(f\"   Successful: {successful}\")\n",
        "        print(f\"   Failed: {failed}\")\n",
        "        print(f\"   Success rate: {successful/total_pairs*100:.1f}%\")\n",
        "        \n",
        "        return all_results\n",
        "    \n",
        "    def _process_flexible_batch(self, batch_df, batch_idx):\n",
        "        \"\"\"Process a single batch with flexible docking\"\"\"\n",
        "        batch_results = []\n",
        "        \n",
        "        for idx, row in batch_df.iterrows():\n",
        "            try:\n",
        "                result = self.flexible_docker.run_docking(row)\n",
        "                result['batch_idx'] = batch_idx\n",
        "                batch_results.append(result)\n",
        "                \n",
        "                # Update progress\n",
        "                with self.progress_lock:\n",
        "                    status = \"‚úÖ\" if result['status'] == 'success' else \"‚ùå\"\n",
        "                    flex_info = \" (Flex)\" if row['receptor'] in self.flexible_docker.flexible_residues else \"\"\n",
        "                    print(f\"   {status} Batch {batch_idx}: {row['receptor']}-{row['ligand']}{flex_info}\")\n",
        "                    \n",
        "            except Exception as e:\n",
        "                error_result = {\n",
        "                    'status': 'error',\n",
        "                    'error': str(e),\n",
        "                    'row': row,\n",
        "                    'batch_idx': batch_idx\n",
        "                }\n",
        "                batch_results.append(error_result)\n",
        "        \n",
        "        return batch_results\n",
        "\n",
        "# Initialize flexible workflow\n",
        "flexible_workflow = FlexibleTieredWorkflow(CONFIG, max_workers=4)\n",
        "print(\"‚úÖ Flexible Receptor Workflow initialized\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# PDB Preparation Wizard Integration\n",
        "# =============================================================================\n",
        "\n",
        "class PDBPreparationIntegration:\n",
        "    \"\"\"Integration with PDB preparation wizard for enhanced preprocessing\"\"\"\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.pdb_wizard_config = {\n",
        "            'force_field': 'AMBER',\n",
        "            'ph': 7.4,\n",
        "            'plip_enabled': True,\n",
        "            'clean_structure': True,\n",
        "            'add_hydrogens': True,\n",
        "            'optimize_geometry': False,\n",
        "            'validate_structures': True\n",
        "        }\n",
        "    \n",
        "    def download_pdb_wizard(self):\n",
        "        \"\"\"Download and setup PDB preparation wizard\"\"\"\n",
        "        print(\"üì• Downloading PDB preparation wizard...\")\n",
        "        \n",
        "        # Clone the repository\n",
        "        if not os.path.exists(\"pdb-prepare-wizard\"):\n",
        "            !git clone https://github.com/OASolliman590/pdb-prepare-wizard.git\n",
        "        else:\n",
        "            print(\"‚úÖ PDB preparation wizard already exists\")\n",
        "        \n",
        "        # Install dependencies\n",
        "        print(\"üì¶ Installing PDB wizard dependencies...\")\n",
        "        %pip install -q biopython plip pdb2pqr\n",
        "        \n",
        "        print(\"‚úÖ PDB preparation wizard setup complete\")\n",
        "    \n",
        "    def prepare_ligands_with_wizard(self, ligands_dir=\"ligands_raw\", output_dir=\"ligands_prep\"):\n",
        "        \"\"\"Prepare ligands using the PDB wizard\"\"\"\n",
        "        print(\"üß™ Preparing ligands with PDB wizard...\")\n",
        "        \n",
        "        # Create output directory\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        \n",
        "        # Use the wizard's ligand preparation\n",
        "        wizard_script = \"\"\"\n",
        "import sys\n",
        "sys.path.append('pdb-prepare-wizard')\n",
        "from autodock_preparation import AutoDockPreparationPipeline, PreparationConfig\n",
        "\n",
        "# Create configuration\n",
        "config = PreparationConfig(\n",
        "    ligands_input='{ligands_dir}',\n",
        "    receptors_input='receptors_raw',  # Dummy, not used for ligands\n",
        "    ligands_output='{output_dir}',\n",
        "    receptors_output='receptors_prep',  # Dummy, not used for ligands\n",
        "    force_field='{force_field}',\n",
        "    ph={ph},\n",
        "    plip_enabled={plip_enabled}\n",
        ")\n",
        "\n",
        "# Initialize and run pipeline\n",
        "pipeline = AutoDockPreparationPipeline(config)\n",
        "success = pipeline.run_enhanced_preparation()\n",
        "\n",
        "if success:\n",
        "    print(\"‚úÖ Ligand preparation completed successfully\")\n",
        "else:\n",
        "    print(\"‚ùå Ligand preparation failed\")\n",
        "\"\"\".format(\n",
        "            ligands_dir=ligands_dir,\n",
        "            output_dir=output_dir,\n",
        "            force_field=self.pdb_wizard_config['force_field'],\n",
        "            ph=self.pdb_wizard_config['ph'],\n",
        "            plip_enabled=self.pdb_wizard_config['plip_enabled']\n",
        "        )\n",
        "        \n",
        "        # Write and execute script\n",
        "        with open('prepare_ligands.py', 'w') as f:\n",
        "            f.write(wizard_script)\n",
        "        \n",
        "        try:\n",
        "            !python prepare_ligands.py\n",
        "            print(\"‚úÖ Ligands prepared successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error preparing ligands: {e}\")\n",
        "            # Fallback to basic preparation\n",
        "            self._fallback_ligand_preparation(ligands_dir, output_dir)\n",
        "    \n",
        "    def prepare_receptors_with_wizard(self, receptors_dir=\"receptors_raw\", output_dir=\"receptors_prep\"):\n",
        "        \"\"\"Prepare receptors using the PDB wizard\"\"\"\n",
        "        print(\"üß¨ Preparing receptors with PDB wizard...\")\n",
        "        \n",
        "        # Create output directory\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        \n",
        "        # Use the wizard's receptor preparation\n",
        "        wizard_script = \"\"\"\n",
        "import sys\n",
        "sys.path.append('pdb-prepare-wizard')\n",
        "from autodock_preparation import AutoDockPreparationPipeline, PreparationConfig\n",
        "\n",
        "# Create configuration\n",
        "config = PreparationConfig(\n",
        "    ligands_input='ligands_raw',  # Dummy, not used for receptors\n",
        "    receptors_input='{receptors_dir}',\n",
        "    ligands_output='ligands_prep',  # Dummy, not used for receptors\n",
        "    receptors_output='{output_dir}',\n",
        "    force_field='{force_field}',\n",
        "    ph={ph},\n",
        "    plip_enabled={plip_enabled}\n",
        ")\n",
        "\n",
        "# Initialize and run pipeline\n",
        "pipeline = AutoDockPreparationPipeline(config)\n",
        "success = pipeline.run_enhanced_preparation()\n",
        "\n",
        "if success:\n",
        "    results = pipeline.analyze_preparation_results('{output_dir}')\n",
        "    print(f\"‚úÖ Receptors prepared: {{results['receptors']['count']}}\")\n",
        "else:\n",
        "    print(\"‚ùå Receptor preparation failed\")\n",
        "\"\"\".format(\n",
        "            receptors_dir=receptors_dir,\n",
        "            output_dir=output_dir,\n",
        "            force_field=self.pdb_wizard_config['force_field'],\n",
        "            ph=self.pdb_wizard_config['ph'],\n",
        "            plip_enabled=self.pdb_wizard_config['plip_enabled']\n",
        "        )\n",
        "        \n",
        "        # Write and execute script\n",
        "        with open('prepare_receptors.py', 'w') as f:\n",
        "            f.write(wizard_script)\n",
        "        \n",
        "        try:\n",
        "            !python prepare_receptors.py\n",
        "            print(\"‚úÖ Receptors prepared successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error preparing receptors: {e}\")\n",
        "            # Fallback to basic preparation\n",
        "            self._fallback_receptor_preparation(receptors_dir, output_dir)\n",
        "    \n",
        "    def _fallback_ligand_preparation(self, ligands_dir, output_dir):\n",
        "        \"\"\"Fallback ligand preparation using basic tools\"\"\"\n",
        "        print(\"üîÑ Using fallback ligand preparation...\")\n",
        "        \n",
        "        for ligand_file in os.listdir(ligands_dir):\n",
        "            if ligand_file.endswith(('.sdf', '.mol2', '.pdb')):\n",
        "                input_path = os.path.join(ligands_dir, ligand_file)\n",
        "                output_path = os.path.join(output_dir, ligand_file.replace('.sdf', '.pdbqt').replace('.mol2', '.pdbqt').replace('.pdb', '.pdbqt'))\n",
        "                \n",
        "                try:\n",
        "                    # Use meeko for preparation\n",
        "                    !mk_prepare_ligand.py -i \"{input_path}\" -o \"{output_path}\"\n",
        "                except:\n",
        "                    print(f\"‚ö†Ô∏è Failed to prepare {ligand_file}\")\n",
        "    \n",
        "    def _fallback_receptor_preparation(self, receptors_dir, output_dir):\n",
        "        \"\"\"Fallback receptor preparation using basic tools\"\"\"\n",
        "        print(\"üîÑ Using fallback receptor preparation...\")\n",
        "        \n",
        "        for receptor_file in os.listdir(receptors_dir):\n",
        "            if receptor_file.endswith('.pdb'):\n",
        "                input_path = os.path.join(receptors_dir, receptor_file)\n",
        "                output_path = os.path.join(output_dir, receptor_file.replace('.pdb', '.pdbqt'))\n",
        "                \n",
        "                try:\n",
        "                    # Use meeko for preparation\n",
        "                    !mk_prepare_receptor.py --read_pdb \"{input_path}\" -p \"{output_path}\" --allow_bad_res --default_altloc A\n",
        "                except:\n",
        "                    print(f\"‚ö†Ô∏è Failed to prepare {receptor_file}\")\n",
        "    \n",
        "    def analyze_preparation_results(self, prep_dir):\n",
        "        \"\"\"Analyze preparation results\"\"\"\n",
        "        print(f\"üìä Analyzing preparation results in {prep_dir}...\")\n",
        "        \n",
        "        if not os.path.exists(prep_dir):\n",
        "            print(f\"‚ùå Directory {prep_dir} not found\")\n",
        "            return None\n",
        "        \n",
        "        # Count prepared files\n",
        "        pdbqt_files = [f for f in os.listdir(prep_dir) if f.endswith('.pdbqt')]\n",
        "        \n",
        "        results = {\n",
        "            'total_files': len(pdbqt_files),\n",
        "            'successful_preparations': len(pdbqt_files),\n",
        "            'failed_preparations': 0,\n",
        "            'file_list': pdbqt_files\n",
        "        }\n",
        "        \n",
        "        print(f\"‚úÖ Preparation analysis complete:\")\n",
        "        print(f\"   Total files: {results['total_files']}\")\n",
        "        print(f\"   Successful: {results['successful_preparations']}\")\n",
        "        print(f\"   Failed: {results['failed_preparations']}\")\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def validate_prepared_structures(self, prep_dir):\n",
        "        \"\"\"Validate prepared structures\"\"\"\n",
        "        print(f\"üîç Validating prepared structures in {prep_dir}...\")\n",
        "        \n",
        "        validation_results = {\n",
        "            'valid_structures': [],\n",
        "            'invalid_structures': [],\n",
        "            'validation_errors': []\n",
        "        }\n",
        "        \n",
        "        for pdbqt_file in os.listdir(prep_dir):\n",
        "            if pdbqt_file.endswith('.pdbqt'):\n",
        "                file_path = os.path.join(prep_dir, pdbqt_file)\n",
        "                \n",
        "                try:\n",
        "                    # Basic validation - check if file is not empty and has proper format\n",
        "                    with open(file_path, 'r') as f:\n",
        "                        content = f.read()\n",
        "                        \n",
        "                    if len(content.strip()) > 0 and 'ATOM' in content:\n",
        "                        validation_results['valid_structures'].append(pdbqt_file)\n",
        "                    else:\n",
        "                        validation_results['invalid_structures'].append(pdbqt_file)\n",
        "                        validation_results['validation_errors'].append(f\"{pdbqt_file}: Empty or invalid format\")\n",
        "                        \n",
        "                except Exception as e:\n",
        "                    validation_results['invalid_structures'].append(pdbqt_file)\n",
        "                    validation_results['validation_errors'].append(f\"{pdbqt_file}: {str(e)}\")\n",
        "        \n",
        "        print(f\"‚úÖ Validation complete:\")\n",
        "        print(f\"   Valid structures: {len(validation_results['valid_structures'])}\")\n",
        "        print(f\"   Invalid structures: {len(validation_results['invalid_structures'])}\")\n",
        "        \n",
        "        if validation_results['validation_errors']:\n",
        "            print(\"‚ö†Ô∏è Validation errors:\")\n",
        "            for error in validation_results['validation_errors']:\n",
        "                print(f\"   - {error}\")\n",
        "        \n",
        "        return validation_results\n",
        "\n",
        "class EnhancedGNINAWorkflow(FlexibleTieredWorkflow):\n",
        "    \"\"\"Complete GNINA workflow with PDB preparation wizard integration\"\"\"\n",
        "    \n",
        "    def __init__(self, config, max_workers=None, resume_file=\"workflow_state.json\"):\n",
        "        super().__init__(config, max_workers, resume_file)\n",
        "        self.pdb_integration = PDBPreparationIntegration(config)\n",
        "    \n",
        "    def setup_complete_workflow(self, pairlist_df, use_pdb_wizard=True):\n",
        "        \"\"\"Setup complete workflow with PDB preparation\"\"\"\n",
        "        print(\"üöÄ Setting up complete GNINA workflow...\")\n",
        "        \n",
        "        if use_pdb_wizard:\n",
        "            print(\"üì• Setting up PDB preparation wizard...\")\n",
        "            self.pdb_integration.download_pdb_wizard()\n",
        "            \n",
        "            print(\"üß™ Preparing ligands with wizard...\")\n",
        "            self.pdb_integration.prepare_ligands_with_wizard()\n",
        "            \n",
        "            print(\"üß¨ Preparing receptors with wizard...\")\n",
        "            self.pdb_integration.prepare_receptors_with_wizard()\n",
        "            \n",
        "            # Validate preparations\n",
        "            ligand_validation = self.pdb_integration.validate_prepared_structures(\"ligands_prep\")\n",
        "            receptor_validation = self.pdb_integration.validate_prepared_structures(\"receptors_prep\")\n",
        "            \n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Skipping PDB wizard, using basic preparation\")\n",
        "        \n",
        "        # Configure flexible docking\n",
        "        print(\"üîÑ Configuring flexible receptor docking...\")\n",
        "        self.set_bulk_flexibility(pairlist_df, auto_detect=True)\n",
        "        \n",
        "        print(\"‚úÖ Complete workflow setup finished!\")\n",
        "    \n",
        "    def run_enhanced_workflow(self, pairlist_df, stages=['A', 'B'], use_pdb_wizard=True):\n",
        "        \"\"\"Run the complete enhanced workflow\"\"\"\n",
        "        print(\"üéØ Starting Enhanced GNINA Workflow with PDB Wizard Integration\")\n",
        "        \n",
        "        # Setup workflow\n",
        "        self.setup_complete_workflow(pairlist_df, use_pdb_wizard)\n",
        "        \n",
        "        # Run the tiered workflow\n",
        "        results = self.run_complete_workflow_with_resume(pairlist_df, stages)\n",
        "        \n",
        "        # Generate enhanced analysis\n",
        "        self.generate_enhanced_analysis(results)\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def generate_enhanced_analysis(self, results):\n",
        "        \"\"\"Generate enhanced analysis with PDB wizard integration\"\"\"\n",
        "        print(\"üìä Generating enhanced analysis...\")\n",
        "        \n",
        "        # Create analysis directory\n",
        "        analysis_dir = \"enhanced_analysis\"\n",
        "        os.makedirs(analysis_dir, exist_ok=True)\n",
        "        \n",
        "        # Analyze preparation quality\n",
        "        if os.path.exists(\"ligands_prep\"):\n",
        "            ligand_analysis = self.pdb_integration.analyze_preparation_results(\"ligands_prep\")\n",
        "            with open(f\"{analysis_dir}/ligand_preparation_analysis.json\", 'w') as f:\n",
        "                json.dump(ligand_analysis, f, indent=2)\n",
        "        \n",
        "        if os.path.exists(\"receptors_prep\"):\n",
        "            receptor_analysis = self.pdb_integration.analyze_preparation_results(\"receptors_prep\")\n",
        "            with open(f\"{analysis_dir}/receptor_preparation_analysis.json\", 'w') as f:\n",
        "                json.dump(receptor_analysis, f, indent=2)\n",
        "        \n",
        "        # Generate comprehensive report\n",
        "        self._generate_comprehensive_report(results, analysis_dir)\n",
        "        \n",
        "        print(\"‚úÖ Enhanced analysis complete!\")\n",
        "    \n",
        "    def _generate_comprehensive_report(self, results, analysis_dir):\n",
        "        \"\"\"Generate comprehensive analysis report\"\"\"\n",
        "        report = {\n",
        "            'workflow_summary': {\n",
        "                'total_results': len(results),\n",
        "                'successful_dockings': len([r for r in results if r['status'] == 'success']),\n",
        "                'failed_dockings': len([r for r in results if r['status'] != 'success']),\n",
        "                'stages_completed': list(self.stage_results.keys()),\n",
        "                'flexible_receptors_used': len(self.flexible_docker.flexible_residues)\n",
        "            },\n",
        "            'preparation_quality': {\n",
        "                'pdb_wizard_used': True,\n",
        "                'ligand_preparation': 'Enhanced with PDB wizard',\n",
        "                'receptor_preparation': 'Enhanced with PDB wizard'\n",
        "            },\n",
        "            'performance_metrics': {\n",
        "                'parallel_processing': True,\n",
        "                'max_workers': self.max_workers,\n",
        "                'resume_capability': True\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        with open(f\"{analysis_dir}/comprehensive_report.json\", 'w') as f:\n",
        "            json.dump(report, f, indent=2)\n",
        "\n",
        "# Initialize enhanced workflow\n",
        "enhanced_workflow = EnhancedGNINAWorkflow(CONFIG, max_workers=4)\n",
        "print(\"‚úÖ Enhanced GNINA Workflow with PDB Wizard Integration initialized\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Phase 2: Enhanced Workflow Execution\n",
        "# =============================================================================\n",
        "\n",
        "def run_enhanced_workflow(workflow_type=\"enhanced\", stages=['A', 'B'], use_pdb_wizard=True):\n",
        "    \"\"\"\n",
        "    Run the enhanced workflow with flexible receptors and PDB wizard integration\n",
        "    \n",
        "    Args:\n",
        "        workflow_type: \"enhanced\", \"flexible\", \"basic\"\n",
        "        stages: List of stages to run ['A', 'B', 'C']\n",
        "        use_pdb_wizard: Whether to use PDB preparation wizard\n",
        "    \"\"\"\n",
        "    \n",
        "    if 'pairlist_df' not in locals() or pairlist_df is None:\n",
        "        print(\"‚ùå Please ensure pairlist.csv is loaded before running workflow\")\n",
        "        return None\n",
        "    \n",
        "    print(f\"üéØ Running {workflow_type.upper()} workflow with stages: {stages}\")\n",
        "    print(f\"   PDB Wizard: {'Enabled' if use_pdb_wizard else 'Disabled'}\")\n",
        "    \n",
        "    if workflow_type == \"enhanced\":\n",
        "        # Complete enhanced workflow with PDB wizard and flexible receptors\n",
        "        return enhanced_workflow.run_enhanced_workflow(pairlist_df, stages, use_pdb_wizard)\n",
        "    \n",
        "    elif workflow_type == \"flexible\":\n",
        "        # Flexible receptor workflow without PDB wizard\n",
        "        flexible_workflow.set_bulk_flexibility(pairlist_df, auto_detect=True)\n",
        "        return flexible_workflow.run_complete_workflow_with_resume(pairlist_df, stages)\n",
        "    \n",
        "    elif workflow_type == \"basic\":\n",
        "        # Basic workflow without enhancements\n",
        "        return resume_workflow.run_complete_workflow_with_resume(pairlist_df, stages)\n",
        "    \n",
        "    else:\n",
        "        print(f\"‚ùå Unknown workflow type: {workflow_type}\")\n",
        "        return None\n",
        "\n",
        "# =============================================================================\n",
        "# Flexible Receptor Configuration Examples\n",
        "# =============================================================================\n",
        "\n",
        "def configure_flexible_docking_examples():\n",
        "    \"\"\"Examples of how to configure flexible docking\"\"\"\n",
        "    \n",
        "    print(\"üîß Flexible Docking Configuration Examples:\")\n",
        "    print(\"\\n1. Auto-detect flexible residues (recommended):\")\n",
        "    print(\"   flexible_workflow.set_bulk_flexibility(pairlist_df, auto_detect=True)\")\n",
        "    \n",
        "    print(\"\\n2. Manual specification for specific receptor:\")\n",
        "    print(\"   flexible_workflow.set_receptor_flexibility(\")\n",
        "    print(\"       'receptor_name',\")\n",
        "    print(\"       flexible_residues=['A:123', 'A:124', 'A:125']\")\n",
        "    print(\"   )\")\n",
        "    \n",
        "    print(\"\\n3. Configure flexible docking parameters:\")\n",
        "    print(\"   flexible_workflow.configure_flexible_docking({\")\n",
        "    print(\"       'distance_threshold': 6.0,      # Distance from binding site\")\n",
        "    print(\"       'max_flexible_residues': 15,    # Maximum flexible residues\")\n",
        "    print(\"       'flexdist': 4.0                 # GNINA flexdist parameter\")\n",
        "    print(\"   })\")\n",
        "    \n",
        "    print(\"\\n4. Run enhanced workflow with flexible receptors:\")\n",
        "    print(\"   results = run_enhanced_workflow('enhanced', stages=['A', 'B'])\")\n",
        "\n",
        "# =============================================================================\n",
        "# Quality Control and Validation\n",
        "# =============================================================================\n",
        "\n",
        "class QualityControlValidator:\n",
        "    \"\"\"Quality control and validation for the enhanced workflow\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.validation_results = {}\n",
        "    \n",
        "    def validate_input_files(self, pairlist_df):\n",
        "        \"\"\"Validate input files and structure\"\"\"\n",
        "        print(\"üîç Validating input files...\")\n",
        "        \n",
        "        validation_results = {\n",
        "            'pairlist_valid': True,\n",
        "            'missing_receptors': [],\n",
        "            'missing_ligands': [],\n",
        "            'invalid_coordinates': [],\n",
        "            'warnings': []\n",
        "        }\n",
        "        \n",
        "        # Check pairlist structure\n",
        "        required_cols = ['receptor', 'ligand', 'center_x', 'center_y', 'center_z', \n",
        "                        'size_x', 'size_y', 'size_z']\n",
        "        missing_cols = [col for col in required_cols if col not in pairlist_df.columns]\n",
        "        \n",
        "        if missing_cols:\n",
        "            validation_results['pairlist_valid'] = False\n",
        "            validation_results['warnings'].append(f\"Missing columns: {missing_cols}\")\n",
        "        \n",
        "        # Check file existence\n",
        "        for _, row in pairlist_df.iterrows():\n",
        "            receptor_file = f\"receptors_prep/{row['receptor']}.pdbqt\"\n",
        "            ligand_file = f\"ligands_prep/{row['ligand']}.pdbqt\"\n",
        "            \n",
        "            if not os.path.exists(receptor_file):\n",
        "                validation_results['missing_receptors'].append(row['receptor'])\n",
        "            \n",
        "            if not os.path.exists(ligand_file):\n",
        "                validation_results['missing_ligands'].append(row['ligand'])\n",
        "            \n",
        "            # Check coordinate validity\n",
        "            coords = [row['center_x'], row['center_y'], row['center_z']]\n",
        "            sizes = [row['size_x'], row['size_y'], row['size_z']]\n",
        "            \n",
        "            if any(not isinstance(c, (int, float)) or np.isnan(c) for c in coords + sizes):\n",
        "                validation_results['invalid_coordinates'].append(f\"{row['receptor']}-{row['ligand']}\")\n",
        "        \n",
        "        # Print validation summary\n",
        "        print(f\"‚úÖ Pairlist valid: {validation_results['pairlist_valid']}\")\n",
        "        print(f\"   Missing receptors: {len(validation_results['missing_receptors'])}\")\n",
        "        print(f\"   Missing ligands: {len(validation_results['missing_ligands'])}\")\n",
        "        print(f\"   Invalid coordinates: {len(validation_results['invalid_coordinates'])}\")\n",
        "        \n",
        "        if validation_results['warnings']:\n",
        "            print(\"‚ö†Ô∏è Warnings:\")\n",
        "            for warning in validation_results['warnings']:\n",
        "                print(f\"   - {warning}\")\n",
        "        \n",
        "        self.validation_results['input_validation'] = validation_results\n",
        "        return validation_results\n",
        "    \n",
        "    def validate_docking_results(self, results):\n",
        "        \"\"\"Validate docking results quality\"\"\"\n",
        "        print(\"üîç Validating docking results...\")\n",
        "        \n",
        "        validation_results = {\n",
        "            'total_results': len(results),\n",
        "            'successful_dockings': 0,\n",
        "            'failed_dockings': 0,\n",
        "            'quality_issues': [],\n",
        "            'score_distribution': {}\n",
        "        }\n",
        "        \n",
        "        successful_results = [r for r in results if r['status'] == 'success']\n",
        "        validation_results['successful_dockings'] = len(successful_results)\n",
        "        validation_results['failed_dockings'] = len(results) - len(successful_results)\n",
        "        \n",
        "        if successful_results:\n",
        "            # Analyze score distribution\n",
        "            all_scores = []\n",
        "            for result in successful_results:\n",
        "                scores = result.get('scores', [])\n",
        "                if scores:\n",
        "                    all_scores.extend([s.get('cnn_score', 0) for s in scores])\n",
        "            \n",
        "            if all_scores:\n",
        "                validation_results['score_distribution'] = {\n",
        "                    'mean': np.mean(all_scores),\n",
        "                    'std': np.std(all_scores),\n",
        "                    'min': np.min(all_scores),\n",
        "                    'max': np.max(all_scores),\n",
        "                    'median': np.median(all_scores)\n",
        "                }\n",
        "                \n",
        "                # Quality checks\n",
        "                if np.mean(all_scores) < 0.3:\n",
        "                    validation_results['quality_issues'].append(\"Low average CNN scores\")\n",
        "                \n",
        "                if np.std(all_scores) < 0.1:\n",
        "                    validation_results['quality_issues'].append(\"Low score variance - possible issues\")\n",
        "        \n",
        "        # Print validation summary\n",
        "        print(f\"‚úÖ Total results: {validation_results['total_results']}\")\n",
        "        print(f\"   Successful: {validation_results['successful_dockings']}\")\n",
        "        print(f\"   Failed: {validation_results['failed_dockings']}\")\n",
        "        \n",
        "        if validation_results['score_distribution']:\n",
        "            dist = validation_results['score_distribution']\n",
        "            print(f\"   Score distribution:\")\n",
        "            print(f\"     Mean: {dist['mean']:.3f}\")\n",
        "            print(f\"     Std: {dist['std']:.3f}\")\n",
        "            print(f\"     Range: {dist['min']:.3f} - {dist['max']:.3f}\")\n",
        "        \n",
        "        if validation_results['quality_issues']:\n",
        "            print(\"‚ö†Ô∏è Quality issues detected:\")\n",
        "            for issue in validation_results['quality_issues']:\n",
        "                print(f\"   - {issue}\")\n",
        "        \n",
        "        self.validation_results['docking_validation'] = validation_results\n",
        "        return validation_results\n",
        "    \n",
        "    def generate_quality_report(self):\n",
        "        \"\"\"Generate comprehensive quality report\"\"\"\n",
        "        print(\"üìä Generating quality control report...\")\n",
        "        \n",
        "        report = {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'validation_results': self.validation_results,\n",
        "            'recommendations': []\n",
        "        }\n",
        "        \n",
        "        # Generate recommendations based on validation results\n",
        "        if 'input_validation' in self.validation_results:\n",
        "            input_val = self.validation_results['input_validation']\n",
        "            if input_val['missing_receptors']:\n",
        "                report['recommendations'].append(\"Check receptor file preparation\")\n",
        "            if input_val['missing_ligands']:\n",
        "                report['recommendations'].append(\"Check ligand file preparation\")\n",
        "            if input_val['invalid_coordinates']:\n",
        "                report['recommendations'].append(\"Validate binding site coordinates\")\n",
        "        \n",
        "        if 'docking_validation' in self.validation_results:\n",
        "            dock_val = self.validation_results['docking_validation']\n",
        "            if dock_val['failed_dockings'] > dock_val['successful_dockings']:\n",
        "                report['recommendations'].append(\"High failure rate - check input structures\")\n",
        "            if dock_val['quality_issues']:\n",
        "                report['recommendations'].append(\"Review docking parameters and structures\")\n",
        "        \n",
        "        # Save report\n",
        "        with open('quality_control_report.json', 'w') as f:\n",
        "            json.dump(report, f, indent=2)\n",
        "        \n",
        "        print(\"‚úÖ Quality control report saved to quality_control_report.json\")\n",
        "        return report\n",
        "\n",
        "# Initialize quality control validator\n",
        "qc_validator = QualityControlValidator()\n",
        "print(\"‚úÖ Quality Control Validator initialized\")\n",
        "\n",
        "# =============================================================================\n",
        "# Main Execution Options for Phase 2\n",
        "# =============================================================================\n",
        "\n",
        "print(\"üöÄ Phase 2 Enhanced Workflow Ready!\")\n",
        "print(\"\\nAvailable workflow types:\")\n",
        "print(\"  1. enhanced  - Complete workflow with PDB wizard + flexible receptors\")\n",
        "print(\"  2. flexible  - Flexible receptor workflow (no PDB wizard)\")\n",
        "print(\"  3. basic     - Basic workflow (no enhancements)\")\n",
        "\n",
        "print(\"\\nExample usage:\")\n",
        "print(\"  # Complete enhanced workflow (recommended)\")\n",
        "print(\"  results = run_enhanced_workflow('enhanced', stages=['A', 'B'])\")\n",
        "print(\"\")\n",
        "print(\"  # Flexible receptor only\")\n",
        "print(\"  results = run_enhanced_workflow('flexible', stages=['A', 'B'])\")\n",
        "print(\"\")\n",
        "print(\"  # Basic workflow\")\n",
        "print(\"  results = run_enhanced_workflow('basic', stages=['A', 'B'])\")\n",
        "\n",
        "print(\"\\nQuality control:\")\n",
        "print(\"  qc_validator.validate_input_files(pairlist_df)\")\n",
        "print(\"  qc_validator.validate_docking_results(results)\")\n",
        "print(\"  qc_validator.generate_quality_report()\")\n",
        "\n",
        "print(\"\\nFlexible docking configuration:\")\n",
        "configure_flexible_docking_examples()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGL7o7_4_rEL",
        "outputId": "82664ef2-7180-4438-9e14-67738c37bfc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files under /content/drive/MyDrive/EFA_Docking\n",
            "gnina  gnina_out  ligands  ligands_prep  ligands_raw  pairlist.gsheet  pairlist1.csv  pairlist2.csv\n",
            "pairlist3.csv  pairlist4.numbers  receptors  receptors_prep  receptors_raw\n"
          ]
        }
      ],
      "source": [
        "import os, glob, textwrap\n",
        "root = '/content/drive/MyDrive/EFA_Docking'   # same value you used\n",
        "print(\"Files under\", root)\n",
        "print(textwrap.fill(\"  \".join(sorted(os.listdir(root))), 100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqzblzMJuZRM",
        "outputId": "4170a2c2-d4aa-499b-f79b-139b1fdf713a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# fresh install of everything needed for ligand ‚Üí PDBQT\n",
        "!pip install -q rdkit-pypi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAqI0QrV1-m1",
        "outputId": "48b0a388-0a41-4705-c08c-48ecf05f1c1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: rdkit-pypi\n",
            "Version: 2022.9.5\n",
            "Summary: A collection of chemoinformatics and machine-learning software written in C++ and Python\n",
            "Home-page: https://github.com/kuelumbus/rdkit-pypi\n",
            "Name: meeko\n",
            "Version: 0.6.1\n",
            "Summary: Python package for preparing small molecule for docking\n",
            "Home-page: https://github.com/ccsb-scripps/meeko\n",
            "ERROR: Pipe to stdout was broken\n",
            "Exception ignored in: <_io.TextIOWrapper name='<stdout>' mode='w' encoding='utf-8'>\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n"
          ]
        }
      ],
      "source": [
        "!pip show rdkit-pypi | head -4\n",
        "!pip show meeko      | head -4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Fw6el8JU4Z7E",
        "outputId": "55613872-ba50-41bb-86d5-af5ef8451c5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß™  Ligand preparation -----------------------------------\n",
            "Input molecules processed: 1, skipped: 0\n",
            "PDBQT files written: 1\n",
            "PDBQT files not written due to error: 0\n",
            "Input molecules with errors: 0\n",
            "Input molecules processed: 1, skipped: 0\n",
            "PDBQT files written: 1\n",
            "PDBQT files not written due to error: 0\n",
            "Input molecules with errors: 0\n",
            "Input molecules processed: 1, skipped: 0\n",
            "PDBQT files written: 1\n",
            "PDBQT files not written due to error: 0\n",
            "Input molecules with errors: 0\n",
            "Input molecules processed: 1, skipped: 0\n",
            "PDBQT files written: 1\n",
            "PDBQT files not written due to error: 0\n",
            "Input molecules with errors: 0\n",
            "Input molecules processed: 1, skipped: 0\n",
            "PDBQT files written: 1\n",
            "PDBQT files not written due to error: 0\n",
            "Input molecules with errors: 0\n",
            "Input molecules processed: 1, skipped: 0\n",
            "PDBQT files written: 1\n",
            "PDBQT files not written due to error: 0\n",
            "Input molecules with errors: 0\n",
            "Input molecules processed: 1, skipped: 0\n",
            "PDBQT files written: 1\n",
            "PDBQT files not written due to error: 0\n",
            "Input molecules with errors: 0\n",
            "Input molecules processed: 1, skipped: 0\n",
            "PDBQT files written: 1\n",
            "PDBQT files not written due to error: 0\n",
            "Input molecules with errors: 0\n",
            "\n",
            "üß¨  Receptor preparation -------------------------------\n",
            "\n",
            "Error: Creation of data structure for receptor failed.\n",
            "\n",
            "Details:\n",
            "- Template matching failed for: ['B:417', 'B:427', 'B:581', 'B:585', 'B:1010', 'B:1275', 'B:1299', 'B:1440', 'B:1447']\n",
            "These residues can be ignored with option allow_bad_res.\n",
            "\n",
            "Recommendations:\n",
            "1. (for batch processing) Use -a/--allow_bad_res to automatically remove residues\n",
            "that do not match templates, and --default_altloc to set\n",
            "a default altloc variant. Use these at your own risk.\n",
            "\n",
            "2. (processing individual structure) Inspecting and fixing the input structure is recommended.\n",
            "Use --wanted_altloc to set variants for specific residues.\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No template matched for residue_key='B:417'\n",
            "tried 6 templates for residue_key='B:417'excess_H_ok=False\n",
            "LYS        heavy_miss=4 heavy_excess=0 H_excess=[] bond_miss={4} bond_excess=set()\n",
            "NLYS       heavy_miss=4 heavy_excess=0 H_excess=[] bond_miss=set() bond_excess=set()\n",
            "CLYS       heavy_miss=5 heavy_excess=0 H_excess=[] bond_miss={5} bond_excess={1}\n",
            "LYN        heavy_miss=4 heavy_excess=0 H_excess=[] bond_miss={4} bond_excess=set()\n",
            "NLYN       heavy_miss=4 heavy_excess=0 H_excess=[] bond_miss=set() bond_excess=set()\n",
            "CLYN       heavy_miss=5 heavy_excess=0 H_excess=[] bond_miss={5} bond_excess={1}\n",
            "\n",
            "No template matched for residue_key='B:427'\n",
            "tried 6 templates for residue_key='B:427'excess_H_ok=False\n",
            "GLU        heavy_miss=4 heavy_excess=0 H_excess=[] bond_miss=set() bond_excess=set()\n",
            "NGLU       heavy_miss=4 heavy_excess=0 H_excess=[] bond_miss=set() bond_excess={4}\n",
            "CGLU       heavy_miss=5 heavy_excess=0 H_excess=[] bond_miss=set() bond_excess={1}\n",
            "GLH        heavy_miss=4 heavy_excess=0 H_excess=[] bond_miss=set() bond_excess=set()\n",
            "NGLH       heavy_miss=4 heavy_excess=0 H_excess=[] bond_miss=set() bond_excess={4}\n",
            "CGLH       heavy_miss=5 heavy_excess=0 H_excess=[] bond_miss=set() bond_excess={1}\n",
            "\n",
            "No template matched for residue_key='B:581'\n",
            "tried 6 templates for residue_key='B:581'excess_H_ok=False\n",
            "LYS        heavy_miss=4 heavy_excess=0 H_excess=[] bond_miss=set() bond_excess=set()\n",
            "NLYS       heavy_miss=4 heavy_excess=0 H_excess=[] bond_miss=set() bond_excess={4}\n",
            "CLYS       heavy_miss=5 heavy_excess=0 H_excess=[] bond_miss=set() bond_excess={1}\n",
            "LYN        heavy_miss=4 heavy_excess=0 H_excess=[] bond_miss=set() bond_excess=set()\n",
            "NLYN       heavy_miss=4 heavy_excess=0 H_excess=[] bond_miss=set() bond_excess={4}\n",
            "CLYN       heavy_miss=5 heavy_excess=0 H_excess=[] bond_miss=set() bond_excess={1}\n",
            "\n",
            "No template matched for residue_key='B:585'\n",
            "tried 6 templates for residue_key='B:585'excess_H_ok=False\n",
            "GLU        heavy_miss=4 heavy_excess=0 H_excess=[] bond_miss=set() bond_excess=set()\n",
            "NGLU       heavy_miss=4 heavy_excess=0 H_excess=[] bond_miss=set() bond_excess={4}\n",
            "CGLU       heavy_miss=5 heavy_excess=0 H_excess=[] bond_miss=set() bond_excess={1}\n",
            "GLH        heavy_miss=4 heavy_excess=0 H_excess=[] bond_miss=set() bond_excess=set()\n",
            "NGLH       heavy_miss=4 heavy_excess=0 H_excess=[] bond_miss=set() bond_excess={4}\n",
            "CGLH       heavy_miss=5 heavy_excess=0 H_excess=[] bond_miss=set() bond_excess={1}\n",
            "\n",
            "No template matched for residue_key='B:1010'\n",
            "tried 3 templates for residue_key='B:1010'excess_H_ok=False\n",
            "ASN        heavy_miss=3 heavy_excess=0 H_excess=[] bond_miss={4} bond_excess=set()\n",
            "NASN       heavy_miss=3 heavy_excess=0 H_excess=[] bond_miss=set() bond_excess=set()\n",
            "CASN       heavy_miss=4 heavy_excess=0 H_excess=[] bond_miss={5} bond_excess={1}\n",
            "\n",
            "No template matched for residue_key='B:1275'\n",
            "tried 6 templates for residue_key='B:1275'excess_H_ok=False\n",
            "GLU        heavy_miss=4 heavy_excess=0 H_excess=[] bond_miss=set() bond_excess=set()\n",
            "NGLU       heavy_miss=4 heavy_excess=0 H_excess=[] bond_miss=set() bond_excess={4}\n",
            "CGLU       heavy_miss=5 heavy_excess=0 H_excess=[] bond_miss=set() bond_excess={1}\n",
            "GLH        heavy_miss=4 heavy_excess=0 H_excess=[] bond_miss=set() bond_excess=set()\n",
            "NGLH       heavy_miss=4 heavy_excess=0 H_excess=[] bond_miss=set() bond_excess={4}\n",
            "CGLH       heavy_miss=5 heavy_excess=0 H_excess=[] bond_miss=set() bond_excess={1}\n",
            "\n",
            "No template matched for residue_key='B:1299'\n",
            "tried 3 templates for residue_key='B:1299'excess_H_ok=False\n",
            "ARG        heavy_miss=6 heavy_excess=0 H_excess=[] bond_miss=set() bond_excess=set()\n",
            "NARG       heavy_miss=6 heavy_excess=0 H_excess=[] bond_miss=set() bond_excess={4}\n",
            "CARG       heavy_miss=7 heavy_excess=0 H_excess=[] bond_miss=set() bond_excess={1}\n",
            "\n",
            "No template matched for residue_key='B:1440'\n",
            "tried 6 templates for residue_key='B:1440'excess_H_ok=False\n",
            "LYS        heavy_miss=4 heavy_excess=0 H_excess=[] bond_miss=set() bond_excess=set()\n",
            "NLYS       heavy_miss=4 heavy_excess=0 H_excess=[] bond_miss=set() bond_excess={4}\n",
            "CLYS       heavy_miss=5 heavy_excess=0 H_excess=[] bond_miss=set() bond_excess={1}\n",
            "LYN        heavy_miss=4 heavy_excess=0 H_excess=[] bond_miss=set() bond_excess=set()\n",
            "NLYN       heavy_miss=4 heavy_excess=0 H_excess=[] bond_miss=set() bond_excess={4}\n",
            "CLYN       heavy_miss=5 heavy_excess=0 H_excess=[] bond_miss=set() bond_excess={1}\n",
            "\n",
            "No template matched for residue_key='B:1447'\n",
            "tried 6 templates for residue_key='B:1447'excess_H_ok=False\n",
            "GLU        heavy_miss=4 heavy_excess=0 H_excess=[] bond_miss=set() bond_excess=set()\n",
            "NGLU       heavy_miss=4 heavy_excess=0 H_excess=[] bond_miss=set() bond_excess={4}\n",
            "CGLU       heavy_miss=5 heavy_excess=0 H_excess=[] bond_miss=set() bond_excess={1}\n",
            "GLH        heavy_miss=4 heavy_excess=0 H_excess=[] bond_miss=set() bond_excess=set()\n",
            "NGLH       heavy_miss=4 heavy_excess=0 H_excess=[] bond_miss=set() bond_excess={4}\n",
            "CGLH       heavy_miss=5 heavy_excess=0 H_excess=[] bond_miss=set() bond_excess={1}\n",
            "\n"
          ]
        },
        {
          "ename": "CalledProcessError",
          "evalue": "Command 'b'set -e\\n# \\xe2\\x94\\x80\\xe2\\x94\\x80 adjust these four folders if your names differ \\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\nROOT=\"/content/drive/MyDrive/EFA_Docking\"\\nRAW_LIG=\"$ROOT/ligands_raw\"          # *.mol2  or *.sdf\\nRAW_REC=\"$ROOT/receptors_raw\"        # *.pdb\\nOUT_LIG=\"$ROOT/ligands_prep\"         # output *.pdbqt (ligands)\\nOUT_REC=\"$ROOT/receptors_prep\"       # output *.pdbqt (receptors)\\nmkdir -p \"$OUT_LIG\" \"$OUT_REC\"\\n\\necho \"\\xf0\\x9f\\xa7\\xaa  Ligand preparation -----------------------------------\"\\nshopt -s nullglob\\nfor mol in \"$RAW_LIG\"/*.{mol2,sdf}; do\\n  base=$(basename \"${mol%.*}\")\\n  out=\"$OUT_LIG/${base}.pdbqt\"\\n  [[ -f \"$out\" ]] && { echo \"skip $base\"; continue; }\\n  mk_prepare_ligand.py  -i \"$mol\"  -o \"$out\"\\ndone\\n\\necho -e \"\\\\n\\xf0\\x9f\\xa7\\xac  Receptor preparation -------------------------------\"\\nfor pdb in \"$RAW_REC\"/*.pdb; do\\n  base=$(basename \"${pdb%.*}\")\\n  out=\"$OUT_REC/${base}.pdbqt\"\\n  [[ -f \"$out\" ]] && { echo \"skip $base\"; continue; }\\n  mk_prepare_receptor.py  --read_pdb \"$pdb\"  -p \"$out\"\\ndone\\n\\necho -e \"\\\\n\\xe2\\x9c\\x85  Done.\"\\necho   \"Ligands   written: $(ls -1q $OUT_LIG/*.pdbqt 2>/dev/null | wc -l)\"\\necho   \"Receptors written: $(ls -1q $OUT_REC/*.pdbqt 2>/dev/null | wc -l)\"\\n'' returned non-zero exit status 1.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-40ccc8e5a47f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'set -e\\n# ‚îÄ‚îÄ adjust these four folders if your names differ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\\nROOT=\"/content/drive/MyDrive/EFA_Docking\"\\nRAW_LIG=\"$ROOT/ligands_raw\"          # *.mol2  or *.sdf\\nRAW_REC=\"$ROOT/receptors_raw\"        # *.pdb\\nOUT_LIG=\"$ROOT/ligands_prep\"         # output *.pdbqt (ligands)\\nOUT_REC=\"$ROOT/receptors_prep\"       # output *.pdbqt (receptors)\\nmkdir -p \"$OUT_LIG\" \"$OUT_REC\"\\n\\necho \"üß™  Ligand preparation -----------------------------------\"\\nshopt -s nullglob\\nfor mol in \"$RAW_LIG\"/*.{mol2,sdf}; do\\n  base=$(basename \"${mol%.*}\")\\n  out=\"$OUT_LIG/${base}.pdbqt\"\\n  [[ -f \"$out\" ]] && { echo \"skip $base\"; continue; }\\n  mk_prepare_ligand.py  -i \"$mol\"  -o \"$out\"\\ndone\\n\\necho -e \"\\\\nüß¨  Receptor preparation -------------------------------\"\\nfor pdb in \"$RAW_REC\"/*.pdb; do\\n  base=$(basename \"${pdb%.*}\")\\n  out=\"$OUT_REC/${base}.pdbqt\"\\n  [[ -f \"$out\" ]] && { echo \"skip $base\"; continue; }\\n  mk_prepare_receptor.py  --read_pdb \"$pdb\"  -p \"$out\"\\ndone\\n\\necho -e \"\\\\n‚úÖ  Done.\"\\necho   \"Ligands   written: $(ls -1q $OUT_LIG/*.pdbqt 2>/dev/null | wc -l)\"\\necho   \"Receptors written: $(ls -1q $OUT_REC/*.pdbqt 2>/dev/null | wc -l)\"\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m       \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2471\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2474\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-103>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'set -e\\n# \\xe2\\x94\\x80\\xe2\\x94\\x80 adjust these four folders if your names differ \\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\nROOT=\"/content/drive/MyDrive/EFA_Docking\"\\nRAW_LIG=\"$ROOT/ligands_raw\"          # *.mol2  or *.sdf\\nRAW_REC=\"$ROOT/receptors_raw\"        # *.pdb\\nOUT_LIG=\"$ROOT/ligands_prep\"         # output *.pdbqt (ligands)\\nOUT_REC=\"$ROOT/receptors_prep\"       # output *.pdbqt (receptors)\\nmkdir -p \"$OUT_LIG\" \"$OUT_REC\"\\n\\necho \"\\xf0\\x9f\\xa7\\xaa  Ligand preparation -----------------------------------\"\\nshopt -s nullglob\\nfor mol in \"$RAW_LIG\"/*.{mol2,sdf}; do\\n  base=$(basename \"${mol%.*}\")\\n  out=\"$OUT_LIG/${base}.pdbqt\"\\n  [[ -f \"$out\" ]] && { echo \"skip $base\"; continue; }\\n  mk_prepare_ligand.py  -i \"$mol\"  -o \"$out\"\\ndone\\n\\necho -e \"\\\\n\\xf0\\x9f\\xa7\\xac  Receptor preparation -------------------------------\"\\nfor pdb in \"$RAW_REC\"/*.pdb; do\\n  base=$(basename \"${pdb%.*}\")\\n  out=\"$OUT_REC/${base}.pdbqt\"\\n  [[ -f \"$out\" ]] && { echo \"skip $base\"; continue; }\\n  mk_prepare_receptor.py  --read_pdb \"$pdb\"  -p \"$out\"\\ndone\\n\\necho -e \"\\\\n\\xe2\\x9c\\x85  Done.\"\\necho   \"Ligands   written: $(ls -1q $OUT_LIG/*.pdbqt 2>/dev/null | wc -l)\"\\necho   \"Receptors written: $(ls -1q $OUT_REC/*.pdbqt 2>/dev/null | wc -l)\"\\n'' returned non-zero exit status 1."
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env bash\n",
        "set -euo pipefail\n",
        "\n",
        "# ‚îÄ‚îÄ edit these three paths if you renamed your folders ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "ROOT=\"${1:-$(pwd)}\"\n",
        "RAW_LIG=\"$ROOT/ligands_raw\"          # *.sdf or *.mol2\n",
        "RAW_REC=\"$ROOT/receptors_raw\"        # *.pdb\n",
        "OUT_LIG=\"$ROOT/ligands_prep\"         # <- output *.pdbqt\n",
        "OUT_REC=\"$ROOT/receptors_prep\"       # <- output *.pdbqt\n",
        "mkdir -p \"$OUT_LIG\" \"$OUT_REC\"\n",
        "\n",
        "# optional: PDB2PQR force-field (AMBER, PARSE, CHARMM, OPLS)\n",
        "FF=\"AMBER\"\n",
        "\n",
        "shopt -s nullglob\n",
        "\n",
        "echo \"üß™  Ligand preparation -----------------------------------\"\n",
        "for mol in \"$RAW_LIG\"/*.{mol2,sdf}; do\n",
        "  base=${mol##*/}; base=${base%.*}\n",
        "  out=\"$OUT_LIG/${base}.pdbqt\"\n",
        "  [[ -f $out ]] && { echo \"skip $base\"; continue; }\n",
        "  mk_prepare_ligand.py -i \"$mol\" -o \"$out\"\n",
        "done\n",
        "\n",
        "echo -e \"\\nüß¨  Receptor preparation (PDB ‚Üí PQR ‚Üí clean PDB) --------\"\n",
        "for pdb in \"$RAW_REC\"/*.pdb; do\n",
        "  base=${pdb##*/}; base=${base%.*}\n",
        "  pqr=\"$OUT_REC/${base}.pqr\"\n",
        "  clean_pdb=\"$OUT_REC/${base}_clean.pdb\"\n",
        "  pdbqt=\"$OUT_REC/${base}.pdbqt\"\n",
        "\n",
        "  [[ -f $pdbqt ]] && { echo \"skip $base\"; continue; }\n",
        "\n",
        "  # 1) PDB ‚Üí PQR  (repairs and protonates)\n",
        "  pdb2pqr30 --ff \"$FF\" --with-ph 7.4 \"$pdb\" \"$pqr\" >/dev/null\n",
        "\n",
        "  # 2) PQR ‚Üí cleaned-PDB  (strip charges/radii)\n",
        "  obabel \"$pqr\" -O \"$clean_pdb\"  >/dev/null\n",
        "\n",
        "  # 3) PDB ‚Üí PDBQT via Meeko  (still keep guard flags)\n",
        "  mk_prepare_receptor.py --read_pdb \"$clean_pdb\" \\\n",
        "                         -p \"$pdbqt\"              \\\n",
        "                         --allow_bad_res          \\\n",
        "                         --default_altloc A\n",
        "done\n",
        "\n",
        "echo -e \"\\n‚úÖ  Finished.\"\n",
        "echo \"Ligands   prepared: $(ls -1q $OUT_LIG/*.pdbqt  2>/dev/null | wc -l)\"\n",
        "echo \"Receptors prepared: $(ls -1q $OUT_REC/*.pdbqt  2>/dev/null | wc -l)\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfO1MTEC5meq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "n10PzyeRAPXB",
        "outputId": "4e7ef663-4fb2-4910-da96-4fd8ac6dff4f"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1252540215.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Load CSV robustly + sanity checks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# --------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{root}/pairlist.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipinitialspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m            \u001b[0;31m# trim & lowercase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "# --------------------------------------------------------------\n",
        "# Load CSV robustly + sanity checks\n",
        "# --------------------------------------------------------------\n",
        "pairs = pd.read_csv(f'{root}/pairlist.csv', skipinitialspace=True)\n",
        "pairs.columns = [c.strip().lower() for c in pairs.columns]            # trim & lowercase\n",
        "\n",
        "required = {'receptor','site_id','ligand',\n",
        "            'center_x','center_y','center_z',\n",
        "            'size_x','size_y','size_z'}\n",
        "missing_cols = required.difference(pairs.columns)\n",
        "assert not missing_cols, f\"CSV is missing columns: {missing_cols}\"\n",
        "\n",
        "# check files exist\n",
        "for rec in pairs['receptor'].unique():\n",
        "    assert pathlib.Path(f\"{rec_dir}/{rec}.pdbqt\").is_file(), f\"receptor file missing: {rec}\"\n",
        "for lig in pairs['ligand'].unique():\n",
        "    assert pathlib.Path(f\"{lig_dir}/{lig}.pdbqt\").is_file(), f\"ligand file missing: {lig}\"\n",
        "print(\"‚úì CSV headers OK and all receptor / ligand files found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "08e759f098244d95ae22fbf9e8b0c639",
            "d9ed27a1fea04207b080dfe1e9678ca8",
            "db614613ca7546dea7a58cb08c54a2d5",
            "a0fab85925714784abbbf6ebb62bffe1",
            "7e499c64af9344fdafb1cfc70c97c9b7",
            "6ef0b37cb4e947f58e44bae492685382",
            "69f54fdbc73c485685d0235e80e0e728",
            "734e3b3dc2be446a879b95ddc5c4e4be",
            "9d0a79b9297d4890a8c2a95b50cefde9",
            "ef8e05a61a2a47d683f192517576dd39",
            "0daff165ad2f49dd9d849470a06e8c25"
          ]
        },
        "id": "o2iqa6Jv9APn",
        "outputId": "355339b4-b333-4c26-fd4e-cd2d66aaa0e9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "08e759f098244d95ae22fbf9e8b0c639",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Docking:   0%|          | 0/125 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# --------------------------------------------------------------\n",
        "#  Batch GNINA docking  (CPU build, no --gpu flag)\n",
        "# --------------------------------------------------------------\n",
        "import os, re, pathlib, shlex, subprocess\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "root    = '/content/drive/MyDrive/Sertaline_Derv_docking'\n",
        "rec_dir = f'{root}/receptors'\n",
        "lig_dir = f'{root}/ligands'\n",
        "out_dir = pathlib.Path(f'{root}/gnina_out'); out_dir.mkdir(exist_ok=True)\n",
        "\n",
        "GNINA_BIN = pathlib.Path(\"/content/drive/MyDrive/Sertaline_Derv_docking/gnina\")\n",
        "assert GNINA_BIN.is_file(), \"gnina binary not found!\"\n",
        "\n",
        "pairs = pd.read_csv(f'{root}/pairlist.csv', skipinitialspace=True)\n",
        "pairs.columns = [c.strip().lower() for c in pairs.columns]\n",
        "\n",
        "def clean_cell(x):\n",
        "    s = str(x).strip().strip('\"').strip(\"'\")\n",
        "    s = re.sub(r'\\s+', '', s)  # remove inner spaces if any were accidental\n",
        "    return s\n",
        "\n",
        "# Clean up key columns\n",
        "for col in ['receptor', 'ligand', 'site_id']:\n",
        "    if col in pairs.columns:\n",
        "        pairs[col] = pairs[col].map(clean_cell)\n",
        "\n",
        "# Ensure numeric docking box\n",
        "for col in ['center_x','center_y','center_z','size_x','size_y','size_z']:\n",
        "    pairs[col] = pd.to_numeric(pairs[col], errors='raise')\n",
        "\n",
        "# Build case-insensitive indices of existing files\n",
        "def build_index(directory):\n",
        "    return {fname.lower(): fname for fname in os.listdir(directory)}\n",
        "\n",
        "rec_index = build_index(rec_dir)\n",
        "lig_index = build_index(lig_dir)\n",
        "\n",
        "def resolve_path(directory, name, ext='.pdbqt', index=None):\n",
        "    \"\"\"Return a real, existing path with exactly one ext; case-insensitive match.\"\"\"\n",
        "    base = clean_cell(name)\n",
        "    cand = base if base.lower().endswith(ext) else base + ext\n",
        "    if index is not None:\n",
        "        real = index.get(cand.lower())\n",
        "        if real:\n",
        "            return os.path.join(directory, real)\n",
        "    # Fallback: direct path (case-sensitive)\n",
        "    path = os.path.join(directory, cand)\n",
        "    return path if os.path.exists(path) else None\n",
        "\n",
        "# Preflight: check that all receptor/ligand files exist after normalization\n",
        "missing = []\n",
        "for _, row in pairs.iterrows():\n",
        "    rec_path = resolve_path(rec_dir, row['receptor'], index=rec_index)\n",
        "    lig_path = resolve_path(lig_dir, row['ligand'],   index=lig_index)\n",
        "    if not rec_path or not lig_path:\n",
        "        missing.append({\n",
        "            'receptor': row['receptor'], 'ligand': row['ligand'],\n",
        "            'missing_receptor': not bool(rec_path), 'missing_ligand': not bool(lig_path)\n",
        "        })\n",
        "\n",
        "if missing:\n",
        "    print(\"‚ö†Ô∏è Some files are missing after normalization/case-resolution:\\n\")\n",
        "    for m in missing[:20]:\n",
        "        print(m)\n",
        "    print(f\"\\nTotal missing: {len(missing)}. Fix these names or files before docking.\\n\")\n",
        "\n",
        "# ---- Docking (GPU/CPU logic optional; focus here is filename fix)\n",
        "EXHAUSTIVENESS, NUM_MODES, SEED = 16, 20, 0\n",
        "gpu_flag = \"\"  # or \"--gpu\" if you‚Äôre using the GPU version\n",
        "\n",
        "def gnina_cmd(row):\n",
        "    rec  = resolve_path(rec_dir, row['receptor'], index=rec_index)\n",
        "    lig  = resolve_path(lig_dir, row['ligand'],   index=lig_index)\n",
        "    assert rec and lig, f\"Missing file(s): rec={row['receptor']} lig={row['ligand']}\"\n",
        "    tag  = f\"{row['receptor']}_{row.get('site_id','NA')}_{row['ligand']}\"\n",
        "    pose = out_dir / f\"{tag}_top.sdf\"\n",
        "    log  = out_dir / f\"{tag}.log\"\n",
        "    return (\n",
        "        f\"{GNINA_BIN} {gpu_flag} \"\n",
        "        f\"--receptor {shlex.quote(rec)} --ligand {shlex.quote(lig)} \"\n",
        "        f\"--center_x {row['center_x']} --center_y {row['center_y']} --center_z {row['center_z']} \"\n",
        "        f\"--size_x {row['size_x']} --size_y {row['size_y']} --size_z {row['size_z']} \"\n",
        "        f\"--exhaustiveness {EXHAUSTIVENESS} --num_modes {NUM_MODES} --seed {SEED} \"\n",
        "        f\"--cnn_scoring rescore --out {shlex.quote(str(pose))} --log {shlex.quote(str(log))}\"\n",
        "    ).strip()\n",
        "\n",
        "failures = []\n",
        "for _, row in tqdm(pairs.iterrows(), total=len(pairs), desc=\"Docking\"):\n",
        "    try:\n",
        "        cmd = gnina_cmd(row)\n",
        "        subprocess.run(shlex.split(cmd), check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    except AssertionError as e:\n",
        "        print(f\"‚ö†Ô∏è  Skipping {row['receptor']}-{row.get('site_id','NA')}-{row['ligand']} :: {e}\")\n",
        "        failures.append(row)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"‚ö†Ô∏è  Dock failed for {row['receptor']}-{row.get('site_id','NA')}-{row['ligand']}\")\n",
        "        print(e.stderr.decode(errors=\"ignore\")[:300], \"‚Ä¶\")\n",
        "        failures.append(row)\n",
        "\n",
        "print(f\"‚úÖ  Docking finished: {len(pairs)-len(failures)} successes, {len(failures)} failures\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405,
          "referenced_widgets": [
            "136ab28342594bd2aec47aaa756f4b27",
            "98a68afd9cc447ca9805baf607a80170",
            "b075fb56729945139b82bd2bd970cd02",
            "30591625c5f14156836bceb05591313c",
            "bf97bcf2b08240c49b97de54736e9393",
            "72ff375982a4426eb58d0831c495dc32",
            "a75da0b3453144199814e67d9a69f36e",
            "c3a100697dcb4fe6b251d95606913743",
            "c456fd31fdfa4ad7a3fca6725c7009ce",
            "bc93b0cd1e9b4279a004ebad2a3deac9",
            "0e5f7c45d17241f2ab084110512d7422"
          ]
        },
        "id": "EpGvSQFIXcTV",
        "outputId": "622a8a2b-e1da-4197-d782-0c8b506db829"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö†Ô∏è GPU requested but not available or gnina not built with CUDA. Falling back to CPU.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "136ab28342594bd2aec47aaa756f4b27",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Docking:   0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2707845134.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m# inherit env so CUDA_VISIBLE_DEVICES is respected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshlex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCalledProcessError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"‚ö†Ô∏è  Dock failed for {row['receptor']}-{row['site_id']}-{row['ligand']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTimeoutExpired\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1209\u001b[0;31m                 \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1210\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m                 \u001b[0;31m# https://bugs.python.org/issue25942\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36m_communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2113\u001b[0m                             'failed to raise TimeoutExpired.')\n\u001b[1;32m   2114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                     \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# --------------------------------------------------------------\n",
        "#  Batch GNINA docking  (prefers GPU; falls back to CPU)\n",
        "# --------------------------------------------------------------\n",
        "import os, pathlib, shlex, subprocess\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ---- Paths (adjust as needed)\n",
        "root    = '/content/drive/MyDrive/Sertaline_Derv_docking'\n",
        "rec_dir = f'{root}/receptors'\n",
        "lig_dir = f'{root}/ligands'\n",
        "out_dir = pathlib.Path(f'{root}/gnina_out'); out_dir.mkdir(exist_ok=True)\n",
        "\n",
        "GNINA_BIN = pathlib.Path(\"/content/drive/MyDrive/Sertaline_Derv_docking/gnina\")  # CUDA build preferred\n",
        "assert GNINA_BIN.is_file(), \"gnina binary not found!\"\n",
        "\n",
        "pairs = pd.read_csv(f'{root}/pairlist.csv', skipinitialspace=True)\n",
        "pairs.columns = [c.strip().lower() for c in pairs.columns]\n",
        "\n",
        "# ---- User knobs\n",
        "PREFER_GPU      = True                 # try to use GPU if present & supported\n",
        "SELECT_GPU_ID   = 0                    # which GPU to use (if multiple)\n",
        "EXHAUSTIVENESS  = 16\n",
        "NUM_MODES       = 20\n",
        "SEED            = 0\n",
        "\n",
        "# ---- Helpers\n",
        "def have_nvidia_gpu() -> bool:\n",
        "    # Colab-style quick checks\n",
        "    if os.path.exists(\"/proc/driver/nvidia/version\"):\n",
        "        return True\n",
        "    try:\n",
        "        out = subprocess.run([\"nvidia-smi\", \"-L\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)\n",
        "        return b\"GPU\" in out.stdout\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def gnina_supports_gpu() -> bool:\n",
        "    \"\"\"Best-effort check: run `gnina --help` and search for '--gpu' flag.\"\"\"\n",
        "    try:\n",
        "        out = subprocess.run([str(GNINA_BIN), \"--help\"], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, check=True)\n",
        "        txt = out.stdout.decode(errors=\"ignore\").lower()\n",
        "        return \"--gpu\" in txt  # gnina CUDA builds expose this flag\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "USE_GPU = PREFER_GPU and have_nvidia_gpu() and gnina_supports_gpu()\n",
        "\n",
        "if USE_GPU:\n",
        "    # Limit gnina to a specific GPU if requested\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(SELECT_GPU_ID)\n",
        "    gpu_flag = \"--gpu\"\n",
        "    print(f\"‚úÖ Using GPU (CUDA_VISIBLE_DEVICES={os.environ['CUDA_VISIBLE_DEVICES']})\")\n",
        "else:\n",
        "    gpu_flag = \"\"\n",
        "    if PREFER_GPU:\n",
        "        print(\"‚ö†Ô∏è GPU requested but not available or gnina not built with CUDA. Falling back to CPU.\")\n",
        "    else:\n",
        "        print(\"‚ÑπÔ∏è Using CPU as requested.\")\n",
        "\n",
        "def gnina_cmd(row):\n",
        "    rec  = f\"{rec_dir}/{row['receptor']}.pdbqt\"\n",
        "    lig  = f\"{lig_dir}/{row['ligand']}.pdbqt\"\n",
        "    tag  = f\"{row['receptor']}_{row['site_id']}_{row['ligand']}\"\n",
        "    pose = out_dir / f\"{tag}_top.sdf\"\n",
        "    log  = out_dir / f\"{tag}.log\"\n",
        "\n",
        "    return (\n",
        "        f\"{GNINA_BIN} {gpu_flag} \"\n",
        "        f\"--receptor {shlex.quote(rec)} --ligand {shlex.quote(lig)} \"\n",
        "        f\"--center_x {row['center_x']} --center_y {row['center_y']} --center_z {row['center_z']} \"\n",
        "        f\"--size_x {row['size_x']} --size_y {row['size_y']} --size_z {row['size_z']} \"\n",
        "        f\"--exhaustiveness {EXHAUSTIVENESS} --num_modes {NUM_MODES} --seed {SEED} \"\n",
        "        f\"--cnn_scoring rescore --out {shlex.quote(str(pose))} --log {shlex.quote(str(log))}\"\n",
        "    ).strip()\n",
        "\n",
        "failures = []\n",
        "for _, row in tqdm(pairs.iterrows(), total=len(pairs), desc=\"Docking\"):\n",
        "    cmd = gnina_cmd(row)\n",
        "    try:\n",
        "        # inherit env so CUDA_VISIBLE_DEVICES is respected\n",
        "        res = subprocess.run(shlex.split(cmd), check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=os.environ)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"‚ö†Ô∏è  Dock failed for {row['receptor']}-{row['site_id']}-{row['ligand']}\")\n",
        "        print(e.stderr.decode(errors=\"ignore\")[:300], \"‚Ä¶\")\n",
        "        failures.append(row)\n",
        "\n",
        "print(f\"‚úÖ  Docking finished: {len(pairs)-len(failures)} successes, {len(failures)} failures\")\n",
        "\n",
        "# Optional: save failures for quick reruns\n",
        "if failures:\n",
        "    pd.DataFrame(failures).to_csv(out_dir / \"failures.csv\", index=False)\n",
        "    print(f\"üíæ Saved failure rows to {out_dir/'failures.csv'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "U31OauaDtvbL",
        "outputId": "28d97868-5c79-4609-c08e-9603f9f26109"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'rdkit'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-8c5502d00c7c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrdkit\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrdkit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAllChem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mopenbabel\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rdkit'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# ‚¨áÔ∏è  post_gnina.py ‚Äì parses logs, builds complexes, splits poses\n",
        "import re, argparse, pathlib, csv\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "import openbabel as ob\n",
        "\n",
        "def parse_log(log_path):\n",
        "    tag = log_path.stem\n",
        "    rows = []\n",
        "    with open(log_path) as fh:\n",
        "        for ln in fh:\n",
        "            if ln.startswith(\"CNNaffinity\"):\n",
        "                rows.append(\n",
        "                    dict(tag=tag,\n",
        "                         cnn_aff=float(ln.split()[1]),\n",
        "                         cnn_score=float(ln.split()[3].strip('()'))))\n",
        "    return rows\n",
        "\n",
        "def make_complexes(pose_sdf, rec_pdbqt, out_multi, out_dir):\n",
        "    conv = ob.OBConversion(); conv.SetInAndOutFormats(\"pdbqt\", \"pdb\")\n",
        "    recmol = ob.OBMol();  conv.ReadFile(recmol, str(rec_pdbqt))\n",
        "    rec_pdb = rec_pdbqt.with_suffix(\".pdb\")\n",
        "    conv.WriteFile(recmol, str(rec_pdb))\n",
        "\n",
        "    suppl = Chem.SDMolSupplier(str(pose_sdf), removeHs=False)\n",
        "    out_dir.mkdir(exist_ok=True)\n",
        "    with open(out_multi, \"w\") as big:\n",
        "        for i, lig in enumerate(suppl, 1):\n",
        "            with open(rec_pdb) as fh: big.write(fh.read())\n",
        "            big.write(Chem.MolToPDBBlock(lig));  big.write(\"ENDMDL\\n\")\n",
        "            split = out_dir / f\"{pose_sdf.stem}_pose{i:02d}.pdb\"\n",
        "            with open(split, \"w\") as sp:\n",
        "                with open(rec_pdb) as fh: sp.write(fh.read())\n",
        "                sp.write(Chem.MolToPDBBlock(lig))\n",
        "    rec_pdb.unlink()\n",
        "\n",
        "def post_gnina(gnina_out, rec_prep):\n",
        "    gnina_out = pathlib.Path(gnina_out)\n",
        "    rec_prep  = pathlib.Path(rec_prep)\n",
        "\n",
        "    # 1 logs ‚Üí CSV\n",
        "    rows = []\n",
        "    for log in tqdm(gnina_out.glob(\"*.log\"), desc=\"parse logs\"):\n",
        "        rows += parse_log(log)\n",
        "    pd.DataFrame(rows).to_csv(gnina_out/\"all_scores.csv\", index=False)\n",
        "\n",
        "    # 2 + 3 complex build & split\n",
        "    cmp_dir = gnina_out/\"complexes\"; cmp_dir.mkdir(exist_ok=True)\n",
        "    for sdf in tqdm(gnina_out.glob(\"*_top.sdf\"), desc=\"make complexes\"):\n",
        "        rec = rec_prep/f\"{sdf.stem.split('_')[0]}.pdbqt\"\n",
        "        if not rec.exists():\n",
        "            print(\"skip, receptor not found:\", rec)\n",
        "            continue\n",
        "        make_complexes(sdf, rec,\n",
        "                       out_multi = cmp_dir/f\"{sdf.stem}_complexes.pdb\",\n",
        "                       out_dir   = cmp_dir)\n",
        "\n",
        "# ------- call the helper --------------------------------------\n",
        "GNINA_OUT   = \"/content/drive/MyDrive/EFA_Docking/gnina_out\"     # ‚Üê adjust\n",
        "RECEPTOR_PREP = \"/content/drive/MyDrive/EFA_Docking/receptors\"\n",
        "post_gnina(GNINA_OUT, RECEPTOR_PREP)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "08e759f098244d95ae22fbf9e8b0c639": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d9ed27a1fea04207b080dfe1e9678ca8",
              "IPY_MODEL_db614613ca7546dea7a58cb08c54a2d5",
              "IPY_MODEL_a0fab85925714784abbbf6ebb62bffe1"
            ],
            "layout": "IPY_MODEL_7e499c64af9344fdafb1cfc70c97c9b7"
          }
        },
        "0daff165ad2f49dd9d849470a06e8c25": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e5f7c45d17241f2ab084110512d7422": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "136ab28342594bd2aec47aaa756f4b27": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_98a68afd9cc447ca9805baf607a80170",
              "IPY_MODEL_b075fb56729945139b82bd2bd970cd02",
              "IPY_MODEL_30591625c5f14156836bceb05591313c"
            ],
            "layout": "IPY_MODEL_bf97bcf2b08240c49b97de54736e9393"
          }
        },
        "30591625c5f14156836bceb05591313c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc93b0cd1e9b4279a004ebad2a3deac9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0e5f7c45d17241f2ab084110512d7422",
            "value": "‚Äá0/15‚Äá[00:08&lt;?,‚Äá?it/s]"
          }
        },
        "69f54fdbc73c485685d0235e80e0e728": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ef0b37cb4e947f58e44bae492685382": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72ff375982a4426eb58d0831c495dc32": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "734e3b3dc2be446a879b95ddc5c4e4be": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e499c64af9344fdafb1cfc70c97c9b7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98a68afd9cc447ca9805baf607a80170": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72ff375982a4426eb58d0831c495dc32",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a75da0b3453144199814e67d9a69f36e",
            "value": "Docking:‚Äá‚Äá‚Äá0%"
          }
        },
        "9d0a79b9297d4890a8c2a95b50cefde9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a0fab85925714784abbbf6ebb62bffe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef8e05a61a2a47d683f192517576dd39",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0daff165ad2f49dd9d849470a06e8c25",
            "value": "‚Äá0/125‚Äá[00:00&lt;?,‚Äá?it/s]"
          }
        },
        "a75da0b3453144199814e67d9a69f36e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b075fb56729945139b82bd2bd970cd02": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3a100697dcb4fe6b251d95606913743",
            "max": 15,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c456fd31fdfa4ad7a3fca6725c7009ce",
            "value": 0
          }
        },
        "bc93b0cd1e9b4279a004ebad2a3deac9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf97bcf2b08240c49b97de54736e9393": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3a100697dcb4fe6b251d95606913743": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c456fd31fdfa4ad7a3fca6725c7009ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d9ed27a1fea04207b080dfe1e9678ca8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ef0b37cb4e947f58e44bae492685382",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_69f54fdbc73c485685d0235e80e0e728",
            "value": "Docking:‚Äá‚Äá‚Äá0%"
          }
        },
        "db614613ca7546dea7a58cb08c54a2d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_734e3b3dc2be446a879b95ddc5c4e4be",
            "max": 125,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9d0a79b9297d4890a8c2a95b50cefde9",
            "value": 0
          }
        },
        "ef8e05a61a2a47d683f192517576dd39": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
